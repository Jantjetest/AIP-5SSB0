{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression - Illustration\n",
    "\\begin{center}\n",
    "\\includegraphics[height=4.5cm]{./figures/fig-bishop12}\n",
    "\\end{center}\n",
    "\n",
    "Given a set of (noisy) data measurements, find the 'best' (linear) relation between an input variables $x$ and input-dependent outcomes $y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The Regression Model\n",
    "\n",
    "\n",
    "- Observe $N$ IID data **pairs** $D=\\{(x_1,y_1),\\dotsc,(x_N,y_N)\\}$, $x_n \\in \\Re^D$ and $y_n \\in \\Re$.\n",
    "- [Q.] We could try to build a model for the data by density estimation, $p(x,y)$, but what if we are interested only in (a model for) the responses $y_n$ for **given inputs** $x_n$?\n",
    "- [A.] We will build models for the conditional distribution $p(y|x)$. Note that, since $p(x,y)=p(y|x)\\, p(x)$, this is a building block for the joint data density.\n",
    "- In a _regression model_, we try to 'explain the data' by a purely deterministic term $f(x,w)$, plus a purely random term $\\epsilon_n$ for 'unexplained noise',\n",
    "    $$\n",
    "    y_n  = f(x_n,w) + \\epsilon_n\n",
    "    $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Model Specification for Linear Regression\n",
    "\n",
    "-  In **linear** regression, we assume that $f(x,w)=w^T x$.\n",
    "-  In **ordinary linear regression**, the noise process $\\epsilon_n$ is zero-mean Gaussian with constant variance $\\sigma^2$, i.e.\n",
    "$$\n",
    "y_n  = w^T x_n  + \\mathcal{N}(0,\\sigma^2) ,\n",
    "$$\n",
    "or equivalently, the likelihood model is (use $\\mathrm{E}[y_n|x_n,w] =\\mathrm{E}[ w^T x_n  + \\epsilon_n|\\,x_n,w] = w^T x_n$)\n",
    "$$\n",
    "p(y_n|\\,x_n,w) = \\mathcal{N}(y_n|\\,w^T x_n,\\sigma^2) .\n",
    "$$\n",
    "-  Remember that for full Bayesian learning we should also choose a prior $p(w)$; In ML estimation, the prior is uniformly distributed (so it can be ignored).\n",
    "<!--- \n",
    "%$y_n$ is Gaussian around $\\theta^T x_n$ with `unexplained' noise $\\sigma^2$.\n",
    "% -   Note that 1st term RHS is purely deterministic; 2nd term is purely random and independent of $x_n$\n",
    "%-  Choose a prior over the parameters (possibly flat: $\\sigma\\rightarrow \\infty$)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  ML Estimation for Linear Regression Model\n",
    "\n",
    "-  Inference by ML; work out the log-likelihood for $w$,\n",
    "\\begin{align}\n",
    "\\log p(D|w) &\\stackrel{IID}{=} \\sum_n \\log \\mathcal{N}(y_n|\\,w^T x_n,\\sigma^2)  \\propto -\\frac{1}{2\\sigma^2} \\sum_{n} {(y_n - w^T x_n)^2}\\\\\n",
    "    &= -\\frac{1}{2\\sigma^2}\\left( {y - \\mathbf{X}w } \\right)^T \\left( {y - \\mathbf{X} w } \\right)\n",
    "\\end{align}\n",
    "where  we defined $N\\times 1$ vector $y  = \\left(y_1 ,y_2 , \\ldots ,y_N \\right)^T$ and $(N\\times D)$-dim matrix $\\mathbf{X}  = \\left( x_1 ,x_2 , \\ldots ,x_n \\right)^T$.\n",
    "\n",
    "-  Set the derivative $\\nabla_{w} \\log p(D|w) = \\frac{1}{\\sigma^2} \\mathbf{X}^T(y-\\mathbf{X} w)$ to zero for\n",
    "the maximum likelihood estimate\n",
    "$$\n",
    "\\boxed{\\hat w = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T y}\n",
    "$$\n",
    "-  **Prediction** of new data points by\n",
    "$$\\hat y_{\\text{new}} = \\hat w^T x_{\\text{new}} = y^T \\mathbf{X} \\left( \\mathbf{X}^T \\mathbf{X} \\right)^{-1} \\, x_{\\text{new}}$$\n",
    "\n",
    "-  N.B. The matrix $\\mathbf{X}^\\dagger \\equiv  (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T$ is also known as the **Moore-Penrose pseudo-inverse**.\n",
    "-  Note that size ($N\\times D$) of the data matrix $\\mathbf{X}$ grows with number of observations, but the size ($K\\times K$) of $\\mathbf{X}^T\\mathbf{X}$ is independent of training data set.\n",
    "- Also note that the _prediction_ of a new data point\n",
    "\n",
    "$$\n",
    "\\hat y_{\\text{new}} = \\hat w^T x_{\\text{new}} = x_{\\text{new}}^T\\hat{w} = \\left( x_{\\text{new}}^T \\mathbf{X}^\\dagger \\right)y\n",
    "$$\n",
    "is a linear combination of the previous data points $y  = \\left( {y_1 ,y_1 , \\ldots ,y_N } \\right)^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Deterministic Least-Squares Regression\n",
    "\n",
    "-  (You may say that) we don't need to work with probabilistic models. E.g., there's also the deterministic **least-squares** solution: minimize sum of squared errors,\n",
    "$$ \\hat w_{LS} = \\arg\\min_{w} \\sum_n {\\left( {y_n  - w ^T x_n } \\right)} ^2 $$\n",
    "\n",
    "-  Derivative $ \\partial \\left( {y - \\mathbf{X}w } \\right)^T \\left( {y - \\mathbf{X}w } \\right) / \\partial w = -2 \\mathbf{X}^T \\left(y - \\mathbf{X} w  \\right)$\n",
    "-  Setting derivative to zero yields **normal equations** $\\mathbf{X}^T\\mathbf{X} \\hat w_{LS} = \\mathbf{X}^T y$ and\n",
    "$$\n",
    "\\hat w_{LS} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T y\n",
    "$$\n",
    "\n",
    "$\\Rightarrow$ Least-squares regression corresponds to (probabilistic) maximum likelihood if\n",
    "  -  **IID samples** (determines how errors are combined)\n",
    "  -  Noise $\\epsilon_n \\sim \\mathcal{N}(0,\\,\\sigma^2)$ is **Gaussian** (determines error metric)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  Probabilistic vs. Deterministic Approach\n",
    "\n",
    "-  The (deterministic) least-squares approach assumed IID Gaussian distributed data, but these assumptions are not obvious from looking at the least-squares (LS) criterion.\n",
    "-  If the data were better modeled by non-Gaussian assumptions (or not IID), then LS might not be appropriate.\n",
    "-  The probabilistic approach makes all these issues completely transparent by focusing on the **model specification** rather than the error criterion.\n",
    "\n",
    "-  Next, we will show this by two examples: (1) samples not identically distributed, and (2) few data points.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  Not Identically Distributed Data\n",
    "\n",
    "-  What if we assume that the variance of the measurement error varies with the sampling index,  $\\epsilon_n \\sim \\mathcal{N}(0,\\sigma_n^2)$?\n",
    "$$\n",
    "\\mathrm{L(w)} \\triangleq \\log p(D|w) \\sim -\\frac{1}{2} \\sum_n \\frac{(y_n-w^T x_n)^2}{\\sigma_n^2}  = -\\frac{1}{2} (y- \\mathbf{X}w)^T \\Lambda (y- \\mathbf{X} w)\n",
    "$$\n",
    "where  $\\Lambda = \\mathrm{diag}[1/\\sigma_n^2]$.\n",
    "-  Set derivative $\\partial \\mathrm{L(w)} / \\partial w = -\\mathbf{X}^T\\Lambda (y-\\mathbf{X} w)$ to zero to get the **normal equations** $\\mathbf{X}^T \\Lambda \\mathbf{X} \\hat{w} = \\mathbf{X}^T \\Lambda y $ and consequently \n",
    "$$ \\boxed{\\hat{w} = \\left(\\mathbf{X}^T \\Lambda \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\Lambda y}$$ \n",
    "-  This is called the **Weighted Least Squares** (WLS) solution. (Note that we just stumbled upon it, the crucial aspect is appropriate model specification!)\n",
    "-  Note also that the dimension of $\\Lambda$ grows with the number of data points. In general, models for which the number of parameters grow as the number of observations increase are called **non-parametric models**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Too Few Training Samples\n",
    "\n",
    "-  [Q.]: If we have fewer training samples than input dimensions, $\\mathbf{X}^T\\mathbf{X}$ will not be invertible.\n",
    "-  [A.]: In case of (expected) problems, **go back to full Bayesian!** Do proper model specification, Bayesian inference etc.\n",
    "-  **Model specification**. Let's try a Gaussian prior for $w$ (why is this reasonable?),\n",
    "  $$p(w) = \\mathcal{N}(w|0,\\Sigma) = \\mathcal{N}(w|0,\\varepsilon I)$$\n",
    "-  **Learning**. Let's do Bayesian inference,\n",
    "\\begin{align}\n",
    "\\log p(w|D) &\\propto \\log p(D|w)p(w) \\stackrel{IID}{=} \\log \\sum_n p(y_n|x_n,w) + \\log p(w)\\\\\n",
    "   &= \\log \\sum_n \\mathcal{N}(y_n|\\,w^Tx_n,\\sigma^2) + \\log \\mathcal{N}(w|0,\\varepsilon I)\\\\\n",
    "   &\\propto \\frac{1}{2\\sigma^2}\\left( {y - \\mathbf{X}w } \\right)^T \\left( {y - \\mathbf{X}w } \\right)  + \\frac{1}{2 \\epsilon}w^T w\n",
    "\\end{align}\n",
    "-  Done! The posterior $p(w|D)$ specifies all we know about $w$ after seeing the data $D$.\n",
    "-  As discussed, for practical purposes, you often want a point estimate for $w$, rather than a posterior distribution.\n",
    "-  Let's take a **MAP estimate**. Set derivative \n",
    "$\\nabla_{w} \\log p(w|D) = -\\frac{1}{\\sigma^2}\\mathbf{X}^T(y-\\mathbf{X}w) + \\frac{1}{\\varepsilon} w$ to zero, yielding\n",
    "\n",
    "$$\n",
    "\\boxed{ \\hat{w}_{\\text{MAP}} = \\left( \\mathbf{X}^T\\mathbf{X} + \\frac{\\sigma^2}{\\varepsilon} I \\right)^{-1}\\mathbf{X}^T y }\n",
    "$$\n",
    "\n",
    "- Note that the matrix $\\left( \\mathbf{X}^T\\mathbf{X} + (\\sigma^2 / \\varepsilon) I \\right)$ is always invertible! Why?\n",
    "- Note also that LS corresponds to $\\varepsilon  \\rightarrow \\infty$. Does that make sense?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Adaptive Linear Regression\n",
    "-  What if the data arrives one point at a time?\n",
    "-  Two standard _adaptive_ linear regression approaches: RLS and LMS. Here we shortly recap the LMS approach.\n",
    "-  **Least Mean Squares** (LMS) is gradient-descent on a 'local-in-time' approximation of the square-error cost function.\n",
    "-  Define the 'cost of current sample' as  \n",
    "$$\n",
    "        E_n(w) = \\frac{1}{2}(y_n - w^Tx_n)^2\n",
    "$$ \n",
    "and track the optimum by gradient descent:\n",
    "$$\n",
    "w_{n+1} = w_n - \\eta \\, \\left. \\frac{\\partial E_n}{\\partial w} \\right|_{w_n}\n",
    "$$\n",
    "which leads to the LMS update:\n",
    "$$\n",
    " \\boxed{ w_{n+1} = w_n + \\eta \\, (y_n - w_n^T x_n) x_n }\n",
    "$$\n",
    "-  (OPTIONAL) For a really cool Bayesian view on LMS, download G. Deng, A model-based approach for the development of LMS algorithms', _ISCAS-05 symposium}, 2005.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.6",
   "language": "julia",
   "name": "julia 0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
