{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Maximum likelihood estimates for various linear regression variants\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional\n",
    "    - Bishop pp. 140-144      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression - Illustration\n",
    "\n",
    "\n",
    "<img src=\"./figures/fig-bishop12.png\" width=300px>\n",
    "\n",
    "Given a set of (noisy) data measurements, find the 'best' relation between an input variable $x$ and input-dependent outcomes $y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The Regression Model\n",
    "\n",
    "\n",
    "- Observe $N$ IID data **pairs** $D=\\{(x_1,y_1),\\dotsc,(x_N,y_N)\\}$ with $x_n \\in \\Re^D$ and $y_n \\in \\Re$.\n",
    "\n",
    "\n",
    "- <span style=\"color:blue\">[Q.]</span> We could try to build a model for the data by density estimation, $p(x,y)$, but what if we are interested only in (a model for) the responses $y_n$ for **given inputs** $x_n$?\n",
    "\n",
    "\n",
    "- <span style=\"color:blue\">[A.]</span> We will build models for the conditional distribution $p(y|x)$. \n",
    "  - Note that, since $p(x,y)=p(y|x)\\, p(x)$, this is a building block for the joint data density.\n",
    "\n",
    "\n",
    "- In a _regression model_, we try to 'explain the data' by a purely deterministic term $f(x,w)$, plus a purely random term $\\epsilon_n$ for 'unexplained noise',\n",
    "    $$\n",
    "    y_n  = f(x_n,w) + \\epsilon_n\n",
    "    $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Model Specification for Linear Regression\n",
    "\n",
    "-  In **linear** regression, we assume that \n",
    "$$f(x,w)=w^T x \\,.$$\n",
    "\n",
    "-  In **ordinary linear regression**, the noise process $\\epsilon_n$ is zero-mean Gaussian with constant variance $\\sigma^2$, i.e.\n",
    "$$\n",
    "y_n  = w^T x_n  + \\mathcal{N}(0,\\sigma^2) \\,,\n",
    "$$\n",
    "or equivalently, the likelihood model is \n",
    "$$\n",
    "p(y_n|\\,x_n,w) = \\mathcal{N}(y_n|\\,w^T x_n,\\sigma^2) \\,.\n",
    "$$\n",
    "\n",
    "-  Remember that for full Bayesian learning we should also choose a prior $p(w)$; In ML estimation, the prior is uniformly distributed (so it can be ignored).\n",
    "<!--- \n",
    "%$y_n$ is Gaussian around $\\theta^T x_n$ with `unexplained' noise $\\sigma^2$.\n",
    "% -   Note that 1st term RHS is purely deterministic; 2nd term is purely random and independent of $x_n$\n",
    "%-  Choose a prior over the parameters (possibly flat: $\\sigma\\rightarrow \\infty$)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  ML Estimation for Linear Regression Model\n",
    "\n",
    "-  **Inference** by ML; Let's first work out the log-likelihood for $w$,\n",
    "$$\\begin{align*}\n",
    "\\log p(D|w) &\\stackrel{\\text{IID}}{=} \\sum_n \\log \\mathcal{N}(y_n|\\,w^T x_n,\\sigma^2)\\\\  &\\propto -\\frac{1}{2\\sigma^2} \\sum_{n} {(y_n - w^T x_n)^2}\\\\\n",
    "    &= -\\frac{1}{2\\sigma^2}\\left( {y - \\mathbf{X}w } \\right)^T \\left( {y - \\mathbf{X} w } \\right)\n",
    "\\end{align*}$$\n",
    "where  we defined $N\\times 1$ vector $y  = \\left(y_1 ,y_2 , \\ldots ,y_N \\right)^T$ and $(N\\times D)$-dim matrix $\\mathbf{X}  = \\left( x_1 ,x_2 , \\ldots ,x_n \\right)^T$.\n",
    "\n",
    "-  Set the derivative $$\\nabla_{w} \\log p(D|w) = \\frac{1}{\\sigma^2} \\mathbf{X}^T(y-\\mathbf{X} w)$$ to zero for\n",
    "the maximum likelihood estimate\n",
    "$$\\begin{equation*}\n",
    "\\boxed{\\hat w_{\\text{ML}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T y}\n",
    "\\end{equation*}$$\n",
    "\n",
    "\n",
    "-  The matrix $\\mathbf{X}^\\dagger \\equiv  (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T$ is also known as the **Moore-Penrose pseudo-inverse** (which is sort-of-an-inverse for non-square matrices).\n",
    "\n",
    "-  Note that size ($N\\times D$) of the data matrix $\\mathbf{X}$ grows with number of observations, but the size ($K\\times K$) of $\\mathbf{X}^T\\mathbf{X}$ is independent of training data set.\n",
    "\n",
    "\n",
    "-  **Prediction** of new data points by\n",
    "$$\\begin{equation*}\\hat y_{\\text{new}} = \\hat w_{\\text{ML}}^T x_{\\text{new}} = y^T \\mathbf{X} \\left( \\mathbf{X}^T \\mathbf{X} \\right)^{-1} \\, x_{\\text{new}}\n",
    "\\end{equation*}$$\n",
    "\n",
    "- Also note that the _prediction_ of a new data point\n",
    "$$\n",
    "\\hat y_{\\text{new}} = \\hat w_{\\text{ML}}^T x_{\\text{new}} = x_{\\text{new}}^T\\hat{w}_{\\text{ML}} = \\left( x_{\\text{new}}^T \\mathbf{X}^\\dagger \\right)y\n",
    "$$\n",
    "is a linear combination of the previous data points $y  = \\left( {y_1 ,y_1 , \\ldots ,y_N } \\right)^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Deterministic Least-Squares Regression\n",
    "\n",
    "-  (You may say that) we don't need to work with probabilistic models. E.g., there's also the deterministic **least-squares** solution: minimize sum of squared errors,\n",
    "$$ \\hat w_{LS} = \\arg\\min_{w} \\sum_n {\\left( {y_n  - w ^T x_n } \\right)} ^2 $$\n",
    "\n",
    "-  Setting the derivative \n",
    "$$ \\partial \\left( {y - \\mathbf{X}w } \\right)^T \\left( {y - \\mathbf{X}w } \\right) / \\partial w = -2 \\mathbf{X}^T \\left(y - \\mathbf{X} w  \\right)\n",
    "$$\n",
    "to zero yields the **normal equations** \n",
    "$$\\mathbf{X}^T\\mathbf{X} \\hat w_{\\text{LS}} = \\mathbf{X}^T y\n",
    "$$ \n",
    "and consequently\n",
    "$$\n",
    "\\boxed{\\hat w_{\\text{LS}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T y} \n",
    "$$\n",
    "\n",
    "$\\Rightarrow$ Least-squares regression ($\\hat w_{\\text{LS}}$) corresponds to (probabilistic) maximum likelihood ($\\hat w_{\\text{ML}}$) if \n",
    "  1. **IID samples** (determines how errors are combined), and\n",
    "  1.  Noise $\\epsilon_n \\sim \\mathcal{N}(0,\\,\\sigma^2)$ is **Gaussian** (determines error metric)      \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  Probabilistic vs. Deterministic Approach\n",
    "\n",
    "-  The (deterministic) least-squares approach assumed IID Gaussian distributed data, but these assumptions are not obvious from looking at the least-squares (LS) criterion.\n",
    "\n",
    "-  If the data were better modeled by non-Gaussian assumptions (or not IID), then LS might not be appropriate.\n",
    "\n",
    "-  The probabilistic approach makes all these issues completely transparent by focusing on the **model specification** rather than the error criterion.\n",
    "\n",
    "-  Next, we will show this by two examples: (1) samples not identically distributed, and (2) few data points.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  Not Identically Distributed Data\n",
    "\n",
    "-  What if we assume that the variance of the measurement error varies with the sampling index,  $\\epsilon_n \\sim \\mathcal{N}(0,\\sigma_n^2)$?\n",
    "\n",
    "- Let's make the log-likelihood again: \n",
    "$$\\begin{align*}\n",
    "\\mathrm{L(w)} &\\triangleq \\log p(D|w) \\\\ \n",
    "  &\\propto -\\frac{1}{2} \\sum_n \\frac{(y_n-w^T x_n)^2}{\\sigma_n^2} \\\\ \n",
    "  &= -\\frac{1}{2} (y- \\mathbf{X}w)^T \\Lambda (y- \\mathbf{X} w)\n",
    "\\end{align*}$$\n",
    "where  $\\Lambda \\triangleq \\mathrm{diag}[1/\\sigma_n^2]$.\n",
    "\n",
    "-  Set derivative \n",
    "$$\\partial \\mathrm{L(w)} / \\partial w = -\\mathbf{X}^T\\Lambda (y-\\mathbf{X} w)$$ \n",
    "to zero to get the **normal equations** \n",
    "$$\\mathbf{X}^T \\Lambda \\mathbf{X} \\hat{w_{\\text{WLS}}} = \\mathbf{X}^T \\Lambda y$$ \n",
    "and consequently \n",
    "$$ \\boxed{\\hat{w_{\\text{WLS}}} = \\left(\\mathbf{X}^T \\Lambda \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\Lambda y}$$ \n",
    "\n",
    "-  This is called the **Weighted Least Squares** (WLS) solution. (Note that we just stumbled upon it, the crucial aspect is appropriate model specification!)\n",
    "\n",
    "-  Note also that the dimension of $\\Lambda$ grows with the number of data points. In general, models for which the number of parameters grow as the number of observations increase are called **non-parametric models**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Marco can you insert a Julia example here that illustrates the least-squares (LS) solution vs the Weighted-LS (WLS) on a data set where the noise variance increases with the sample index. Hopefully WLS does better here.  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Too Few Training Samples\n",
    "\n",
    "-  <span style=\"color:blue\">[Q.]</span>: If we have fewer training samples than input dimensions, $\\mathbf{X}^T\\mathbf{X}$ will not be invertible.\n",
    "\n",
    "-  <span style=\"color:blue\">[A.]</span>: In case of (expected) problems, **go back to full Bayesian!** Do proper model specification, Bayesian inference etc.\n",
    "\n",
    "-  **Model specification**. Let's try a Gaussian prior for $w$ (why is this reasonable?),\n",
    "$$\n",
    "p(w) = \\mathcal{N}(w|0,\\Sigma) = \\mathcal{N}(w|0,\\varepsilon I)\n",
    "$$\n",
    "\n",
    "-  **Learning**. Let's do Bayesian inference,\n",
    "$$\\begin{align*}\n",
    "\\log p(w|D) &\\propto \\log p(D|w)p(w) \\\\\n",
    "   &\\stackrel{IID}{=} \\log \\sum_n p(y_n|x_n,w) + \\log p(w)\\\\\n",
    "   &= \\log \\sum_n \\mathcal{N}(y_n|\\,w^Tx_n,\\sigma^2) + \\log \\mathcal{N}(w|0,\\varepsilon I)\\\\\n",
    "   &\\propto \\frac{1}{2\\sigma^2}\\left( {y - \\mathbf{X}w } \\right)^T \\left( {y - \\mathbf{X}w } \\right)  + \\frac{1}{2 \\epsilon}w^T w\n",
    "\\end{align*}$$\n",
    "\n",
    "-  **Done!** The posterior $p(w|D)$ specifies all we know about $w$ after seeing the data $D$\n",
    ".\n",
    "-  As discussed, for practical purposes, you often want a point estimate for $w$, rather than a posterior distribution.\n",
    "\n",
    "-  For instance, let's take a **MAP estimate**. Set derivative \n",
    "$$\\nabla_{w} \\log p(w|D) = -\\frac{1}{\\sigma^2}\\mathbf{X}^T(y-\\mathbf{X}w) + \\frac{1}{\\varepsilon} w\n",
    "$$ \n",
    "to zero, yielding\n",
    "$$\n",
    "\\boxed{ \\hat{w}_{\\text{MAP}} = \\left( \\mathbf{X}^T\\mathbf{X} + \\frac{\\sigma^2}{\\varepsilon} I \\right)^{-1}\\mathbf{X}^T y }\n",
    "$$\n",
    "\n",
    "- Note that, in contrast to $\\mathbf{X}^T\\mathbf{X}$, the matrix $\\left( \\mathbf{X}^T\\mathbf{X} + (\\sigma^2 / \\varepsilon) I \\right)$ is always invertible! Why?\n",
    "\n",
    "- Note also that $\\hat{w}_{\\text{LS}}$ is retrieved by letting $\\varepsilon  \\rightarrow \\infty$. Does that make sense?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Adaptive Linear Regression\n",
    "-  What if the data arrives one point at a time?\n",
    "-  Two standard _adaptive_ linear regression approaches: RLS and LMS. Here we shortly recap the LMS approach.\n",
    "-  **Least Mean Squares** (LMS) is gradient-descent on a 'local-in-time' approximation of the square-error cost function.\n",
    "-  Define the 'cost of current sample' as  \n",
    "$$\n",
    "        E_n(w) = \\frac{1}{2}(y_n - w^Tx_n)^2\n",
    "$$ \n",
    "and track the optimum by gradient descent (at each sample index $n$):\n",
    "$$\n",
    "w_{n+1} = w_n - \\eta \\, \\left. \\frac{\\partial E_n}{\\partial w} \\right|_{w_n}\n",
    "$$\n",
    "which leads to the LMS update:\n",
    "$$\n",
    " \\boxed{ w_{n+1} = w_n + \\eta \\, (y_n - w_n^T x_n) x_n }\n",
    "$$\n",
    "\n",
    "-  <span style=\"color:red\">(OPTIONAL)</span> Is there also a Bayesian treatment of LMS? Sure, e.g., have a look at [G. Deng et al., _A model-based approach for the development of LMS algorithms_', ISCAS-05 symposium, 2005](./files/Deng-2005-A-model-based-approach-for-the-development-of-LMS-algorithms.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "_The cell below loads the style file_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.cell { /* set cell width */\n",
       "    width: 750px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    width: 1000px;\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:600px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.css\") do f\n",
    "    display(\"text/html\", readall(f))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
