{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Introduction to linear generative classification with multinomial-Gaussian generative model\n",
    "  \n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional\n",
    "    - Bishop pp. pp. 196-202     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Problem: an apple or a peach?\n",
    "\n",
    "You're given numerical values for the skin features _roughness_ and _color_ for 200 pieces of fruit, where for each piece of fruit you also know if it is an apple or a peach. Now you receive the roughness and color values for a new piece of fruit but you don't get its class label (apple or peach). What is the probability that the new piece is an apple? \n",
    "\n",
    "<span style=\"color:red\"> MARCO Can you generate a data set in Julia and plot similar to the one below: all 2D features plus labels plus one piece withour label.</span>\n",
    "\n",
    "<img src=\"./figures/fig-classification-generative-1.png\" width=\"400\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Classification Problem Statement\n",
    "\n",
    "- Given is a data set  $D = \\{(x_1,y_1),\\dotsc,(x_N,y_N)\\}$\n",
    "  - inputs $x_n \\in \\mathbb{R}^D$ are called **features**.\n",
    "  - outputs $y_n \\in \\mathcal{C}_k$, with $k=1,\\ldots,K$; The **discrete** targets $\\mathcal{C}_k$ are called **classes**.\n",
    "\n",
    "- We will again use the 1-of-$K$ notation for the discrete classes. Define the binary **class selection variable**\n",
    "\n",
    "$$\n",
    "y_{nk} = \\begin{cases} 1 & \\text{if $y_n$ in class $\\mathcal{C}_k$}\\\\\n",
    "        0 & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "-  The plan for generative classification: build a model for the joint pdf $p(x,y)= p(x|y)p(y)$ and use Bayes to infer the posterior class probabilities \n",
    "\n",
    "$$\n",
    "p(y|x) = \\frac{p(x|y) p(y)}{\\sum_{y} p(x|y) p(y)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1 - Model specification \n",
    "\n",
    "##### Likelihood\n",
    "\n",
    "- Assume Gaussian **class-conditional distributions** with **constant covariance matrix** across the classes,\n",
    " $$\n",
    " p(x_n|\\mathcal{C}_{k}) = \\mathcal{N}(x_n|\\mu_k,\\Sigma)\n",
    " $$\n",
    "with notational shorthand: $\\mathcal{C}_{k} \\triangleq (y_{nk}=1)$.\n",
    "\n",
    "##### Prior\n",
    "\n",
    "- We use a categorical distribution for the class labels $y_{nk}$: \n",
    "$$p(\\mathcal{C}_{k}) = \\pi_k$$\n",
    "\n",
    "- This leads to\n",
    "$$\n",
    " p(x_n,\\mathcal{C}_{k}) =  \\pi_k \\cdot \\mathcal{N}(x_n|\\mu_k,\\Sigma)\n",
    "$$ \n",
    "\n",
    "- The log-likelihood for the full data set is then\n",
    "$$\\begin{align*}\n",
    "\\log\\, &p(D|\\theta) \\stackrel{\\text{IID}}{=} \\sum_n \\log p(x_n,\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{K} \\,|\\,\\theta) \\\\\n",
    "  &= \\sum_n \\log \\prod_k p(x_n,\\mathcal{C}_{k}|\\theta)^{y_{nk}} \\;\\;\\text{(use 1-of-K coding)} \\\\\n",
    "  &=  \\sum_{n,k} y_{nk} \\log p(x_n,\\mathcal{C}_{k}|\\theta) \\\\\n",
    "   &=  \\sum_{n,k} y_{nk}  \\log\\mathcal{N}(x_n|\\mu_k,\\Sigma)  +  \\sum_{n,k} y_{nk} \\log \\pi_k \\\\\n",
    "   &=  \\sum_{n,k} y_{nk} \\underbrace{ \\log\\mathcal{N}(x_n|\\mu_k,\\Sigma) }_{ \\text{Gaussian} } + \\underbrace{ \\sum_k m_k \\log \\pi_k }_{ \\text{multinomial} } \n",
    "\\end{align*}$$\n",
    "where we used $m_k \\triangleq \\sum_n y_{nk}$.\n",
    "\n",
    "- As usual, the rest (inference for parameters and model prediction) through straight probability theory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2 -  Parameter Inference for Classification\n",
    "\n",
    "We'll do ML estimation of $\\theta = \\{ \\pi_k, \\mu_k, \\Sigma \\}$ from data $D$\n",
    "\n",
    "-  Recall (from the previous slide) the log-likelihood\n",
    "$$\n",
    "\\log\\, p(D|\\theta) =  \\sum_{n,k} y_{nk} \\underbrace{ \\log\\mathcal{N}(x_n|\\mu_k,\\Sigma) }_{ \\text{Gaussian} } + \\underbrace{ \\sum_k m_k \\log \\pi_k }_{ \\text{multinomial} } \n",
    "$$\n",
    "\n",
    "- Maximization of the LLH breaks down into\n",
    "  -  **Gaussian density estimation** for parameters $\\mu_k, \\Sigma$, since the first term contains exactly the LLH for MVG density estimation (see lesson on Density Est., Eq.1) \n",
    "  - **Multinomial density estimation** for class priors $\\pi_k$, since the second term holds exactly the LLH for multinomial density estimation (see lesson on Density Estimation, Eq.2). \n",
    "  \n",
    "It follows (by similar computations) that\n",
    "\n",
    "- ML for multinomial prior (we've done this before!)\n",
    "\\begin{align}   \n",
    "\\hat \\pi_k = m_k/N \\tag{class prior}\n",
    "\\end{align}\n",
    "\n",
    "- Now group the data into separate classes and do MVG ML estimation for class-conditional parameters (we've done this as well).\n",
    "\n",
    "$$\\begin{align*}\n",
    " \\hat \\mu_k &= \\frac{ \\sum_n y_{nk} x_n} { \\sum_n y_{nk} } = \\frac{1}{m_k} \\sum_n y_{nk} x_n \\tag{class-cond. mean}\\\\\n",
    " \\hat \\Sigma  &= \\frac{1}{N} \\sum_{n,k} y_{nk} (x_n-\\hat \\mu_k)(x_n-\\hat \\mu_k)^T \\tag{variance}\\\\\n",
    "  &= \\sum_k \\hat \\pi_k \\cdot \\underbrace{ \\left( \\frac{1}{m_k} \\sum_{n} y_{nk} (x_n-\\hat \\mu_k)(x_n-\\hat \\mu_k)^T  \\right) }_{ \\text{class-cond. variance} } \\\\\n",
    "  &= \\sum_k \\hat \\pi_k \\cdot \\hat \\Sigma_k\n",
    "\\end{align*}$$\n",
    "\n",
    "- Note that $y_{nk}$ groups data from the same class.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  3 - Application: Class prediction for new Data\n",
    "\n",
    "-  Given a 'new' input $x$, use Bayes rule to get posterior class probability\n",
    "$$\\begin{align*}\n",
    " p(\\mathcal{C}_k|x,\\theta ) &\\propto p(\\mathcal{C}_k) \\,p(x|\\mathcal{C}_k) \\\\\n",
    "  &\\propto \\pi_k \\exp \\left\\{ { - {\\frac{1}{2}}(x - \\mu_k )^T \\Sigma^{ - 1} (x - \\mu_k )} \\right\\}\\\\\n",
    "  &\\propto \\exp \\left\\{ {\\mu_k^T \\Sigma^{ - 1} x - {\\frac{1}{2}}\\mu_k^T \\Sigma^{ - 1} \\mu_k  + \\log \\pi_k } \\right\\}  \\\\\n",
    "  &=  \\exp\\{\\beta_k^T x + \\gamma_k\\}\n",
    "\\end{align*}$$\n",
    "where \n",
    "$$\\begin{align*}\n",
    "\\beta_k &= \\Sigma^{-1} \\mu_k \\\\\n",
    "\\gamma_k &= - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k  + \\log \\pi_k \\,.\n",
    "\\end{align*}$$\n",
    "\n",
    "- The class posterior $\\frac{\\exp(a_k)}{\\sum_{k^\\prime}\\exp(a_{k^\\prime})}$ is called a **softmax** function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Discrimination Boundaries\n",
    "\n",
    "-  The class posterior $\\log p(\\mathcal{C}_k|x) \\propto \\beta_k^T x + \\gamma_k$ is a linear function of the input features.\n",
    "\n",
    "-  Thus, the contours of equal probability (**discriminant functions**) are lines (hyperplanes) in feature space\"\n",
    "$$\n",
    "\\log \\frac{{p(\\mathcal{C}_k|x,\\theta )}}{{p(\\mathcal{C}_j|x,\\theta )}} = \\beta_{kj}^T x + \\gamma_{kj} = 0\n",
    "$$\n",
    "where we defined $\\beta_{kj} \\triangleq \\beta_k - \\beta_j$ and similarly for $\\gamma_{kj}$.\n",
    "\n",
    "-  (homework). What happens if we had not assumed class-independent variances $\\Sigma_k=\\Sigma$? Are the discrimination functions still linear? quadratic?\n",
    "\n",
    "-  How to classify a new input $x_\\bullet$? The Bayesian answer is a posterior distribution $ p(\\mathcal{C}_k|x_\\bullet)$. If you must choose, then the class with maximum posterior class probability\n",
    "$$\\begin{align*}\n",
    "k^* &= \\arg\\max_k p(\\mathcal{C}_k|x_\\bullet) \\\\\n",
    "  &= \\arg\\max_k \\left( \\beta _k^T x_\\bullet + \\gamma_k \\right)\n",
    "\\end{align*}$$\n",
    "is an appealing decision. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:red\"> MARCO Can you solve the problem now in Julia, eg, compute parameters with formula's above and predict  $p(x_\\bullet \\in \\mathcal{C}_{\\text{apple}})$. Also plot the discrimination boundary</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Recap Generative Classification\n",
    "\n",
    "- Model spec. $p(x,\\mathcal{C}_k|\\,\\theta) = \\pi_k \\cdot \\mathcal{N}(x|\\mu_k,\\Sigma)$\n",
    "\n",
    "- If the class-conditional distributions are Gaussian with equal covariance matrices across classes ($\\Sigma_k = \\Sigma$), then\n",
    "    the discriminant functions are hyperplanes in feature space.\n",
    "\n",
    "- ML estimation for $\\{\\pi_k,\\mu_k,\\Sigma\\}$ breaks down to simple density estimation for Gaussian and multinomial.\n",
    "\n",
    "- Posterior class probability is a softmax function\n",
    "$$ p(\\mathcal{C}_k|x,\\theta ) \\propto \\exp\\{\\beta_k^T x + \\gamma_k\\}$$\n",
    "where $\\beta _k= \\Sigma^{-1} \\mu_k$ and $\\gamma_k=- \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k  + \\log \\pi_k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----\n",
    "_The cell below loads the style file_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.cell { /* set cell width */\n",
       "    width: 750px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    width: 1000px;\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:600px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.css\") do f\n",
    "    display(\"text/html\", readall(f))\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
