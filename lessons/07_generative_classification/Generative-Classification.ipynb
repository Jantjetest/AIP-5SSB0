{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Classification\n",
    "\n",
    "\\only<presentation>{\n",
    "  \\includegraphics[height=8cm]<1>{./figures/fig-classification-generative-1}\n",
    "  \\includegraphics[height=8cm]<2>{./figures/fig-classification-generative-2}\n",
    "  \\includegraphics[height=8cm]<3>{./figures/fig-classification-generative-3}\n",
    "}\n",
    "\\includegraphics[height=8cm]<4->{./figures/fig-classification-generative-4}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Classification Problem Statement\n",
    "Given is data  $D = \\{(x_1,y_1),\\dotsc,(x_N,y_N)\\}$\n",
    "  - inputs $x_n \\in \\Re^D$ are called **features**.\n",
    "  - outputs $y_n \\in \\mathcal{C}_k$, with $k=1,\\ldots,K$; The **discrete** targets $\\mathcal{C}_k$ are called **classes**.\n",
    "\n",
    "We will again use the 1-of-$K$ notation for the discrete classes. Define the binary **class selection variable**\n",
    "$$\n",
    "y_{nk} = \\begin{cases} 1 & \\text{if $y_n$ in class $\\mathcal{C}_k$}\\\\\n",
    "        0 & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "- Supervised learning goal: build a model for the joint $$p(x,y)= p(x|y)p(y)$$\n",
    "-  The plan for generative classification: model spec for the joint pdf $p(x|y)p(y)$ and use Bayes to infer the posterior class probabilities \n",
    "$$\n",
    "p(y|x) = \\frac{p(x|y) p(y)}{\\sum_{y} p(x|y) p(y)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model specification\n",
    "\n",
    "-  **Likelihood**. Assume Gaussian **class-conditional distributions** with **constant covariance matrix** across the classes,\n",
    " $$\n",
    " p(x|\\mathcal{C}_k,\\theta) = \\mathcal{N}(x|\\mu_k,\\Sigma)\n",
    " $$\n",
    "\n",
    "with notational shorthand: $\\mathcal{C}_k \\equiv (y_k=1)$\n",
    "\n",
    "- **Prior** on class labels $y_k$ is multinomial $$p(\\mathcal{C}_k|\\pi) = \\pi_k$$\n",
    "\n",
    "- This leads to\n",
    "$$\n",
    " p(x,\\mathcal{C}_k) =  \\pi_k \\cdot \\mathcal{N}(x|\\mu_k,\\Sigma)\n",
    "$$ \n",
    "- As usual, the rest (inference for parameters and model prediction) through straight probability theory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Parameter Inference for Classification\n",
    "\n",
    "-  Goal: ML estimation of $\\theta = \\{ \\pi_k, \\mu_k, \\Sigma \\}$ from data $D$\n",
    "\n",
    "-  We will maximize the _joint_ log-likelihood $\\sum_n \\log p(x_n,y_n|\\theta)$,\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(D|\\theta) &= \\sum_n \\log \\prod_k p(x_n,y_{nk}|\\theta)^{y_{nk}} \\\\\n",
    "   &=  \\sum_{n,k} y_{nk} \\log  p(x_n,y_{nk}|\\theta)\\\\\n",
    "   &=  \\sum_{n,k} y_{nk} \\underbrace{ \\log\\mathcal{N}(x_n|\\mu_k,\\sigma) }_{ \\text{Gaussian dens. est.} } + \\underbrace{ \\sum_{n,k} y_{nk} \\log \\pi_k }_{ \\text{multinom. est.} } \n",
    "\\end{align}\n",
    "\n",
    "-  Problem breaks down into\n",
    "  -  **Multinomial density estimation** for class priors $\\pi_k$\n",
    "  -  **Gaussian density estimation** for parameters $\\mu_k, \\Sigma$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Estimation for Generative Classification\n",
    "\n",
    "Prior is ML for multinomial (done this before!)\n",
    "\\begin{align}   \n",
    "\\hat \\pi_k = N_k/N \\tag{class prior}\n",
    "\\end{align}\n",
    "\n",
    "Now group the data into separate classes and do MVG ML for class-conditional parameters (done this as well).\n",
    "\\begin{align}\n",
    " \\hat \\mu_k &= \\frac{ \\sum_n y_{nk} x_n} { \\sum_n y_{nk} } = \\frac{1}{N_k} \\sum_n y_{nk} x_n \\tag{class-cond. mean}\\\\\n",
    " \\hat \\Sigma  &= \\frac{1}{N} \\sum_{n,k} y_{nk} (x_n-\\hat \\mu_k)(x_n-\\hat \\mu_k)^T \\tag{variance}\\\\\n",
    "  &= \\sum_k \\hat \\pi_k \\cdot \\underbrace{ \\left( \\frac{1}{N_k} \\sum_{n} y_{nk} (x_n-\\hat \\mu_k)(x_n-\\hat \\mu_k)^T  \\right) }_{ \\text{class-cond. variance} }\n",
    "\\end{align}\n",
    "\n",
    "Note that $y_{nk}$ groups data from the same class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Model Prediction for Generative Classification\n",
    "\n",
    "-  Given a `new' input $x$, use Bayes rule to get posterior class probability\n",
    "\n",
    "\\begin{align}\n",
    " p(\\mathcal{C}_k|x,\\theta ) &= \\frac{{p(x|\\mathcal{C}_k ,\\theta )p(\\mathcal{C}_k |\\pi)}}{{\\sum_j {p(x|\\mathcal{C}_j ,\\theta )p(\\mathcal{C}_j |\\pi)} }}  \\\\\n",
    "  &= \\frac{{\\pi_k \\exp \\left\\{ { - {\\frac{1}{2}}(x - \\mu_k )^T \\Sigma^{ - 1} (x - \\mu_k )} \\right\\}}}{{\\sum_j {\\pi_j \\exp \\left\\{ { - {\\frac{1}{2}}(x - \\mu_j )^T \\Sigma^{ - 1} (x - \\mu_j )} \\right\\}} }} \\\\\n",
    "  &= \\frac{{\\exp \\left\\{ {\\mu_k^T \\Sigma^{ - 1} x - {\\frac{1}{2}}\\mu_k^T \\Sigma^{ - 1} \\mu_k  + \\log \\pi_k } \\right\\}}}{{\\sum_j {\\exp \\left\\{ {\\mu_j^T \\Sigma^{ - 1} x - {\\frac{1}{2}}\\mu_j^T \\Sigma^{ - 1} \\mu_j  + \\log \\pi_j } \\right\\}} }}  \\\\\n",
    "  &=  \\exp\\{\\beta_k^T x + \\gamma_k\\}/Z\n",
    "\\end{align}\n",
    "\n",
    "where $\\beta_k = \\Sigma^{-1} \\mu_k$, and $\\gamma_k = - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k  + \\log \\pi_k$ and $Z=\\sum_j \\mathrm{exp}\\{\\beta_j^T x + \\gamma_j\\}$ is a normalizing factor.\n",
    "%-  Note how the class priors are represented in the posterior class probabilities.\n",
    "\n",
    "Note that the (softmax)} function $\\phi(a_k) = \\frac{\\mathrm{exp}\\{a_k\\}}{\\sum_j \\mathrm{exp}\\{a_j\\}}$ is by construction normalized, $\\sum_k \\phi(_k)=1$ .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Discrimination Boundaries\n",
    "\n",
    "-  The class posterior $\\log p(\\mathcal{C}_k|x,\\theta)= \\beta_k^T x + \\gamma_k - \\log Z$ is a linear function of the input features.\n",
    "\n",
    "-  Thus, the contours of equal probability (**discriminant functions**) are lines (hyperplanes) in feature space\n",
    "$$\n",
    "\\log \\frac{{p(\\mathcal{C}_k|x,\\theta )}}{{p(\\mathcal{C}_j|x,\\theta )}} = \\beta_{kj}^T x + \\gamma_{kj} = 0\n",
    "$$\n",
    "where we defined $\\beta_{kj}=\\beta_k - \\beta_j$ and similarly for $\\gamma_{kj}$.\n",
    "\n",
    "-  (homework). What happens if we had not assumed class-independent variances $\\Sigma_k=\\Sigma$? Are the discrimination functions still linear? quadratic?\n",
    "\n",
    "-  How to apply a trained classifier to a classification problem? E.g., choose class with\n",
    "maximum posterior class probability\n",
    "\\begin{align}\n",
    "k^* &= \\arg\\max_k p(\\mathcal{C}_k|x_{new},\\theta) \\\\\n",
    "  &= \\arg\\max_k \\left( \\beta _k^T x_{new} + \\gamma_k \\right)\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Binary Classification\n",
    "\n",
    "Special case example: binary classification ($y \\in \\{0,1\\}$)\n",
    "\\begin{align}\n",
    "p(y=1|x,\\theta) &= \\frac{\\mathrm{exp}\\{\\beta_1^T x + \\gamma_1\\}} {\\mathrm{exp}\\{\\beta_1^T x + \\gamma_1\\} + \\mathrm{exp}\\{\\beta_0^T x + \\gamma_0\\}} \\\\\n",
    "  & = \\frac{1}{1+\\mathrm{E}\\{-\\left(\\beta_{10}^T x + \\gamma_{10}\\right)\\}} \\\\\n",
    " p(y=0|x,\\theta) &= 1- p(y=1|x,\\theta) = \\frac{1}{1+\\mathrm{E}\\{-\\left(\\beta_{01}^T x + \\gamma_{01}\\right)\\}}\n",
    "\\end{align}\n",
    "\n",
    "-   The function $\\phi(a) \\equiv 1/(1+e^{-a})$ is called the **logistic** function, which is the binary case of the **softmax** function.\n",
    "\n",
    "The Logistic Function as Posterior Probability}\n",
    "\\begin{center}\n",
    "\\includegraphics[height=8cm]{./figures/fig-logistic}\n",
    "\\end{center}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Recap Generative Classification\n",
    "\n",
    "- Model spec. $p(x,\\mathcal{C}_k|\\,\\theta) = \\pi_k \\cdot \\mathcal{N}(x|\\mu_k,\\Sigma)$\n",
    "- If the class-conditional distributions are Gaussian with equal covariance matrices across classes ($\\Sigma_k = \\Sigma$), then\n",
    "    the discriminant functions are hyperplanes in feature space.\n",
    "- ML estimation for $\\{\\pi_k,\\mu_k,\\Sigma\\}$ breaks down to simple density estimation for Gaussian and multinomial.\n",
    "- Posterior class probability is a softmax (or logistic) function\n",
    "$$ p(\\mathcal{C}_k|x,\\theta ) = \\mathrm{exp}\\{\\beta_k^T x + \\gamma_k\\}/Z$$\n",
    "where $\\beta _k= \\Sigma^{-1} \\mu_k$ and $\\gamma_k=- \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k  + \\log \\pi_k$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.6",
   "language": "julia",
   "name": "julia 0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
