{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Simple maximum likelihood estimates for Gaussian and Bernoulli/categorical distributions\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional\n",
    "    - Bishop pp. 67-70, 74-76, 93-94      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Density Estimation?\n",
    "\n",
    "Density estimation relates to building a model $p(x|\\theta)$ from observations $D=\\{x_1,\\dotsc,x_N\\}$. \n",
    "\n",
    "Why is this interesting? Some examples:\n",
    "\n",
    "- **Outlier detection**. Suppose $D=\\{x_n\\}$ are benign mammogram images. Build $p(x | \\theta)$ from $D$. Then low value for $p(x^\\prime | \\theta)$ indicates that $x^\\prime$ is a risky mammogram.\n",
    "\n",
    "\n",
    "- **Compression**. Code a new data item based on **entropy**, which is a functional of $p(x|\\theta)$: \n",
    "$$\n",
    "H[p] = -\\sum_x p(x | \\theta)\\log p(x |\\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "- **Classification**. Let $p(x | \\theta_1)$ be a model of attributes $x$ for credit-card holders that paid on time and $p(x | \\theta_2)$ for clients that defaulted on payments. Then, assign a potential new client $x^\\prime$ to either class based on the relative probability of $p(x^\\prime | \\theta_1)$ vs. $p(x^\\prime|\\theta_2)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Log-Likelihood for a Multivariate Gaussian (MVG)\n",
    "\n",
    "Assume we are given a set of IID data points $D=\\{x_1,\\ldots,x_N\\}$, where $x_n \\in \\Re^D$. We want to build a model for these data.\n",
    "\n",
    "**Model specification**. Let's assume a MVG model $x_n=\\mu+\\epsilon_n$ with $\\epsilon_n \\sim \\mathcal{N}(0,\\Sigma)$, or equivalently,\n",
    "\n",
    "$$\n",
    "p(x|\\mu,\\Sigma) = |2 \\pi \\Sigma|^{-\\frac{1}{2}} \\mathrm{exp} \\left\\{-\\frac{1}{2}(x_n-\\mu)^T\n",
    "\\Sigma^{-1} (x_n-\\mu) \\right\\}\n",
    "$$\n",
    "\n",
    "Since the data are IID, $p(D|\\theta)$ factorizes as\n",
    "$$  \n",
    "p(D|\\theta) = p(x_1,\\ldots,x_N|\\theta) \\stackrel{\\text{IID}}{=} \\prod_n p(x_n|\\theta)\n",
    "$$\n",
    "\n",
    "This choice of model yields the following log-likelihood (use (B-C.9) and (B-C.4)),\n",
    "\n",
    "$$\\begin{align*}\n",
    " \\log p(D|\\theta) &= \\log \\prod_n p(x_n|\\theta) = \\sum_n \\log p(x_n|\\theta) \\\\\n",
    "     &= N \\cdot \\log | 2\\pi\\Sigma |^{-1/2} - \\frac{1}{2} \\sum\\nolimits_{n} (x_n-\\mu)^T \\Sigma^{-1} (x_n-\\mu)\n",
    " \\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood estimation of mean of MVG\n",
    "\n",
    "We want to maximize $\\log p(D|\\theta)$ wrt the parameters $\\theta=\\{\\mu,\\Sigma\\}$. Let's take derivatives; first to mean $\\mu$, (making use of  (B-C.25) and (B-C.27)),\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla_\\mu \\log p(D|\\theta) &= -\\frac{1}{2}\\sum_n \\nabla_\\mu \\left[ (x_n-\\mu)^T \\Sigma^{-1} (x_n-\\mu) \\right] \\\\\n",
    "&= -\\frac{1}{2}\\sum_n \\nabla_\\mu \\mathrm{Tr} \\left[ -2\\mu^T\\Sigma^{-1}x_n + \\mu^T\\Sigma^{-1}\\mu \\right]  \\\\\n",
    "&= -\\frac{1}{2}\\sum_n \\left( -2\\Sigma^{-1}x_n + 2\\Sigma^{-1}\\mu \\right) \\\\\n",
    "&= \\Sigma^{-1}\\,\\sum_n \\left( x_n-\\mu \\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "Set to zero yields the **sample mean**\n",
    "$$\\begin{equation*}\n",
    "\\boxed{\n",
    "\\hat \\mu = \\frac{1}{N} \\sum_n x_n\n",
    "}\n",
    "\\end{equation*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Maximum Likelihood estimation of variance of MVG\n",
    "\n",
    "Now we take the gradient of the log-likelihood **wrt the precision matrix** $\\Sigma^{-1}$ (making use of B-C.28 and B-C.24)\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla_{\\Sigma^{-1}}  &\\log p(D|\\theta) \\\\\n",
    "&= \\nabla_{\\Sigma^{-1}} \\left[ \\frac{N}{2} \\log |2\\pi\\Sigma|^{-1} - \\frac{1}{2} \\sum_{n=1}^N (x_n-\\mu)^T \\Sigma^{-1} (x_n-\\mu)\\right] \\\\\n",
    "&= \\nabla_{\\Sigma^{-1}} \\left[ \\frac{N}{2} \\log |\\Sigma^{-1}| - \\frac{1}{2} \\sum_{n=1}^N \\mathrm{Tr} \\left[ (x_n-\\mu) (x_n-\\mu)^T \\Sigma^{-1}\\right] \\right]\\\\\n",
    "&= \\frac{N}{2}\\Sigma -\\frac{1}{2}\\sum_n (x_n-\\mu)(x_n-\\mu)^T\n",
    "\\end{align*}$$\n",
    "\n",
    "Get optimum by setting the gradient to zero,\n",
    "$$\\begin{equation*}\n",
    "\\boxed{\n",
    "\\hat \\Sigma = \\frac{1}{N} \\sum_n (x_n-\\hat\\mu)(x_n - \\hat\\mu)^T}\n",
    "\\end{equation*}$$\n",
    "which is the **sample variance**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">(OPTIONAL SLIDE)</span> Sufficient Statistics\n",
    "\n",
    "Note that the ML estimates can also be written as\n",
    "$$\\begin{equation*}\n",
    "\\hat \\Sigma = \\sum_n x_n x_n^T - \\left( \\sum_n x_n\\right)\\left( \\sum_n x_n\\right)^T, \\quad \\hat \\mu = \\frac{1}{N} \\sum_n x_n\n",
    "\\end{equation*}$$\n",
    "\n",
    "I.o.w., the statistics (a 'statistic' is a function of the data) $\\sum_n x_n$ and $\\sum_n x_n x_n^T$ are sufficient to estimate the parameters $\\mu$ and $\\Sigma$ from observations. In the literature, $\\sum_n x_n$ and  $\\sum_n x_n x_n^T$ are called **sufficient statistics**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Data: the 1-of-K Coding Scheme\n",
    "\n",
    "- Consider a coin-tossing experiment with outcomes $x \\in\\{0,1\\}$ (corresponding to tail and head, resp.) and $0\\leq \\mu \\leq 1$ the probability of heads. This model can written as a **Bernoulli distribution**:\n",
    "$$ \n",
    "p(x|\\mu) = \\mu^{x}(1-\\mu)^{1-x}\n",
    "$$\n",
    "  - Note that in expression $\\mu^{x}(1-\\mu)^{1-x}$, the variable $x$ acts as a (binary) **selector** for the tail or head probabilities. Think of this as an 'if'-statement in programming. \n",
    "\n",
    "\n",
    "- **1-of-K scheme**. Now consider a $K$-sided coin, a.k.a. a _die_ (pl.: dice). It will be very convenient to code the outcomes by a vector $x=(x_1,\\ldots,x_K)^T$ with **binary selection variables**\n",
    "$$\n",
    "x_k = \\begin{cases} 1 & \\text{if die landed on $k$th face}\\\\\n",
    "0 & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "  - E.g., For $K=6$, if the die lands on the 3rd face, we encode that as $x=(0,0,1,0,0,0)^T$.\n",
    "\n",
    "- Assume the probabilities $p(x_k=1) = \\mu_k$ with  $\\sum_k \\mu_k  = 1$. The data generating distribution is then (note the similarity to the Bernoulli distribution)\n",
    "$$\n",
    "p(x|\\mu) = \\mu_1^{x_1} \\mu_2^{x_2} \\cdots \\mu_k^{x_k}=\\prod_k \\mu_k^{x_k}\n",
    "$$\n",
    "\n",
    "\n",
    "- This generalized Bernoulli distribution is called the **categorical distribution** (or sometimes the 'multi-noulli' distribution).\n",
    "\n",
    "\n",
    "- Note that $\\sum_k x_k = 1$ and verify for yourself that $\\mathrm{E}[x|\\mu] = \\mu$.\n",
    "\n",
    "\n",
    "- In these notes, we use the superscript to indicate that we are working with a **binary selection variable** in a 1-of-$K$ scheme.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Categorical vs. Multinomial Distribution\n",
    "\n",
    "- Observe a data set $D=\\{x_1,\\ldots,x_N\\}$  of $N$ IID rolls of a $K$-sided die, with generating PDF\n",
    "$$\n",
    "p(D|\\mu) = \\prod_n \\prod_k \\mu_k^{x_{nk}} = \\prod_k \\mu_k^{\\sum_n x_{nk}} = \\prod_k \\mu_k^{m_k}\n",
    "$$\n",
    "where $m_k= \\sum_n x_{nk}$ is the total number of occurrences that we 'threw' $k$ eyes.\n",
    "\n",
    "- This distribution depends on the observations **only** through the quantities $\\{m_k\\}$, with generally $K \\ll N$. \n",
    "\n",
    "- A related distribution is the distribution over $D_m=(m_1,\\ldots,m_K)^T$, which is called the **multinomial distribution**,\n",
    "$$\n",
    "p(D_m|\\mu) =\\frac{N!}{m_1! m_2!\\ldots m_K!} \\,\\prod_k \\mu_k^{m_k}\\,.\n",
    "$$\n",
    "\n",
    "- $p(D|\\mu)$ is a distribution over the individual observations, whereas $p(D_m|\\mu)$ is a distribution over the data frequencies $m_k$.\n",
    "  - Note that $p(D|\\mu)$ and $p(D_m|\\mu)$ differ only in the normalization factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">MARCO PLs insert an exampel to show the difference between teh multi-noulli and the multi-nomial dsitributions. See above for diference</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Maximum Likelihood Estimation for the Multinomial\n",
    "\n",
    "Now let's find the ML estimate for $\\mu$, based on $N$ throws of a $K$-sided die. \n",
    "\n",
    "- The log-likelihood with Lagrange multiplier to include the constraint is\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathrm{L}^\\prime &\\triangleq \\log \\prod_k \\mu_k^{m_k}  + \\lambda \\cdot(1 - \\sum_k \\mu_k ) \\\\\n",
    "\t&= \\sum_k m_k \\log \\mu_k  + \\lambda \\cdot (1 - \\sum_k \\mu_k )\\,.\n",
    "\\end{align*}$$\n",
    "\n",
    "- Set derivative to zero yields the **sample proportion** for $\\mu_k$ \n",
    "\n",
    "$$\\begin{equation*}\n",
    "\\nabla_{\\mu_k}   \\mathrm{L}^\\prime = \\frac{m_k }\n",
    "{\\hat\\mu_k } - \\lambda  \\overset{!}{=} 0 \\; \\Rightarrow \\; \\boxed{\\hat\\mu_k  = \\frac{m_k }\n",
    "{\\lambda } = \\frac{m_k }{N}}\n",
    "\\end{equation*}$$\n",
    "\n",
    "  where we get $\\lambda$ from the constraint \n",
    "\n",
    "$$\\begin{equation*}\n",
    "\\sum_k \\hat \\mu_k = \\sum_k \\frac{m_k}\n",
    "{\\lambda} = \\frac{N}{\\lambda} \\overset{!}{=}  1\n",
    "\\end{equation*}$$\n",
    "\n",
    "- Compare this answer to Laplace's rule for predicting the next coin toss $\\left( p(h|D)=(N_h+1)/(N+2) \\right)$\n",
    "\n",
    "- Interesting special case: **Binomial** (=$N$ coin tosses); \n",
    "$$p(x_n|\\theta)= \\theta^{x_n^h}(1-\\theta)^{1-x_n^h}=\\theta_h^{x_n^h} \\theta_t^{x_n^t}\n",
    "$$ \n",
    "yields $$ \\hat \\theta = \\frac{N_h}{N_h +N_t} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Recap ML for Density Estimation\n",
    "\n",
    "Given $N$ IID observations $D=\\{x_1,\\dotsc,x_N\\}$\n",
    "\n",
    "- For a **multivariate Gaussian** model $p(x_n|\\theta) = \\mathcal{N}(x_n|\\mu,\\Sigma)$, we obtain ML estimates\n",
    "\n",
    "$$\\begin{align}\n",
    "\\hat \\mu &= \\frac{1}{N} \\sum_n x_n \\tag{sample mean} \\\\\n",
    "\\hat \\Sigma &= \\frac{1}{N} \\sum_n (x_n-\\hat\\mu)(x_n - \\hat \\mu)^T \\tag{sample variance}\n",
    "\\end{align}$$\n",
    "\n",
    "- For discrete outcomes modeled by a 1-of-K **categorical distribution** we find\n",
    "\n",
    "$$\\begin{align}\n",
    "\\hat\\mu_k  = \\frac{1}{N} \\sum_n x_{nk} \\quad \\left(= \\frac{m_k}{N} \\right) \\tag{sample proportion}\n",
    "\\end{align}$$\n",
    "  \n",
    "  \n",
    "- Note the similarity for the means between discrete and continuous data. \n",
    "\n",
    "- We didn't use a co-variance matrix for discrete data. Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "_The cell below loads the style file_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.cell { /* set cell width */\n",
       "    width: 750px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    width: 1000px;\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:600px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.css\") do f\n",
    "    display(\"text/html\", readall(f))\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
