{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Gaussians\n",
    "======="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Review of processing of Gaussian distributions in linear systems\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional\n",
    "    - Bishop pp. 85-93       \n",
    "    - [MacKay - 2006 - The Humble Gaussian Distribution](./files/Mackay-2006-The-humble-Gaussian-distribution.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sums and Transformations of Gaussian Variables\n",
    "\n",
    "- The Gaussian distribution\n",
    "$$\n",
    "\\mathcal{N}(x|\\mu,\\Sigma) = |2 \\pi \\Sigma |^{-\\frac{1}{2}} \\,\\mathrm{exp}\\left\\{-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\right\\}\n",
    "$$\n",
    "for variable $x$ is completely specified by its mean $\\mu$ and variance $\\Sigma$. \n",
    "\n",
    "- $\\Lambda = \\Sigma^{-1}$ is called the **precision matrix**.\n",
    "\n",
    "- The sum of two Gaussian _distributions_ is NOT Gaussian. Why not?\n",
    "\n",
    "- A **linear transformation** $z=Ax+b$ of a Gaussian variable $\\mathcal{N}(x|\\mu,\\Sigma)$ is Gaussian distributed as\n",
    "\n",
    "$$\n",
    "p(z) = \\mathcal{N} \\left(z|A\\mu+b, A\\Sigma A^T \\right) \\tag{SRG-4a}\n",
    "$$\n",
    "\n",
    "- The **sum of two independent Gaussian variables** is also Gaussian distributed. Specifically, if $x \\sim \\mathcal{N} \\left(x|\\mu_x, \\Sigma_x \\right)$ and $y \\sim \\mathcal{N} \\left(y|\\mu_y, \\Sigma_y \\right)$, then the PDF for $z=x+y$ is given by\n",
    "\n",
    "\\begin{align}\n",
    "p(z) &= \\mathcal{N}(x|\\mu_x,\\Sigma_x) \\ast \\mathcal{N}(y|\\mu_y,\\Sigma_y) \\notag\\\\\n",
    "  &= \\mathcal{N} \\left(z|\\mu_x+\\mu_y, \\Sigma_x +\\Sigma_y \\right) \\tag{SRG-8}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Gaussian Signals in a Linear System\n",
    "\n",
    "<img src=\"./figures/fig-linear-system.png\" width=300px>\n",
    "\n",
    "- [Q.]: Given independent variables\n",
    "$x \\sim \\mathcal{N}(\\mu_x,\\sigma_y)$ and $y \\sim \\mathcal{N}(\\mu_y,\\sigma_y)$, what is the PDF for $z = A\\cdot(x -y) + b$ ?\n",
    "\n",
    "- [A.]: $z$ is also Gaussian with \n",
    "$$\n",
    "p_z(z) = \\mathcal{N}(z|A(\\mu_x-\\mu_y)+b, \\, A(\\sigma_x \\mathbf{+} \\sigma_y)A^T)\n",
    "$$\n",
    "\n",
    "- Think about the role of the Gaussian distribution for stochastic linear systems in relation to what sinusoidals mean for deterministic linear system analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Bayesian Estimation of a Constant\n",
    "\n",
    "[Question]\n",
    "\n",
    "- Estimate a constant $\\theta$ from one 'noisy' measurement $x$ about that constant. Assume the following model specification:\n",
    "     \n",
    "$$\\begin{align*}\n",
    "x &= \\theta + \\epsilon \\\\\n",
    "\\theta &\\sim \\mathcal{N}(\\theta|\\mu_\\theta,\\sigma_\\theta^2) \\\\\n",
    "\\epsilon &\\sim \\mathcal{N}(\\epsilon|0,\\sigma^2_{\\epsilon})\n",
    "\\end{align*}$$\n",
    "\n",
    "[Answer]\n",
    "\n",
    "1. **Model specification**\n",
    "Note that you can rewrite these specifications in probabilistic notation as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "    p(x|\\theta) &=\\mathcal{N}(x|\\theta,\\sigma^2_{\\epsilon}) \\tag{likelihood}\\\\\n",
    "    p(\\theta) &=\\mathcal{N}(\\theta|\\mu_\\theta,\\sigma_\\theta^2) \\tag{prior}\n",
    "\\end{align}$$\n",
    "\n",
    "2. **Inference** for the posterior PDF $p(\\theta|x)$\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\theta|x)  &= \\frac{p(x|\\theta)p(\\theta)}{p(x)} = \\frac{p(x|\\theta)p(\\theta)} { \\int p(x|\\theta)p(\\theta) \\, \\mathrm{d}\\theta } \\notag \\\\\n",
    "    &= \\frac{1}{C} \\,\\mathcal{N}(x|\\theta,\\sigma^2_{\\epsilon})\\, \\mathcal{N}(\\theta|\\mu_\\theta,\\sigma_\\theta^2) \\notag \\\\\n",
    "    &= \\frac{1}{C_1} \\mathrm{exp} \\left\\{ -\\frac{(x-\\theta)^2}{2\\sigma^2_{\\epsilon}} - \\frac{(\\theta-\\mu_\\theta)^2}{2\\sigma_\\theta^2} \\right\\} \\notag \\\\\n",
    "    &= \\frac{1}{C_1} \\mathrm{exp} \\left\\{ \\theta^2\\left( -\\frac{1}{2\\sigma^2_{\\epsilon}} - \\frac{1}{2\\sigma_\\theta^2} \\right) + \\theta \\left( \\frac{x}{\\sigma^2_{\\epsilon}} + \\frac{\\mu_\\theta}{\\sigma_\\theta^2} \\right) +  C_2 \\right\\} \\notag \\\\\n",
    "    &= \\frac{1}{C_1} \\mathrm{exp} \\left\\{ -\\frac{\\sigma_\\theta^2 + \\sigma^2_{\\epsilon}}{2\\sigma_\\theta^2 \\sigma^2_{\\epsilon}} \\left( \\theta - \\frac{x\\sigma_\\theta^2 + \\mu_s\\sigma^2_{\\epsilon}}{\\sigma_\\theta^2 + \\sigma^2_{\\epsilon}} \\right)^2 + C_3  \\right\\}\n",
    "\\end{align*}$$\n",
    "\n",
    "- This computational 'trick' for multiplying two Gaussians is called **completing the square**. Compare the procedure to $$ax^2+bx+c_1 = a\\left(x+\\frac{b}{2a}\\right)^2+c_2$$\n",
    "\n",
    "- Hence, it follows that the posterior for $\\theta$ is\n",
    "$$\n",
    "    p(\\theta|x) = \\mathcal{N} (\\theta |\\, \\mu_{\\theta|x}, \\sigma_{\\theta|x}^2)\n",
    "$$\n",
    "where\n",
    "\n",
    "$$\\begin{align*}\n",
    "  \\sigma_{\\theta|x}^2  &= \\frac{\\sigma^2_{\\epsilon}\\sigma_\\theta^2}{\\sigma^2_{\\epsilon} + \\sigma_\\theta^2} = \\left( \\frac{1}{\\sigma_\\theta^2} + \\frac{1}{\\sigma^2_{\\epsilon}}\\right)^{-1} \\\\\n",
    "  \\mu_{\\theta|x}   &= \\sigma_{\\theta|x}^2 \\, \\left( \\frac{1}{\\sigma^2_{\\epsilon}}x + \\frac{1}{\\sigma_\\theta^2} \\mu_\\theta \\right) \n",
    "\\end{align*}$$\n",
    "\n",
    "- So, multiplication of two Gaussians yields another (unnormalized) Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"> MARCO please insert interactive example of Gaussian multiplications here </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Gaussian Multiplication\n",
    "- In general, the multiplication of two multi-variate Gaussians yields an (unnormalized) Gaussian, see [SRG-6]:\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(x|\\mu_a,\\Sigma_a) \\cdot \\mathcal{N}(x|\\mu_b,\\Sigma_b) = \\alpha \\cdot \\mathcal{N}(x|\\mu_c,\\Sigma_c)\n",
    "$$\n",
    "where\n",
    "$$\\begin{align*}\n",
    "\\Sigma_c &= \\left( \\Sigma_a^{-1} + \\Sigma_b^{-1} \\right)^{-1}\\\\\n",
    "\\mu_c &= \\Sigma_c \\left( \\Sigma_a^{-1}\\mu_a + \\Sigma_b^{-1}\\mu_b\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "and normalization constant $\\alpha = \\mathcal{N}(\\mu_a|\\, \\mu_b, \\sigma_a + \\sigma_b)$.\n",
    "\n",
    "- If we define the **precision** as $\\Lambda \\equiv \\Sigma^{-1}$, then we see that **precisions add** and **precision-weighted means add** too.\n",
    "- As we just saw, great application to Bayesian inference!\n",
    "\n",
    "$$\n",
    "\\underbrace{\\text{Gaussian}}_{\\text{posterior}}\n",
    " \\propto \\underbrace{\\text{Gaussian}}_{\\text{likelihood}} \\times \\underbrace{\\text{Gaussian}}_{\\text{prior}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditioning and Marginalization of a Gaussian\n",
    "\n",
    "Let $z = \\begin{bmatrix} x \\\\ y \\end{bmatrix}$ be jointly normal distributed as\n",
    "\n",
    "$$\n",
    "p(z|\\mu,\\Sigma) = \\mathcal{N} \\left( \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\left| \\begin{bmatrix} \\mu_x \\\\ \\mu_y \\end{bmatrix}, \n",
    "  \\begin{bmatrix} \\Sigma_x & \\Sigma_{xy} \\\\ \\sigma_{yx} & \\sigma_y \\end{bmatrix} \\right. \\right)\n",
    "$$\n",
    "\n",
    "- Note that the symmetry $\\Sigma=\\Sigma^T$ implies that $\\Sigma_x$ and $\\Sigma_y$ are symmetric and $\\Sigma_{xy} = \\Sigma_{yx}^T$.\n",
    "\n",
    "- Now let's factorize $p(x,y)$ into $p(y|x)\\, p(x)$ through conditioning and marginalization (for applications to Bayesian inference in jointly Gaussian systems).\n",
    "\n",
    "- **Marginalization**\n",
    "$$\n",
    "p(x) = \\int p(x,y)\\,\\mathrm{d}y = \\mathcal{N}\\left( x|\\mu_x, \\Sigma_x \\right), \\qquad p(y)=\\mathcal{N} \\left(y|\\mu_y, \\Sigma_y \\right)\n",
    "$$\n",
    "\n",
    "- **Conditioning**\n",
    "$$\\begin{align*}\n",
    "p(y|x) &= p(x,y)/p(x) \\\\\n",
    " &= \\mathcal{N}\\left(y|\\mu_y + \\Sigma_{yx}\\Sigma_x^{-1}(x-\\mu_x),\\, \\Sigma_y - \\Sigma_{yx}\\Sigma_x^{-1}\\Sigma_{xy} \\right)\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"> MARCO please insert interactive example of Gaussian marginalization and conditioning here </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Example: Conditioning of Gaussian\n",
    "\n",
    "Consider (again) the system \n",
    "\n",
    "$$\\begin{align*}\n",
    "x &= \\theta + \\epsilon \\\\\n",
    "\\theta &\\sim \\mathcal{N}(\\theta|\\mu_\\theta,\\sigma_\\theta^2) \\\\\n",
    "\\epsilon &\\sim \\mathcal{N}(\\epsilon|0,\\sigma^2_{\\epsilon})\n",
    "\\end{align*}$$\n",
    "\n",
    "- This system is equivalent to (derive this!)\n",
    "\n",
    "$$\n",
    "p(\\theta,x|\\,\\mu,\\sigma) = \\mathcal{N} \n",
    "  \\left( \n",
    "  \\begin{bmatrix} \\theta\\\\ x \\end{bmatrix} \n",
    "  \\left| \\begin{bmatrix} \\mu_\\theta\\\\ \\mu_\\theta\\end{bmatrix}, \n",
    "  \\begin{bmatrix} \\sigma_\\theta^2 & \\sigma_\\theta^2\\\\ \\sigma_\\theta^2 & \\sigma_\\theta^2+\\sigma_{\\epsilon}^2 \n",
    "  \\end{bmatrix} \n",
    "  \\right. \n",
    "  \\right)\n",
    "$$\n",
    "\n",
    "- Direct substitution of the rule for Gaussian conditioning:\n",
    "$$\\begin{align*}\n",
    "K &= \\frac{\\sigma_\\theta^2}{\\sigma_\\theta^2+\\sigma_{\\epsilon}^2} \\qquad \\text{($K$ is called: Kalman gain)}\\\\\n",
    "p(\\theta|x) &= \\mathcal{N} \\left( \\theta |\\, \\mu_\\theta + K \\cdot (x-\\mu_\\theta), \\, \\sigma_\\theta^2 \\left( 1-k \\right) \\right)\n",
    "\\end{align*}$$\n",
    "    \n",
    "- Exercises: (1) Actually derive this; (2) show that the result is equivalent to the previous slide on 'estimation of a constant'; and (3) Try to interpret the resulting formula's}\n",
    "- homework: Derive this result\n",
    "- Moral: For jointly Gaussian systems, we do inference simply in one step by using the formulas for conditioning and marginalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: Recursive Bayesian Estimation\n",
    "\n",
    "Now consider the signal $x(t)=\\theta+\\epsilon(t)$, where $D_t= \\left(x(1),\\ldots,x(t)\\right)$ is observed _sequentially_ (over time).\n",
    "\n",
    "- [Q.] We want a recursive algorithm for $p(\\theta|D_t)$.\n",
    "    \n",
    "- [A.] Again we assume prior $p(\\theta) = \\mathcal{N}(\\mu,\\sigma^2)$ and define $p(\\theta|D_t) = \\mathcal{N}(\\mu(t),\\sigma^2(t))$ \n",
    "        \n",
    "- We will solve this by using the estimate after $t-1$ as the **prior distribution** in conjunction with the **likelihood** for observation $x(t)$,\n",
    "$$\n",
    "p(\\mu(t)|D_t) \\propto p(x(t)|\\mu(t-1),\\sigma^2(t-1)) \\times p(\\mu(t)|D_{t-1})\n",
    "$$\n",
    "\n",
    "- Use the 'batch processing' posteriors for $\\mu$ and $\\sigma^2$ to get\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat \\mu(t) &= \\sigma_{\\mu}^2(t) \\, \\left( \\frac{1}{\\sigma^2_{\\epsilon}(t)}x(t) + \\frac{1}{\\sigma_{\\mu}^2(t-1)} \\hat \\mu(t-1) \\right) \\\\\n",
    "    &= \\frac{\\sigma^2_{\\epsilon}(t)}{\\sigma^2_{\\epsilon}(t)+\\sigma_{\\mu}^2(t-1)}x(t) + \\frac{\\sigma_{\\mu}^2(t-1)}{\\sigma^2_{\\epsilon}(t)+\\sigma_{\\mu}^2(t-1)} \\hat \\mu(t-1) \\\\\n",
    "    &= \\hat \\mu(t-1) + K(t) \\left[ x(t) - \\hat \\mu(t-1) \\right] \\\\\n",
    "\\sigma_{\\mu}^2(t) &= \\sigma_{\\mu}^2(t-1) \\frac{\\sigma^2_{\\epsilon}(t)}{\\sigma^2_{\\epsilon}(t)+\\sigma_{\\mu}^2(t-1)} \\\\\n",
    "    &= \\sigma_{\\mu}^2(t-1) \\left( 1-K(t) \\right)\n",
    "\\end{align*}$$\n",
    "where we defined the **Kalman gain**\n",
    "$$\n",
    "    K(t) =  \\frac{\\sigma_{\\mu}^2(t-1)}{\\sigma^2_{\\epsilon}(t)+\\sigma_{\\mu}^2(t-1)}\n",
    "$$\n",
    "- This linear sequential estimator of mean and variance in Gaussian observations is a **Kalman Filter**.\n",
    "- The new observation $x(t)$ 'corrects' the old estimate $\\hat \\mu(t-1)$ by a quantity that is proportional to the _innovation_ (or _residual_)  $\\left( x(t) - \\hat \\mu(t-1) \\right)$.\n",
    "- Note that the uncertainty about $\\mu$ decreases over time\n",
    "- Recursive Bayesian estimation is the basis for **adaptive signal processing** algorithms such as Least Mean Squares (LMS) and Recursive Least Squares (RLS).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">MARCO: Can you insert an interactive evolution of a simple 1D Kalman filter here. Eg estimation of a constant Voltage with some measurement error. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Gaussians\n",
    "The success of Gaussian distributions in probabilistic modeling is large due to the following properties:\n",
    "- The product of two Gaussian functions is another Gaussian function (use in Bayes rule). \n",
    "- The convolution of two Gaussian functions is another Gaussian function (use in sum of 2 variables)\n",
    "- A linear transformation of a Gaussian dsitributed variable is also Gaussian distributed\n",
    "- Conditioning and marginalization of multivariate Gaussian distributions produce Gaussians again (use in working with observations and when doing Bayesian predictions)\n",
    "- The Gaussian PDF has higher entropy than any other with the same variance. (Not discussed in this course).\n",
    "- Any smooth function with single rounded maximum, if raised to higher and higher powers, goes into a Gaussian function. (Not discussed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### What's Next?\n",
    "- We discussed how Bayesian probability theory provides an integrated framework for making predictions based on observed data.\n",
    "- The process involves model specification (your main task!), inference and actual model-based prediction.\n",
    "- The latter two tasks are only difficult because of computational issues.\n",
    "   - Maximum likelihood was introduced as a computationally simpler approximation to the Bayesian approach.\n",
    "   - In particular under some linear Gaussian assumptions, a few interesting models can be designed.\n",
    "   - The rest of this course (part-1) concerns introduction to these Linear Gaussian models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.cell { /* set cell width */\n",
       "    width: 750px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    width: 1000px;\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:600px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.css\") do f\n",
    "    display(\"text/html\", readall(f))\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.10",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
