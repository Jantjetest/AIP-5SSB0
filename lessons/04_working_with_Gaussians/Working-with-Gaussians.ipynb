{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Gaussians\n",
    "======="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Review of processing of Gaussian distributions in linear systems\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional\n",
    "    - Bishop pp. 85-93       \n",
    "    - [MacKay - 2006 - The Humble Gaussian Distribution](./files/Mackay-2006-The-humble-Gaussian-distribution.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sums and Transformations of Gaussian Variables\n",
    "\n",
    "- The Gaussian distribution\n",
    "$$\n",
    "\\mathcal{N}(x|\\mu,\\Sigma) = |2 \\pi \\Sigma |^{-\\frac{1}{2}} \\,\\mathrm{exp}\\left\\{-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\right\\}\n",
    "$$\n",
    "for variable $x$ is completely specified by its mean $\\mu$ and variance $\\Sigma$. \n",
    "\n",
    "- $\\Lambda = \\Sigma^{-1}$ is called the **precision matrix**.\n",
    "\n",
    "- A **linear transformation** $z=Ax+b$ of a Gaussian variable $\\mathcal{N}(x|\\mu,\\Sigma)$ is Gaussian distributed as\n",
    "\n",
    "$$\n",
    "p(z) = \\mathcal{N} \\left(z \\,|\\, A\\mu+b, A\\Sigma A^T \\right) \\tag{SRG-4a}\n",
    "$$\n",
    "\n",
    "- The **sum of two independent Gaussian variables** is also Gaussian distributed. Specifically, if $x \\sim \\mathcal{N} \\left(x|\\mu_x, \\Sigma_x \\right)$ and $y \\sim \\mathcal{N} \\left(y|\\mu_y, \\Sigma_y \\right)$, then the PDF for $z=x+y$ is given by\n",
    "$$\\begin{align}\n",
    "p(z) &= \\mathcal{N}(x\\,|\\,\\mu_x,\\Sigma_x) \\ast \\mathcal{N}(y\\,|\\,\\mu_y,\\Sigma_y) \\notag\\\\\n",
    "  &= \\mathcal{N} \\left(z\\,|\\,\\mu_x+\\mu_y, \\Sigma_x +\\Sigma_y \\right) \\tag{SRG-8}\n",
    "\\end{align}$$\n",
    "  - <span style=\"color:green\">[Exercise]</span>: Show that Eq.SRG-8 is really a special case of Eq.SRG-4a. \n",
    "\n",
    "- The sum of two Gaussian _distributions_ is NOT a Gaussian distribution. Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Gaussian Signals in a Linear System\n",
    "\n",
    "<img src=\"./figures/fig-linear-system.png\" width=300px>\n",
    "\n",
    "- <span style=\"color:blue\">[Q.]</span>: Given independent variables\n",
    "$x \\sim \\mathcal{N}(\\mu_x,\\sigma_y)$ and $y \\sim \\mathcal{N}(\\mu_y,\\sigma_y)$, what is the PDF for $z = A\\cdot(x -y) + b$ ?\n",
    "\n",
    "- <span style=\"color:blue\">[A.]</span>: $z$ is also Gaussian with \n",
    "$$\n",
    "p_z(z) = \\mathcal{N}(z|A(\\mu_x-\\mu_y)+b, \\, A(\\sigma_x \\mathbf{+} \\sigma_y)A^T)\n",
    "$$\n",
    "\n",
    "\n",
    "- Think about the role of the Gaussian distribution for stochastic linear systems in relation to what sinusoidals mean for deterministic linear system analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Bayesian Estimation of a Constant\n",
    "\n",
    "<span style=\"color:blue\">[Question]</span>\n",
    "\n",
    "- Estimate a constant $\\theta$ from one 'noisy' measurement $x$ about that constant. Assume the following model specification:\n",
    "     \n",
    "$$\\begin{align*}\n",
    "x &= \\theta + \\epsilon \\\\\n",
    "\\theta &\\sim \\mathcal{N}(\\theta|\\mu_\\theta,\\sigma_\\theta^2) \\\\\n",
    "\\epsilon &\\sim \\mathcal{N}(\\epsilon|0,\\sigma^2_{\\epsilon})\n",
    "\\end{align*}$$\n",
    "\n",
    "<span style=\"color:blue\">[Answer]</span>\n",
    "\n",
    "1. **Model specification**\n",
    "Note that you can rewrite these specifications in probabilistic notation as follows:\n",
    "$$\\begin{align}\n",
    "    p(x|\\theta) &=\\mathcal{N}(x|\\theta,\\sigma^2_{\\epsilon}) \\tag{likelihood}\\\\\n",
    "    p(\\theta) &=\\mathcal{N}(\\theta|\\mu_\\theta,\\sigma_\\theta^2) \\tag{prior}\n",
    "\\end{align}$$\n",
    "2. **Inference** for the posterior PDF $p(\\theta|x)$\n",
    "$$\\begin{align*}\n",
    "p(\\theta|x)  &= \\frac{p(x|\\theta)p(\\theta)}{p(x)} = \\frac{p(x|\\theta)p(\\theta)} { \\int p(x|\\theta)p(\\theta) \\, \\mathrm{d}\\theta } \\notag \\\\\n",
    "    &= \\frac{1}{C} \\,\\mathcal{N}(x|\\theta,\\sigma^2_{\\epsilon})\\, \\mathcal{N}(\\theta|\\mu_\\theta,\\sigma_\\theta^2) \\notag \\\\\n",
    "    &= \\frac{1}{C_1} \\mathrm{exp} \\left\\{ -\\frac{(x-\\theta)^2}{2\\sigma^2_{\\epsilon}} - \\frac{(\\theta-\\mu_\\theta)^2}{2\\sigma_\\theta^2} \\right\\} \\notag \\\\\n",
    "    &= \\frac{1}{C_1} \\mathrm{exp} \\left\\{ \\theta^2\\left( -\\frac{1}{2\\sigma^2_{\\epsilon}} - \\frac{1}{2\\sigma_\\theta^2} \\right) + \\theta \\left( \\frac{x}{\\sigma^2_{\\epsilon}} + \\frac{\\mu_\\theta}{\\sigma_\\theta^2} \\right) +  C_2 \\right\\} \\notag \\\\\n",
    "    &= \\frac{1}{C_1} \\mathrm{exp} \\left\\{ -\\frac{\\sigma_\\theta^2 + \\sigma^2_{\\epsilon}}{2\\sigma_\\theta^2 \\sigma^2_{\\epsilon}} \\left( \\theta - \\frac{x\\sigma_\\theta^2 + \\mu_s\\sigma^2_{\\epsilon}}{\\sigma_\\theta^2 + \\sigma^2_{\\epsilon}} \\right)^2 + C_3  \\right\\}\n",
    "\\end{align*}$$\n",
    "which we recognize as a Gaussian distribution.\n",
    "  - This computational 'trick' for multiplying two Gaussians is called **completing the square**. Compare the procedure to $$ax^2+bx+c_1 = a\\left(x+\\frac{b}{2a}\\right)^2+c_2$$\n",
    "  \n",
    "  \n",
    "  \n",
    "- Hence, it follows that the posterior for $\\theta$ is\n",
    "$$\\begin{equation*}\n",
    "    p(\\theta|x) = \\mathcal{N} (\\theta |\\, \\mu_{\\theta|x}, \\sigma_{\\theta|x}^2)\n",
    "\\end{equation*}$$\n",
    "where\n",
    "$$\\begin{align*}\n",
    "  \\sigma_{\\theta|x}^2  &= \\frac{\\sigma^2_{\\epsilon}\\sigma_\\theta^2}{\\sigma^2_{\\epsilon} + \\sigma_\\theta^2} = \\left( \\frac{1}{\\sigma_\\theta^2} + \\frac{1}{\\sigma^2_{\\epsilon}}\\right)^{-1} \\\\\n",
    "  \\mu_{\\theta|x}   &= \\sigma_{\\theta|x}^2 \\, \\left( \\frac{1}{\\sigma^2_{\\epsilon}}x + \\frac{1}{\\sigma_\\theta^2} \\mu_\\theta \\right) \n",
    "\\end{align*}$$\n",
    "\n",
    "- So, multiplication of two Gaussians yields another (unnormalized) Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"> MARCO please insert interactive example of Gaussian multiplications here </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Gaussian Multiplication\n",
    "\n",
    "- In general, the multiplication of two multi-variate Gaussians yields an (unnormalized) Gaussian, see [SRG-6]:\n",
    "$$\\begin{equation*}\n",
    "\\mathcal{N}(x|\\mu_a,\\Sigma_a) \\cdot \\mathcal{N}(x|\\mu_b,\\Sigma_b) = \\alpha \\cdot \\mathcal{N}(x|\\mu_c,\\Sigma_c)\n",
    "\\end{equation*}$$\n",
    "where\n",
    "$$\\begin{align*}\n",
    "\\Sigma_c &= \\left( \\Sigma_a^{-1} + \\Sigma_b^{-1} \\right)^{-1}\\\\\n",
    "\\mu_c &= \\Sigma_c \\left( \\Sigma_a^{-1}\\mu_a + \\Sigma_b^{-1}\\mu_b\\right)\n",
    "\\end{align*}$$\n",
    "and normalization constant $\\alpha = \\mathcal{N}(\\mu_a|\\, \\mu_b, \\sigma_a + \\sigma_b)$.\n",
    "\n",
    "- If we define the **precision** as $\\Lambda \\equiv \\Sigma^{-1}$, then we see that **precisions add** and **precision-weighted means add** too.\n",
    "\n",
    "- As we just saw, great application to Bayesian inference!\n",
    "\n",
    "$$\\begin{equation*}\n",
    "\\underbrace{\\text{Gaussian}}_{\\text{posterior}}\n",
    " \\propto \\underbrace{\\text{Gaussian}}_{\\text{likelihood}} \\times \\underbrace{\\text{Gaussian}}_{\\text{prior}}\n",
    "\\end{equation*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditioning and Marginalization of a Gaussian\n",
    "\n",
    "Let $z = \\begin{bmatrix} x \\\\ y \\end{bmatrix}$ be jointly normal distributed as\n",
    "\n",
    "$$\n",
    "p(z|\\mu,\\Sigma) = \\mathcal{N} \\left( \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\left| \\begin{bmatrix} \\mu_x \\\\ \\mu_y \\end{bmatrix}, \n",
    "  \\begin{bmatrix} \\Sigma_x & \\Sigma_{xy} \\\\ \\Sigma_{yx} & \\Sigma_y \\end{bmatrix} \\right. \\right)\n",
    "$$\n",
    "\n",
    "- Note that the symmetry $\\Sigma=\\Sigma^T$ implies that $\\Sigma_x$ and $\\Sigma_y$ are symmetric and $\\Sigma_{xy} = \\Sigma_{yx}^T$.\n",
    "\n",
    "- Now let's factorize $p(x,y)$ into $p(y|x)\\, p(x)$ through conditioning and marginalization (for applications to Bayesian inference in jointly Gaussian systems).\n",
    "\n",
    "- **Marginalization**\n",
    "$$\n",
    "p(x) = \\int p(x,y)\\,\\mathrm{d}y = \\mathcal{N}\\left( x|\\mu_x, \\Sigma_x \\right), \\;\\text{and}\\; p(y)=\\mathcal{N} \\left(y|\\mu_y, \\Sigma_y \\right)\n",
    "$$\n",
    "\n",
    "- **Conditioning**\n",
    "$$\\begin{align*}\n",
    "p(y|x) &= p(x,y)/p(x) \\\\\n",
    " &= \\mathcal{N}\\left(y|\\mu_y + \\Sigma_{yx}\\Sigma_x^{-1}(x-\\mu_x),\\, \\Sigma_y - \\Sigma_{yx}\\Sigma_x^{-1}\\Sigma_{xy} \\right)\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"> MARCO please insert interactive example of Gaussian marginalization and conditioning here </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Example: Conditioning of Gaussian\n",
    "\n",
    "Consider (again) the system \n",
    "\n",
    "$$\\begin{align*}\n",
    "x &= \\theta + \\epsilon \\\\\n",
    "\\theta &\\sim \\mathcal{N}(\\theta|\\mu_\\theta,\\sigma_\\theta^2) \\\\\n",
    "\\epsilon &\\sim \\mathcal{N}(\\epsilon|0,\\sigma^2_{\\epsilon})\n",
    "\\end{align*}$$\n",
    "\n",
    "- This system is equivalent to (<span style=\"color:green\">Exercise: derive this!</span>)\n",
    "\n",
    "$$\n",
    "p(\\theta,x|\\,\\mu,\\sigma) = \\mathcal{N} \\left( \\begin{bmatrix} \\theta\\\\ \n",
    "  x \\end{bmatrix} \n",
    "  \\left| \\begin{bmatrix} \\mu_\\theta\\\\ \n",
    "  \\mu_\\theta\\end{bmatrix}, \n",
    "         \\begin{bmatrix} \\sigma_\\theta^2 & \\sigma_\\theta^2\\\\ \n",
    "         \\sigma_\\theta^2 & \\sigma_\\theta^2+\\sigma_{\\epsilon}^2 \n",
    "  \\end{bmatrix} \n",
    "  \\right. \\right)\n",
    "$$\n",
    "\n",
    "- Direct substitution of the rule for Gaussian conditioning leads to\n",
    "$$\\begin{align*}\n",
    "K &= \\frac{\\sigma_\\theta^2}{\\sigma_\\theta^2+\\sigma_{\\epsilon}^2} \\qquad \\text{($K$ is called: Kalman gain)}\\\\\n",
    "p(\\theta|x) &= \\mathcal{N} \\left( \\theta |\\, \\mu_\\theta + K \\cdot (x-\\mu_\\theta), \\, \\sigma_\\theta^2 \\left( 1-k \\right) \\right)\n",
    "\\end{align*}$$\n",
    "    \n",
    "- <span style=\"color:green\">Homework exercises: (1) Actually derive this; (2) show that the result is equivalent to the previous slide on 'estimation of a constant'; and (3) Try to interpret the resulting formula's.</span>\n",
    "\n",
    "$\\longrightarrow$ Moral: For jointly Gaussian systems, we do inference simply in one step by using the formulas for conditioning and marginalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: Recursive Bayesian Estimation\n",
    "\n",
    "Now consider the signal $x(t)=\\theta+\\epsilon(t)$, where $D_t= \\left(x(1),\\ldots,x(t)\\right)$ is observed _sequentially_ (over time).\n",
    "\n",
    "Question:\n",
    "- Derive a recursive algorithm for $p(\\theta|D_t)$, i.e., an update rule for $p(\\theta|D_t)$ based on $p(\\theta|D_{t-1})$ and $x(t)$.\n",
    " \n",
    "Answer: \n",
    "- Let's define $p(\\theta|D_t) = \\mathcal{N}(\\mu(t),\\sigma^2(t))$ and assume prior $p(\\theta) = \\mathcal{N}(\\mu,\\sigma^2)$.\n",
    "        \n",
    "- We will solve this by using the estimate after $t-1$ as the **prior distribution** in conjunction with the **likelihood** for observation $x(t)$,\n",
    "$$\\begin{equation*}\n",
    "p(\\mu(t)|D_t) \\propto p(x(t)|\\mu(t-1),\\sigma^2(t-1)) \\times p(\\mu(t)|D_{t-1})\n",
    "\\end{equation*}$$\n",
    "\n",
    "- Use the 'batch processing' posteriors for $\\mu$ and $\\sigma^2$ to get\n",
    "$$\\begin{align}\n",
    "\\hat \\mu(t) &= \\sigma_{\\mu}^2(t) \\, \\left( \\frac{1}{\\sigma^2_{\\epsilon}(t)}x(t) + \\frac{1}{\\sigma_{\\mu}^2(t-1)} \\hat \\mu(t-1) \\right) \\notag \\\\\n",
    "    &= \\frac{\\sigma^2_{\\epsilon}(t)}{\\sigma^2_{\\epsilon}(t)+\\sigma_{\\mu}^2(t-1)}x(t) + \\frac{\\sigma_{\\mu}^2(t-1)}{\\sigma^2_{\\epsilon}(t)+\\sigma_{\\mu}^2(t-1)} \\hat \\mu(t-1) \\notag \\\\\n",
    "    &= \\hat \\mu(t-1) + K(t) \\times \\left( x(t) - \\hat \\mu(t-1) \\right) \\\\\n",
    "\\sigma_{\\mu}^2(t) &= \\sigma_{\\mu}^2(t-1) \\frac{\\sigma^2_{\\epsilon}(t)}{\\sigma^2_{\\epsilon}(t)+\\sigma_{\\mu}^2(t-1)} \\notag \\\\\n",
    "    &= \\sigma_{\\mu}^2(t-1) \\left( 1-K(t) \\right)\n",
    "\\end{align}$$\n",
    "where we defined the **Kalman gain**\n",
    "$$\\begin{equation*}\n",
    "    K(t) =  \\frac{\\sigma_{\\mu}^2(t-1)}{\\sigma^2_{\\epsilon}(t)+\\sigma_{\\mu}^2(t-1)}\n",
    "\\end{equation*}$$\n",
    "- This linear sequential estimator of mean and variance in Gaussian observations is a **Kalman Filter**.\n",
    "- The new observation $x(t)$ 'corrects' the old estimate $\\hat \\mu(t-1)$ by a quantity that is proportional to the _innovation_ (or _residual_)  $\\left( x(t) - \\hat \\mu(t-1) \\right)$.\n",
    "- Note that the uncertainty about $\\mu$ decreases over time\n",
    "- Recursive Bayesian estimation is the basis for **adaptive signal processing** algorithms such as Least Mean Squares (LMS) and Recursive Least Squares (RLS).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">MARCO: Can you insert an interactive evolution of a simple 1D Kalman filter here. Eg estimation of a constant Voltage with some measurement error. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product of Normally Distributed Variables\n",
    "\n",
    "- (We've seen that) the sum of two Gausssian distributed variables is also Gaussian distributed.\n",
    "\n",
    "- Has the _product_ of two Gaussian distributed variables also a Gaussian distribution?\n",
    "\n",
    "- **No**! In general this is a difficult computation. As an example, let's compute $p(z)$ for $Z=XY$ for the special case that $X\\sim \\mathcal{N}(0,1)$ and $Y\\sim \\mathcal{N}(0,1)$.\n",
    "$$\\begin{align*}\n",
    "p(z) &= \\int_{X,Y} p(z|x,y)\\,p(x,y)\\,\\mathrm{d}x\\mathrm{d}y \\\\\n",
    "  &= \\frac{1}{2 \\pi}\\int  \\delta(z-xy) \\, e^{-(x^2+y^2)/2} \\, \\mathrm{d}x\\mathrm{d}y \\\\\n",
    "  &=  \\frac{1}{\\pi} \\int_0^\\infty \\frac{1}{x} e^{-(x^2+z^2/x^2)/2} \\, \\mathrm{d}x \\\\\n",
    "  &= \\frac{1}{\\pi} \\mathrm{K}_0( \\lvert z\\rvert )\\,.\n",
    "\\end{align*}$$\n",
    "where  $\\mathrm{K}_n(z)$ is a [modified Bessel function of the second kind](http://mathworld.wolfram.com/ModifiedBesselFunctionoftheSecondKind.html).\n",
    "\n",
    "<span style=\"color:red\"> Marco, Can you plot this distribution, see http://mathworld.wolfram.com/NormalProductDistribution.html</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Gaussians\n",
    "\n",
    "The success of Gaussian distributions in probabilistic modeling is large due to the following properties:\n",
    "\n",
    "- The convolution of two Gaussian functions is another Gaussian function (use in sum of 2 variables)\n",
    "\n",
    "- The product of two Gaussian functions is another Gaussian function (use in Bayes rule). \n",
    "\n",
    "- A linear transformation of a Gaussian distributed variable is also Gaussian distributed\n",
    "\n",
    "- Conditioning and marginalization of multivariate Gaussian distributions produce Gaussians again (use in working with observations and when doing Bayesian predictions)\n",
    "\n",
    "- <span style=\"color:gray\">The Gaussian PDF has higher entropy than any other with the same variance. (Not discussed in this course).</span>\n",
    "\n",
    "- <span style=\"color:gray\">Any smooth function with single rounded maximum, if raised to higher and higher powers, goes into a Gaussian function. (Not discussed).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Useful Matrix Calculus\n",
    "\n",
    "Aside from working with Gaussians, it will be helpful for the next lessons to be familiar with some matrix calculus. We shortly recapitulate used formulas here. \n",
    "\n",
    "- We define the **gradient** of a scalar function $f(A)$ w.r.t. an $n \\times k$ matrix $A$ as\n",
    "$$\n",
    "\\nabla_A f \\triangleq\n",
    "    \\begin{bmatrix}\n",
    "\\frac{\\partial{f}}{\\partial a_{11}} & \\frac{\\partial{f}}{\\partial a_{12}} & \\cdots & \\frac{\\partial{f}}{\\partial a_{1k}}\\\\\n",
    "\\frac{\\partial{f}}{\\partial a_{21}} & \\frac{\\partial{f}}{\\partial a_{22}} & \\cdots & \\frac{\\partial{f}}{\\partial a_{2k}}\\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots\\\\\n",
    "\\frac{\\partial{f}}{\\partial a_{n1}} & \\frac{\\partial{f}}{\\partial a_{n2}} & \\cdots & \\frac{\\partial{f}}{\\partial a_{nk}}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "    \n",
    "- The following formulas are useful (see Bishop App.-C)\n",
    "$$\\begin{align*}\n",
    "|A^{-1}|&=|A|^{-1} \\tag{B-C.4} \\\\\n",
    "\\nabla_A \\log |A| &= (A^{T})^{-1} = (A^{-1})^T \\tag{B-C.28} \\\\\n",
    "\\mathrm{Tr}[ABC]&= \\mathrm{Tr}[CAB] = \\mathrm{Tr}[BCA] \\tag{B-C.9} \\\\\n",
    "\\nabla_A \\mathrm{Tr}[AB] &=\\nabla_A \\mathrm{Tr}[BA]= B^T \\tag{B-C.25} \\\\\n",
    "\\nabla_A \\mathrm{Tr}[ABA^T] &= A(B+B^T)  \\tag{B-C.27}\\\\\n",
    " \\nabla_x x^TAx &= (A+A^T)x \\tag{from B-C.27}\\\\\n",
    "\\nabla_X a^TXb &= \\nabla_X \\mathrm{Tr}[ba^TX] = ab^T \\notag\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What's Next?\n",
    "\n",
    "- We discussed how Bayesian probability theory provides an integrated framework for making predictions based on observed data.\n",
    "\n",
    "- The process involves model specification (your main task!), inference and actual model-based prediction.\n",
    "\n",
    "- The latter two tasks are only difficult because of computational issues.\n",
    "\n",
    "- Maximum likelihood was introduced as a computationally simpler approximation to the Bayesian approach.\n",
    "\n",
    "- In particular under some linear Gaussian assumptions, a few interesting models can be designed.\n",
    "\n",
    "- The rest of this course (part-1) concerns introduction to these Linear Gaussian models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----\n",
    "_The cell below loads the style file_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.cell { /* set cell width */\n",
       "    width: 750px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    width: 1000px;\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:600px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.html\") do f\n",
    "    display(\"text/html\", readall(f))\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
