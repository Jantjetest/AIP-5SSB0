{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Theory Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Review of probability theory from a logical viewpoint (i.e., a Bayesian interpretation)\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional\n",
    "    - Bishop pp. 12-20       \n",
    "    - [Bruininkx - 2002 - Bayesian Probability](./files/Bruyninkx-2002-Bayesian-probability.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Probability Theory?\n",
    "\n",
    "- Probability theory (PT) is the **theory of optimal processing of incomplete information**, and as such provides a quantitative framework for drawing conclusions from a finite (read: incomplete) data set.\n",
    "- Machine learning concerns drawing conclusions from data.\n",
    "- In general, nearly all interesting questions in machine learning can be stated in the following form (a conditional probability):\n",
    "\n",
    "$$p(\\text{whatever-we-want-to-know}\\, | \\,\\text{whatever-we-do-know})$$\n",
    "\n",
    "- For example\n",
    "  - Generate data predictions, $p(x_{\\text{future}}|x_{\\text{past}})$\n",
    "  - Classify a received data point, $p(\\mathcal{C}_k|x)$\n",
    "  - Compress data in an efficient way from $p(x_n|x_{1:n-1})$\n",
    "\n",
    "- Note that **Information theory** (the \"theory of log-probability\") provides a source coding view on machine learning that is consistent with probability theory (more in part-2). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Probability Theory Notation\n",
    "-  An **event** $A$ is a statement, whose truth is contemplated by a person, e.g.,\n",
    "\n",
    "$$A = \\text{`it will rain tomorrow'}$$\n",
    " \n",
    "- Notation: $\\bar{A}$ (latex: \\bar{A}) is **not**-A, the denial of statement $A$\n",
    "- For any event $A$, with background knowledge $I$, the **probability of $A$, given $I$** is written as \n",
    "$$p(A|I)$$\n",
    "which is a number between $0$ and $1$. If, given $I$, you know that $A$ is true, than $p(A|I)=1$.\n",
    "\n",
    "- The probability $p(A|I)$ should be **interpreted as your degree of belief** that event $A$ is true, given that $I$ is true. This is an extremely powerful interpretation (later more about this). \n",
    "\n",
    "Notation for compound events:\n",
    "\n",
    "- $p(A,B|I)$ is the **joint** probability that both $A$ and $B$ are true, given $I$.\n",
    "- $p(A+B|I)$ is the probability that either $A$ or $B$, or both $A$ and $B$, are true, given $I$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Probability Theory Calculus\n",
    " \n",
    "Probability theory **extends** boolean logic to rational reasoning with uncertainty. Under some mild conditions, the following calculation rules can be derived, (see [Cox, 1946](https://en.wikipedia.org/wiki/Cox%27s_theorem)):\n",
    " \n",
    "- **Product rule**. For 2 events $A, B$ with given background $I$,\n",
    "\n",
    "$$ p(A,B|I) = p(A|B,I)\\,p(B|I) \\,.$$\n",
    "\n",
    "- **Sum rule**. For 2 events $A$, $B$ with given background $I$,\n",
    "\n",
    "$$ p(A+B|I) = p(A|I) + p(B|I) - p(A,B|I)\\,.$$\n",
    "\n",
    "- Clearly, it follows from the sum rule that $ p(A|I) + p(\\bar{A}|I) = 1$\n",
    "\n",
    "- Note that the background information may not change, e.g., if $I^\\prime \\neq I$, then \n",
    "\n",
    "$$p(A,B|I) \\neq p(A|B,I)\\,p(B|I^\\prime)$$ \n",
    "\n",
    "- **All legitimate relations between probabilities can be derived from the sum and product rules!!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Notation and useful facts\n",
    "\n",
    "- Iff \n",
    "$$p(A,B|I) = p(A|I)p(B|I)$$\n",
    "then $A$ and $B$ are said to be **independent**, given $I$. Note that this is equivalent to the statement $p(A|B,I)=p(A|I)$.\n",
    "\n",
    "- The product and sum rules are also known as the **axioms of probability theory**, but in fact they can be derived as the unique rules for rational reasoning under uncertainty.\n",
    "- All probabilities are in principle conditional probabilities of the type $p(A|I)$, since there is always some background knowledge. \n",
    " \n",
    "- Shorthand notation\n",
    "  - Still, we often write $p(A)$ rather than $p(A|I)$ if the background knowledge $I$ is assumed to be obviously present. E.g., $p(X=5)$ rather than $p(X=5|\\text{the sun comes up tomorrow})$.    \n",
    "  - If $X$ is a random variable and $X=x$ is an event, then we often write $p(x)$ rather than $p(X=x)$ (hoping again that the reader understands the context ;-)  \n",
    "  - $p(X)$ denotes the distribution over random variable $X$.   \n",
    "\n",
    "- Next, we present two useful corollaries:  (1) Marginalization and (2) Bayes rule \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sum Rule and Marginalization\n",
    "- If $X$ and $Y$ are random variables, than it follows from the sum rule that \n",
    "\n",
    "$$p(X) = \\sum_Y p(X,Y) \\left( = \\sum_Y p(X|Y)p(Y) \\right)$$ \n",
    "\n",
    "- Note that Bishop (p.14) calls this the sum rule.\n",
    "\n",
    "- Proof this!\n",
    "\n",
    "- Of course, in the continuous domain, $p(x)=\\int p(x,y) \\mathrm{d}y$\n",
    "\n",
    "- Integrating $Y$ out of a joint distribution is called **marginalization** and the result $p(X)$ is sometimes referred to as the **marginal probability**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### The Product Rule and Bayes Rule\n",
    "\n",
    "- Consider 2 variables $D$ and $\\theta$; it follows from $p(D,\\theta)=p(D|\\theta)p(\\theta)=p(\\theta|D)p(D)$ that\n",
    "\n",
    "$$ p(\\theta|D) = \\frac{p(D|\\theta) p(\\theta)}{p(D)}$$ \n",
    "\n",
    "- This formula is called **Bayes rule**. While Bayes rule is always true, a particularly useful application occurs when $D$ refers to an observed data set and $\\theta$ is set of model parameters that relates to the data. In that case,\n",
    "\n",
    "  - the **prior** probability $p(\\theta)$ represents our **degree-of-belief** about proper values for $\\theta$, before seeing the data $D$.\n",
    "  - the **posterior** probability $p(\\theta|D)$ relates to our state-of-knowledge about $\\theta$ after we have seen the data.\n",
    "\n",
    "$\\Rightarrow$ Bayes rule tells us how to update our knowledge about model parameters (or other hypotheses) when facing new data. Hence, \n",
    "\n",
    "<center>\n",
    "<div style=\"border:2px solid blue;padding:1em;\">\n",
    "**Bayes rule is the fundamental rule for machine learning!**\n",
    "</div>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Rule Nomenclature\n",
    "- Some nomenclature associated with Bayes rule\n",
    "$$\\begin{equation}\n",
    "\\underbrace{p(\\theta | D)}_{\\text{posterior}} = \\frac{\\overbrace{p(D|\\theta)}^{\\text{likelihood}} \\times \\overbrace{p(\\theta)}^{\\text{prior}}}{\\underbrace{p(D)}_{\\text{evidence}}}\n",
    "\\end{equation}$$\n",
    "\n",
    "- Note that the evidence can be computed through marginalization since\n",
    "$$ p(D) = \\int p(D,\\theta) \\,\\mathrm{d}\\theta = \\int p(D|\\theta)\\,p(\\theta) \\,\\mathrm{d}\\theta$$\n",
    "\n",
    "- For given $D$, the posterior probabilities of the parameters scale relatively against each other as\n",
    "$$\n",
    "p(\\theta|D) \\propto p(D|\\theta) p(\\theta)\n",
    "$$\n",
    "\n",
    "$\\Longrightarrow$ All that we can learn from the observed data is contained in the likelihood function $p(D|\\theta)$. This is called the **likelihood principle**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Likelihood Function\n",
    "\n",
    "Consider a probabilistic model $p(D|\\theta)$, where $D$ relates to a data set and $\\theta$ are model parameters.\n",
    "\n",
    "- In general, $p(D|\\theta)$ is a function of both $D$ and $\\theta$.\n",
    "\n",
    "- The **sampling distribution** $$p(D|\\theta=\\theta_0)$$ describes the probability that data $D$ is observed, assuming that it is generated by the given model with parameter values set to $\\theta = \\theta_0$.\n",
    "\n",
    "- In a machine learning context, often $D=D_0$ is given (observed), and $\\theta$ is the free variable.\n",
    "\n",
    "- When viewed as a function of the free variable $\\theta$ with given $D=D_0$, $$\\mathrm{L}(\\theta) \\triangleq p(D=D_0|\\theta)$$ is called the **likelihood function** (for $\\theta$)\n",
    "\n",
    "- Note that $\\mathrm{L}(\\theta)$ is not a probability distribution for $\\theta$.  (Is $\\sum_\\theta \\mathrm{L}(\\theta)=1$ always true?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE EXAMPLE\n",
    "\n",
    "Consider the following simple model for the outcome of the tossing of a biased coin with parameter $\\theta \\in [0,1]$:\n",
    "\n",
    "$\n",
    "y \\in \\{0,1\\}, \\\\\n",
    "p(y|\\theta) \\triangleq y\\cdot\\theta + (1-y)\\cdot(1-\\theta),\\\\\n",
    "p(D|\\theta) = p(y_1,\\ldots,y_N|\\theta) = \\Pi_{n=1}^N p(y_n|\\theta).\n",
    "$\n",
    "\n",
    "If we consider just one data point, we can plot both the sampling distribution (i.e. $p(y|\\theta=0.8)$) and the likelihood function (i.e. $L(\\theta) = p(y=0|\\theta)$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: ArgumentError: Reactive not found in path\nwhile loading In[1], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "LoadError: ArgumentError: Reactive not found in path\nwhile loading In[1], in expression starting on line 1",
      "",
      " in require at ./loading.jl:233"
     ]
    }
   ],
   "source": [
    "using Reactive, Interact, PyPlot\n",
    "p(y,θ) = y.*θ + (1-y).*(1-θ)\n",
    "f = figure()\n",
    "@manipulate for y=false, θ=0:0.1:1; withfig(f) do\n",
    "        # Plot the sampling distribution\n",
    "        subplot(221); stem([0,1], p([0,1],θ)); title(\"Sampling distribution\"); xlim([-0.5,1.5]); ylim([0,1]); xlabel(\"y\"); ylabel(\"p(y|θ=$(θ))\");\n",
    "        # Plot the likelihood function\n",
    "        _θ = linspace(0.0, 1.0, 100)\n",
    "        subplot(222); plot(_θ, p(convert(Float64,y), _θ)); title(\"Likelihood function\"); xlabel(\"θ\"); ylabel(\"L(θ) = p(y=$(convert(Float64,y))|θ)\");\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (discrete) sampling distribution is a valid probability distribution. \n",
    "However, the likelihood function $L(\\theta)$ clearly isn't, since $\\int_0^1 L(\\theta) \\mathrm{d}\\theta \\neq 1$. \n",
    "\n",
    "#### END OF CODE EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Probabilistic Inference\n",
    "-- **Probabilistic inference** is computing\n",
    "\n",
    "$$\n",
    "p(\\text{whatever-we-want-to-know}\\, | \\,\\text{whatever-we-do-know})\n",
    "$$\n",
    "\n",
    "E.g., \n",
    "\n",
    "   $p(\\,\\text{Mr.S. killed Mrs.S.} \\;|\\; \\text{evidence}\\,),$\n",
    "\n",
    "   $p(\\,\\text{transmitted codeword} \\;|\\;\\text{received codeword}\\,),$\n",
    "\n",
    "   $p(\\,\\text{articulatory movements} \\;|\\; \\text{speech signal}\\,),$\n",
    "\n",
    "   $p(\\,\\text{fetal HR signal} \\;|\\;\\text{mother signal}\\,).$\n",
    "                \n",
    "- This can be accomplished by repeated application of sum and product rules, (of course).\n",
    "\n",
    "- In practice, Bayes Rule and  marginalization are very useful tools.\n",
    "\n",
    "- The resulting expressions often contain a bunch of (hard) integrals and/or sums (with many terms).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Exercise: Disease Diagnosis\n",
    "\n",
    "[Q.] Given a disease $D$ with prevalence of $1\\%$ and a test procedure $T$ with sensitivity ('true positive' rate) of $95\\%$ and specificity ('true negative' rate) of $85\\%$, what is the chance that somebody who tests positive actually has the disease?\n",
    "\n",
    "[A.] The given data are $p(D=1)=0.01$, $p(T=1|D=1)=0.95$ and $p(T=0|D=0)=0.85$. Then according to Bayes rule,\n",
    "\n",
    "$$\\begin{align*}\n",
    "p( & D=1 | T=1) \\\\\n",
    "&= \\frac{p(T=1|D=1)p(D=1)}{p(T=1)} \\tag{Bayes}\\\\\n",
    "&= \\frac{p(T=1|D=1)p(D=1)}{p(T=1|D=1)p(D=1)+p(T=1|D=0)p(D=0)} \\tag{marg.}\\\\\n",
    "&= \\frac{0.95\\times0.01}{0.95\\times0.01 + 0.15\\times0.99} = 0.0601\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Inference Exercise: Bag Counter\n",
    "[Q.] A bag contains one ball, known to be either white or black. A white ball is put in, the bag is shaken,\n",
    " and a ball is drawn out, which proves to be white. What is now the\n",
    " chance of drawing a white ball?\n",
    "\n",
    "[A.]  Again, use Bayes and marginalization to arrive at $p(\\text{white}|\\text{data})=2/3$, see homework exercise\n",
    "\n",
    "$\\Rightarrow$ Note that probabilities describe **a person's state of knowledge** rather than a 'property of nature'.\n",
    "\n",
    "[Q.] Is a speech signal a 'probabilistic' (random) variable? (homework)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Inference Exercise: Causality?\n",
    "\n",
    "[Q.] A dark bag contains five red balls and seven green ones. (a) What is the probability of drawing a red ball on the first draw? Balls are not returned to the bag after each draw. (b) If you know that on the second draw the ball was a green one, what is now the probability of drawing a red ball on the first draw?\n",
    "\n",
    "[A.] (a) $5/12$. (b) $5/11$, see homework.\n",
    "\n",
    "$\\Rightarrow$ Again, we conclude that conditional probabilities reflect **implications for a state of knowledge** rather than temporal causality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### PDF for the Sum of Two Variables\n",
    "\n",
    "[Q.]: Given two random **independent** variables\n",
    "$X$ and $Y$, with PDF's $p_x(x)$ and $p_y(y)$. What is the PDF\n",
    "of $Z = X + Y$?\n",
    "\n",
    "[A.]: Let $p_z(z)$ be the probability that $Z$ has value $z$. This occurs if $X$ has some value $x$ and at the same time $Y=z-x$, with joint probability $p_x(x)p_y(z-x)$. Since $x$ can be any value, we sum over all possible values for $x$ to get\n",
    "\n",
    "$$\n",
    "        p_z (z) = \\int_{ - \\infty }^\\infty  {p_x (x)p_y (z - x)\\,\\mathrm{d}{x}}\n",
    "$$\n",
    "        \n",
    "i.e., the **convolution** of $p_x$ and $p_y$.\n",
    "        \n",
    "Note that $p_z(z) \\neq p_x(x) + p_y(y)\\,$ !!\n",
    "\n",
    "$\\Rightarrow$ In linear stochastic systems theory, the Fourier Transform of a PDF (i.e., the characteristic function) plays an important computational role.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE EXAMPLE\n",
    "\n",
    "Consider the PDF of the sum of two independent Gaussians $X$ and $Y$:\n",
    "\n",
    "$\n",
    "p_X(x) = \\mathcal{N}(x|\\mu_X,\\sigma_X^2), \\\\ \n",
    "p_Y(y) = \\mathcal{N}(y|\\mu_Y,\\sigma_Y^2), \\\\\n",
    "Z = X + Y.\n",
    "$\n",
    "\n",
    "Performing the convolution (nice exercise) yields a Gaussian PDF for $Z$: \n",
    "\n",
    "$\n",
    "p_Z(z) = \\mathcal{N}(z|\\mu_X+\\mu_Y,\\sqrt{\\sigma_X^2+\\sigma_Y^2}).\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: ArgumentError: Reactive not found in path\nwhile loading In[2], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "LoadError: ArgumentError: Reactive not found in path\nwhile loading In[2], in expression starting on line 1",
      "",
      " in require at ./loading.jl:233"
     ]
    }
   ],
   "source": [
    "using Reactive, Interact, PyPlot, Distributions\n",
    "f = figure()\n",
    "@manipulate for μx=-2:0.1:2, σx=0.1:0.1:1.9,μy=0:0.1:4, σy=0.1:0.1:0.9; withfig(f) do\n",
    "        μz = μx+μy; σz = sqrt(σx^2 + σy^2)\n",
    "        x = Normal(μx, σx)\n",
    "        y = Normal(μy, σy)\n",
    "        z = Normal(μz, σz)\n",
    "        range_min = minimum([μx-2*σx, μy-2*σy, μz-2*σz])\n",
    "        range_max = maximum([μx+2*σx, μy+2*σy, μz+2*σz])\n",
    "        range = linspace(range_min, range_max, 100)\n",
    "        plot(range, pdf(x,range), \"k-\")\n",
    "        plot(range, pdf(y,range), \"b-\")\n",
    "        plot(range, pdf(z,range), \"r-\")\n",
    "        legend([L\"p_X\", L\"p_Y\", L\"p_Z\"])\n",
    "        grid()\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END OF CODE EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Expectation and Variance\n",
    "\n",
    "- $\\mathrm{E}[f] \\equiv  \\int f(x) \\,p(x) \\,\\mathrm{d}{x}$, **expected value** or **mean**\n",
    "\n",
    "- $\\mathrm{var}[f] \\equiv \\mathrm{E} \\left[(f(x)-\\mathrm{E}[f(x)])^2 \\right]$, **variance**\n",
    "\n",
    "- The **covariance** between _vectors_ $x$ and $y$,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathrm{cov}[x,y] &= \\mathrm{E}\\left[ (x-\\mathrm{E}[x]) (y^T-\\mathrm{E}[y^T]) \\right]\\\\\n",
    "    &= \\mathrm{E}[x y^T] - \\mathrm{E}[x]\\mathrm{E}[y^T]\n",
    "\\end{align*}\n",
    "\n",
    "-  Also useful as: $\\mathrm{E}[xy^T] = \\mathrm{cov}[x,y] + \\mathrm{E}[x]\\mathrm{E}[y^T]$\n",
    "\n",
    "\n",
    "### Linear Transformations\n",
    "\n",
    "No matter how $x$ is distributed, we can easily derive that **(do as exercise)**\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{E}[Ax +b] &= A\\mathrm{E}[x] + b \\tag{SRG-3a}\\\\\n",
    "\\mathrm{cov}[Ax +b] &= A\\,\\mathrm{cov}[x]\\,A^T \\tag{SRG-3b}\n",
    "\\end{align}\n",
    "\n",
    "-  (The tag (SRG-3a) refers to the corresponding eqn number in Sam Roweis' Gaussian Identities notes.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Mean and Variance for the Sum of Two Variables\n",
    "\n",
    "For any distribution of $x$ and $y$ and $z=x+y$,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathrm{E}[z] &= \\int_z z \\left[\\int_x p_x(x)p_y(z-x) \\,\\mathrm{d}{x} \\right] \\,\\mathrm{d}{z} \\\\\n",
    "&= \\int_x p_x(x) \\left[ \\int_z z p_y(z-x)\\,\\mathrm{d}{z} \\right] \\,\\mathrm{d}{x}  \\\\\n",
    "    &= \\int_x p_x(x) \\left[ \\int_{y^\\prime} (y^\\prime +x)p_y(y^\\prime)\\,\\mathrm{d}{y^\\prime} \\right] \\,\\mathrm{d}{x} \\notag\\\\\n",
    "&= \\int_x p_x(x) \\left( \\mathrm{E}[y]+x \\right) \\,\\mathrm{d}{x} \\notag\\\\\n",
    "    &= \\mathrm{E}[x] + \\mathrm{E}[y] \\qquad \\text{(always; follows from SRG-3a)}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Derive as an exercise that\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{var}[z] &= \\mathrm{var}[x] + \\mathrm{var}[y] + 2\\mathrm{cov}[x,y] \\qquad \\text{(always, see SRG-3b)} \\notag\\\\\n",
    "    &= \\mathrm{var}[x] + \\mathrm{var}[y] \\qquad \\text{(if X and Y are independent)}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Review Probability Theory\n",
    "\n",
    "- Interpretation as a degree of belief, i.e. a state-of-knowledge, not as a property of nature.\n",
    "- We can do everything with only the **sum rule** and the **product rule**. In practice, **Bayes rule** and **marginalization** are often very useful for computing\n",
    "\n",
    "$$p(\\text{what-we-want}|\\,\\text{what-we-know})\\,.$$\n",
    "\n",
    "- Bayes rule $ p(\\theta|D) = \\frac{p(D|\\theta)p(\\theta)} {p(D)} $ is the fundamental rule for learning!\n",
    "\n",
    "- That's really about all you need to know about probability theory, but you need to _really_ know it, so do the exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.cell { /* set cell width */\n",
       "    width: 750px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    width: 1000px;\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:600px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.css\") do f\n",
    "    display(\"text/html\", readall(f))\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
