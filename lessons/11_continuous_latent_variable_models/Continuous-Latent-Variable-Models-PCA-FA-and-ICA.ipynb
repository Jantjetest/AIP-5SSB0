{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Latent Variable Models - PCA, FA and ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Introduction to Linear Latent Variable Models (LLVM) on continuous domains, including factor analysis, principal component analysis and independent component analysis\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional\n",
    "    - Bishop pp. 570-573, 577-580, 584-586 (PCA and FA)\n",
    "    - Bishop pp. 591-592 (ICA)\n",
    "    - M. Tipping and C. Bishop, [Probabilistic Principal Component Analysis](./files/bishop-ppca-jrss.pdf), Journal of the Royal Statistical Society. Series B, Vol.61, No.3, 1999 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Continuous Latent Variable Models\n",
    "\n",
    "-  (Recall that) mixture models use a discrete class variable.\n",
    "\n",
    "-  Sometimes, it is more appropriate to think in terms of **continuous**\n",
    "underlying causes (factors) that control the observed data.\n",
    "\n",
    "  -  E.g., observe test results for subjects: English, Spanish and French\n",
    "\n",
    "$$\\begin{align*}\n",
    "  \\underbrace{ \\begin{bmatrix} x_1\\;(=\\text{English})\\\\ x_2\\;(=\\text{Spanish})\\\\ x_3\\;(=\\text{French}) \\end{bmatrix} }_{\\text{observed}}% &= f(\\text{causes},\\theta) + \\text{noise}\\\\\n",
    "&= \\begin{bmatrix} \\lambda_{11},\\lambda_{12}\\\\ \\lambda_{21},\\lambda_{22}\\\\ \\lambda_{31},\\lambda_{32}\\end{bmatrix} \\cdot \\underbrace{ \\begin{bmatrix} z_1\\;(=\\text{literacy})\\\\ z_2\\;(=\\text{intelligence})\\end{bmatrix} }_{\\text{causes}} +    \\underbrace{\\begin{bmatrix} v_1\\\\v_2\\\\v_3\\end{bmatrix} }_{\\text{noise}}\n",
    "\\end{align*}$$\n",
    "\n",
    "- (**Unsupervised Regression**). This is like (linear) regression with unobserved inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Dimensionality Reduction\n",
    "\n",
    "-  If the dimension for the hidden 'causes' ($z$) is smaller than for the observed data ($x$), then the model (tries to) achieve **dimensionality reduction**.\n",
    "\n",
    "-  Key applications include \n",
    "  1. **compression** (store $z$ rather than $x$) \n",
    "    - Compression through **real-valued** latent variables can be far more efficient than with discrete clusters.\n",
    "    - E.g., with two 8-bit hidden factors, one can describe $2^{16}\\approx 10^5$ settings; this would take $2^{16}$ clusters!\n",
    "  2. **noise reduction** (e.g. in biomedical, financial or speech signals)\n",
    "  3. **feature extraction** (e.g. as a pre-processor for classification) \n",
    "  4. **visualization** (particularly if $\\mathrm{dim}(Z)=2$)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Model Specification for LC-LVM\n",
    "\n",
    "- In this lesson, we focus on _Linear_ Continuous Latent Variable Models (**LC-LVM**).  \n",
    "\n",
    "-  Introduce observation vector ${x}\\in\\mathbb{R}^D$ and $M$-dim ($M<D$) real-valued **latent factor**  $z$:\n",
    "$$\\begin{align*}\n",
    "  x &= W z + \\mu + \\epsilon \\\\\n",
    "  z &\\sim \\mathcal{N}(0,I) \\\\\n",
    "  \\epsilon &\\sim \\mathcal{N}(0,\\Psi)\n",
    "\\end{align*}$$\n",
    "or equivalently\n",
    "$$\\begin{align*}\n",
    "p(x|z,\\theta) &= \\mathcal{N}(x|\\,W z + \\mu,\\Psi) \\tag{likelihood}\\\\\n",
    "p(z) &= \\mathcal{N}(z|\\,0,I) \\tag{prior}\n",
    "\\end{align*}$$\n",
    "where $W$ is the $(D\\times M)$-dim **factor loading matrix** \n",
    "\n",
    "- As we will see, the **observation noise covariance matrix** $\\Psi$ is always **diagonal** for interesting LC-LVM models. \n",
    "\n",
    "- Note that LC-LVM is very similar to very to linear regression: $p(y|x) = \\mathcal{N}(y|\\theta^T {x}, \\sigma^2)$. \n",
    "\n",
    "- Note also that the components of the hidden variables $Z$ are **statistically independent** of each other; the components of the observed vector $X$ may be correlated.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  LC-LVM Analysis (1): The marginal distribution $p({x})$\n",
    "\n",
    "-  Since the product of Gaussians is Gaussian, both the joint $p(x,z) = p(x|z)p(z)$, the marginal $p(x)$ and the conditional\n",
    "$p(z|x)$ distributions are also Gaussian.\n",
    "\n",
    "- The marginal distribution for the observed data is\n",
    "$$\n",
    "\\boxed{ p(x) = \\mathcal{N}({x}|\\,{\\mu},W W^T + \\Psi) } \n",
    "$$\n",
    "since the **mean** evaluates to \n",
    "$$\\begin{align*}\n",
    "\\mathrm{E}[x] &= \\mathrm{E}[W z + \\mu+ \\epsilon] \\\\\n",
    " &= W \\mathrm{E}[z] + \\mu + \\mathrm{E}[\\epsilon] \\\\\n",
    " &= \\mu \n",
    "\\end{align*}$$\n",
    "and the **covariance** matrix is\n",
    "$$\\begin{align*}\n",
    "\\mathrm{cov}[x] &= \\mathrm{E}[({x}-{\\mu})({x}-{\\mu})^T] \\\\\n",
    "  &=  \\mathrm{E}[(W z +\\epsilon)(W z +\\epsilon^T] \\\\\n",
    "   &= W \\mathrm{E}[z z^T] W^T + \\mathrm{E}[\\epsilon \\epsilon^T] \\\\\n",
    "   &= W W^T + \\Psi \n",
    "\\end{align*}$$\n",
    "\n",
    "$\\Rightarrow$ **LC-LVM is just a MultiVariate Gaussian (MVG) model** $x \\sim \\mathcal{N}({\\mu},\\Sigma)$ with the restriction that $\\Sigma= W W^T + \\Psi$.\n",
    "\n",
    "-  The effective covariance is the low-rank outer product of two\n",
    "long skinny matrices plus a diagonal matrix.\n",
    "\n",
    "\n",
    "<img src=\"./figures/fig-FA-eq1.png\" width=\"400px\">\n",
    "\n",
    "$\\Rightarrow$ LC-LVM provides a MVG model of **intermediate complexity**. Compare the number of free parameters:\n",
    "  - $D(D+1)/2$ for full Gaussian covariance $\\Sigma$\n",
    "  - $D(M+1)$  for LC-LVM model where $\\Sigma = W W^T + \\Psi$. \n",
    "  - $D$ for diagonal Gaussian covariance $\\Sigma = \\mathrm{diag}(\\sigma_i^2)$\n",
    "  - $1$ for isotropic Gaussian noise $\\Sigma = \\sigma^2 \\mathrm{I}$\n",
    " \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  LC-LVM Analysis (2): The Factor Loading Matrix $W$ is Not Unique\n",
    "\n",
    "-  The factor loading matrix $W$ can only be estimated up to a rotation matrix $R$. Namely, if we rotate $W \\rightarrow WR $, then the covariance matrix for observations $x$ does not change (N.B.: a rotation (or orthogonal) matrix $R$ is a matrix such that $R^TR = R R^T = I$):\n",
    "\n",
    "$$\n",
    "W R (W R)^T + \\Psi = W R R^T W^T + \\Psi = W W^T + \\Psi\n",
    "$$\n",
    "\n",
    "\n",
    "$\\Rightarrow$ Two persons that estimate ML parameters for FA on the same data are **not guaranteed to find the same parameters**, since any rotation of $W$ is equally likely.\n",
    "\n",
    "$\\Rightarrow$ we can infer latent **subspaces** rather than individual components. One has to be careful when interpreting the numerical values of $W$ and $z$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LC-LVM analysis (3): Constraints on the Noise Variance $\\Psi$\n",
    "\n",
    "-  When doing ML estimation for the parameters, a trivial solution for the covariance matrix $\\Sigma_x = W W^T + \\Psi$ is setting $\\hat W=0$ and $\\hat\\Psi$ equal to the sample variance of the data.\n",
    "\n",
    "-  In this case, all data correlation is explained as noise. (We'd like to avoid this).\n",
    "\n",
    "$\\Rightarrow$ The LC-LVM model is uninteresting without some restriction on the observation noise covariance matrix $\\Psi$. \n",
    "\n",
    "- The interesting cases are mostly for diagonal $\\Psi$. Note that if $\\Psi$ is diagonal, all correlations between the $(D)$ components of $x$ **must be explained** by the rank-$M$ matrix $W W^T$. \n",
    "\n",
    "##### Factor Anaysis \n",
    "\n",
    "- In Factor Analysis (**FA**), $\\Psi$ is restricted to be _diagonal_:\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\Psi = \\mathrm{diag}(\\psi_i) \n",
    "\\end{align*}$$\n",
    "\n",
    "##### Probabilistic Principal Component Analysis \n",
    "\n",
    "- In Probabilistic Principal Component Analysis (**pPCA**), the variances are further restricted to be the same,\n",
    " \n",
    "$$\\begin{align*} \n",
    "\\Psi = \\sigma^2 I \n",
    "\\end{align*}$$\n",
    "\n",
    "##### Principal Component Analysis \n",
    "\n",
    "- The 'regular' (deterministic) Principal Component Analysis (**PCA**) procedure can be obtained by further requiring that\n",
    "$$\\begin{align*} \n",
    "\\Psi &= \\lim_{\\sigma^2\\rightarrow 0} \\sigma^2 I \\\\\n",
    "W^T W &= I\n",
    "\\end{align*}$$ \n",
    "i.e., the noise model is discarded altogether and the columns of $W$ are orthonormal. \n",
    "\n",
    "- Regular PCA is a well-known deterministic procedure for dimensionality reduction (that predates pPCA).\n",
    "\n",
    "$\\Rightarrow$ FA, pPCA and PCA differ by their model for the noise variance $\\Psi$ (namely, diagonal, isotropic and 'zeros')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Typical Applications\n",
    "-  In PCA (or pPCA), the noise variance is assumed to be the same for all components. This is appropriate if all components of the observed data are 'shifted' versions of each other.\n",
    "\n",
    "$\\Rightarrow$ **PCA is very widely applied to image and signal processing tasks!**\n",
    "\n",
    "-  Google (May-2015): [PCA \"face recognition\"] $>$ 300K hits; [PCA \"noise reduction\"] $>$ 100K hits \n",
    "-  FA is insensitive to scaling of individual components in the observed data (see appendix).\n",
    "-  Use FA if the data are not shifted versions of the same kind.\n",
    "\n",
    "$\\Rightarrow$ **FA has strong history in 'social sciences'**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  ML estimation for pPCA Model\n",
    "\n",
    "- Given the generative model for pPCA \n",
    "$$\\begin{align*}\n",
    "p(x_n|z_n) &= \\mathcal{N}(x_n\\mid W z_n + \\mu,\\sigma^2 \\mathrm{I})\\\\\n",
    "p(z_n) &= \\mathcal{N}(z_n \\mid0,\\mathrm{I})\n",
    "\\end{align*}$$\n",
    "and observations ${D}=\\{x_1,\\dotsc,x_N\\}$, find ML estimates for the parameters $\\theta=\\{W,\\mu,\\sigma\\}$ \n",
    "\n",
    "- **Inference for ${\\mu}$** is easy: ${x}$ is a multivariate Gaussian with mean ${\\mu}$, so its ML estimate is\n",
    "$$ \\hat {\\mu} = \\frac{1}{N}\\sum_n {x}_n$$\n",
    "Now subtract $\\hat {\\mu}$ from all data points (${x}_n:= {x}_n-\\hat {\\mu}$) and assume that we have zero-mean data.\n",
    "\n",
    "##### Solution method 1: Gradient-ascent on the log-likelihood. \n",
    "\n",
    "- Work out the gradients for the log-likelihood\n",
    "$$\\log p({D}|{\\theta}) = -\\frac{N}{2} \\log \\lvert 2\\pi(W W^T + \\sigma^2 \\mathrm{I})\\rvert  -\\frac{1}{2}\\sum_n {x}_n^T(W W^T + \\sigma^2 \\mathrm{I})^{-1}{x}_n$$\n",
    "and optimize w.r.t. $W$ and $\\sigma^2$. This turns out to be quite difficult since it is not possible to decouple $W$ and $\\sigma^2$ (but it is possible, see [Tipping and Bishop, 1999](./files/bishop-ppca-jrss.pdf)).\n",
    "\n",
    "##### Solution method 2: Use EM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "###  Inference for the Hidden Variables, $p(z|x)$\n",
    "\n",
    "- Both for dimensionality reduction and the E-step in EM-based parameter estimation, we need $p(z|{x})$.\n",
    "-  (Use Bayes rule). We first derive the joint density $p({x},z)$ and then condition to $p(z|{x})$\n",
    "-  The covariance between ${x}$ and $z$\n",
    "\\begin{align}\n",
    "\\mathrm{cov}[z,x] &= \\mathrm{E}[(z-0)(x-\\mu)^T] \\\\\n",
    "  &= \\mathrm{E}[z (W z +\\epsilon)^T] \\\\\n",
    "  &= \\mathrm{E}[z z^T]W^T + \\mathrm{E}[z \\epsilon^T]= W^T\n",
    "\\end{align}\n",
    "\n",
    "-  Now we can write out the joint distribution $p(x,z)$,\n",
    "$$\n",
    "p(x,z|{\\theta}) = \\mathcal{N} \\left( \\begin{bmatrix} z \\\\{x}\\end{bmatrix} \\left|\\, \\begin{bmatrix} 0\\\\ {\\mu} \\end{bmatrix}, \\begin{bmatrix} I & W^T \\\\ W & W W^T + \\Psi \\end{bmatrix} \\right. \\right)\n",
    "$$\n",
    "\n",
    "-  Direct application of the formula for gaussian-conditioning (SRG-5d) leads to the posterior $p(z|{x})$\n",
    "$$\n",
    "p(z|{x}) = \\mathcal{N}\\left( z| \\tilde{V}(x-\\mu),\\,I-\\tilde{V}\\Psi \\right)\n",
    "$$\n",
    "with $\\tilde{V} = W^T(W W^T + \\Psi)^{-1}$.\n",
    "\n",
    "- In principle, we're done for infering $p(z|{x})$, but note that computing $\\tilde{V}$ requires inversion of a $D\\times D$ matrix. The computational load can be substantially reduced to inversion of a $M\\times M$ matrix (size of $z$), after application of the **matrix inversion identity**, (Bishop eqn. C.7),\n",
    "$$\n",
    "(A+XBX^T)^{-1} = A^{-1} - A^{-1}X(B^{-1}+X^TA^{-1}X)^{-1}X^TA^{-1}\n",
    "$$\n",
    "\n",
    "- Substitution of this identity into the previous formula for $p(z|{x})$ gives\n",
    "\\begin{align}\n",
    "p(z|x) &= \\mathcal{N} \\left( z |m,V \\right) \\\\\n",
    "  m &= V W^T \\Psi^{-1}(x-\\mu) \\\\\n",
    "  V &= (I + W^T \\Psi^{-1} W)^{-1}\n",
    "\\end{align}\n",
    "which relies on an $M\\times M$ matrix inversion.\n",
    "\n",
    "- Note that when ${x}$ is given, the unobserved input $z$ is not known exactly; the uncertainty about input $z$, as expressed by the variance $V=(I + W^T \\Psi^{-1} W)^{-1}$, can be computed _before the data point ${x}$ has been seen_.\n",
    "\n",
    "- Compare this to linear regression, where we have full knowledge about an input-output pair $({x},y)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### EM for Factor Analysis\n",
    "\n",
    "-  Subtract sample mean $\\hat{\\mu} = \\frac{1}{N}\\sum_n {x}_n$ from all data points.\n",
    "#### E-step\n",
    "  - For each data point ${x}_n$, compute the posterior distribution of hidden factors, given the observed data,\n",
    "\\begin{align}\n",
    "\\hat{q}_n(z) &\\triangleq p(z|x_n,\\hat{\\theta}^{-}) = \\mathcal{N}(z|m_n,V)\\\\\n",
    "V &= (I + W^T \\Psi^{-1} W)^{-1} \\\\\n",
    "m_n &= V W^T \\Psi^{-1} x_n\n",
    "\\end{align}\n",
    "\n",
    "#### M-step\n",
    "  - Maximize $\\mathrm{F}(\\theta,\\hat{q})$ w.r.t. ${\\theta}$, (see section ... for details)\n",
    "\\begin{align}\n",
    "\\hat{W} &= \\left( \\sum_n x_n m_n^T \\right) \\left( N V + \\sum_n m_n m_n^T \\right)^{-1}\\\\\n",
    "\\hat \\Psi &= \\hat{W} V \\hat{W}^T + \\frac{1}{N} \\sum_n (x_n-W m_n)(x_n-W m_n)^T \n",
    "\\end{align}\n",
    "\n",
    "- Note again the close relationship between the M-step iteration for LC-LVM and the ML solution for linear regression (in altered notation $x_n=\\theta^T u_n + \\mathcal{N}(0,\\sigma^2 I)$,\n",
    "$$ \n",
    "\\hat{\\theta} = (U^T U)^{-1}U^T x = \\left( \\sum_n u_n u_n^T \\right)^{-1} \\left( \\sum_n u_n x_n^T \\right)\n",
    "$$\n",
    "\n",
    "- The uncertainty about the inputs $z_n$, expressed by variance $V$ is the essential difference between LC-LVM and regression.\n",
    "\n",
    "- In general, as $V= (I + W^T \\Psi^{-1} W)^{-1} \\rightarrow 0$, the equations for linear regression are recovered.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Problem Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Example: Noise Reduction by Auto-Encoder\n",
    "\n",
    "\\begin{center}\\includegraphics[width=9cm]{./figures/fig-diabolo}\\end{center}\n",
    "\n",
    "- [Q.] Suppress noise in observation ${x}$ on the basis of observed data set $D=\\{x_1,\\dotsc,x_N\\}$.\n",
    "- [A.] Use ${D}$ to train FA (or pPCA) network and recover cleaned version $\\hat {x}$ from ${x}$ as\n",
    "$$\n",
    "\\hat {x} = \\underbrace{W}_{(D \\times M)} \\cdot \\underbrace{ V W^T\\Psi^{-1} }_{ (M \\times D)} \\cdot {x}\n",
    "$$\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap LC-LVM Models\n",
    "\n",
    "- Model specification\n",
    "- LC-LVM is just a Gaussian density model for $X$ with restricted covariance matrix $$\n",
    "- The factor loading matrix is not unique\n",
    "- Different names for different assumptions on the diagonal noise covariance matrix\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Independent Component Analysis (ICA)\n",
    "\n",
    "- Often, in nature the observed data are mixtures of **independent non-Gaussian** sources, e.g., cocktail party\n",
    "- The ICA (a.k.a. blind source separation) model is aimed at these kind of problems,\n",
    "\n",
    "\\begin{align}\n",
    "x &= W z + \\text{noise} \\quad &\\text{(x observed)} \\\\\n",
    "p(z) &= \\prod_k p(z_k) \\quad &\\text{(unknown non-Gaussian factorized prior)}\n",
    "\\end{align}\n",
    "\n",
    "-  Many sensor noise models and many algorithms (often EM-based) for estimation of the sources $z$ and the mixing matrix $W$ have been developed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Demo ICA; Blind Signal Separation\n",
    "\\begin{center}\\includegraphics[width=11cm]{./figures/fig-bss}\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----\n",
    "_The cell below loads the style file_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!--\n",
       "This HTML file contains custom styles and some javascript.\n",
       "Include it a Jupyter notebook for improved rendering.\n",
       "-->\n",
       "\n",
       "<!-- Fonts -->\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "\n",
       "<!-- Custom style -->\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.cell { /* set cell width */\n",
       "    width: 750px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    width: 1000px;\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:600px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       ".emphasis {\n",
       "    color: red;\n",
       "}\n",
       "\n",
       ".exercise {\n",
       "    color: green;\n",
       "}\n",
       "\n",
       ".proof {\n",
       "    color: blue;\n",
       "}\n",
       "\n",
       "code {\n",
       "  padding: 2px 4px !important;\n",
       "  font-size: 90% !important;\n",
       "  color: #222 !important;\n",
       "  background-color: #efefef !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       "</style>\n",
       "\n",
       "<!-- MathJax styling -->\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.html\") do f\n",
    "    display(\"text/html\", readall(f))\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.1",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
