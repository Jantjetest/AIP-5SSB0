{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Latent Variable Models - PCA and FA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Introduction to Linear Latent Variable Models (LLVM) on continuous domains, specifically factor analysis and principal component analysis\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional\n",
    "    - Bishop pp. 570-573, 577-580, 584-586 (PCA and FA)\n",
    "    - M. Tipping and C. Bishop, [Probabilistic Principal Component Analysis](./files/bishop-ppca-jrss.pdf), Journal of the Royal Statistical Society. Series B, Vol.61, No.3, 1999 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Continuous Latent Variable Models\n",
    "\n",
    "-  (Recall that) mixture models use a discrete class variable.\n",
    "\n",
    "-  Sometimes, it is more appropriate to think in terms of **continuous**\n",
    "underlying causes (factors) that control the observed data.\n",
    "\n",
    "  -  E.g., observe test results for subjects: English, Spanish and French\n",
    "\n",
    "$$\\begin{align*}\n",
    "  \\underbrace{ \\begin{bmatrix} x_1\\;(=\\text{English})\\\\ x_2\\;(=\\text{Spanish})\\\\ x_3\\;(=\\text{French}) \\end{bmatrix} }_{\\text{observed}}% &= f(\\text{causes},\\theta) + \\text{noise}\\\\\n",
    "&= \\begin{bmatrix} \\lambda_{11},\\lambda_{12}\\\\ \\lambda_{21},\\lambda_{22}\\\\ \\lambda_{31},\\lambda_{32}\\end{bmatrix} \\cdot \\underbrace{ \\begin{bmatrix} z_1\\;(=\\text{literacy})\\\\ z_2\\;(=\\text{intelligence})\\end{bmatrix} }_{\\text{causes}} +    \\underbrace{\\begin{bmatrix} v_1\\\\v_2\\\\v_3\\end{bmatrix} }_{\\text{noise}}\n",
    "\\end{align*}$$\n",
    "\n",
    "- (**Unsupervised Regression**). This is like (linear) regression with unobserved inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Dimensionality Reduction\n",
    "\n",
    "-  If the dimension for the hidden 'causes' ($z$) is smaller than for the observed data ($x$), then the model (tries to) achieve **dimensionality reduction**.\n",
    "\n",
    "-  Key applications include \n",
    "  1. **compression** (store $z$ rather than $x$) \n",
    "    - Compression through **real-valued** latent variables can be far more efficient than with discrete clusters.\n",
    "    - E.g., with two 8-bit hidden factors, one can describe $2^{16}\\approx 10^5$ settings; this would take $2^{16}$ clusters!\n",
    "  2. **noise reduction** (e.g. in biomedical, financial or speech signals)\n",
    "  3. **feature extraction** (e.g. as a pre-processor for classification) \n",
    "  4. **visualization** (particularly if $\\mathrm{dim}(Z)=2$)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Problem: Visualization with missing data\n",
    "\n",
    "\n",
    "- <span class=\"exercise\">We consider 38 examples from the 18-dimensional data set from the\n",
    "_Tobamovirus_ data set, see section 4.1 in [Tipping and Bishop (1999)](./files/bishop-ppca-jrss.pdf) (Originally from [Ripley (1996)](https://www.stats.ox.ac.uk/pub/PRNN/). Let's visualize this data set after projection onto the two principal axes (i.e., axes that explain largest data variance). \n",
    "</span>\n",
    "\n",
    "- We will also consider the visualization problem when 20% of the data set is missing. \n",
    "\n",
    "Marco, can you show the data set here as a 38 by 18 matrix, generated by julia\n",
    "\n",
    "[here is teh data](../files/datasets/virus3.dat) downloaded from https://www.stats.ox.ac.uk/pub/PRNN/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Model Specification for LC-LVM\n",
    "\n",
    "- In this lesson, we focus on _Linear_ Continuous Latent Variable Models (**LC-LVM**).  \n",
    "\n",
    "-  Introduce observation vector ${x}\\in\\mathbb{R}^D$ and $M$-dim ($M<D$) real-valued **latent factor**  $z$:\n",
    "$$\\begin{align*}\n",
    "  x &= W z + \\mu + \\epsilon \\\\\n",
    "  z &\\sim \\mathcal{N}(0,I) \\\\\n",
    "  \\epsilon &\\sim \\mathcal{N}(0,\\Psi)\n",
    "\\end{align*}$$\n",
    "or equivalently\n",
    "$$\\begin{align*}\n",
    "p(x|z,\\theta) &= \\mathcal{N}(x|\\,W z + \\mu,\\Psi) \\tag{likelihood}\\\\\n",
    "p(z) &= \\mathcal{N}(z|\\,0,I) \\tag{prior}\n",
    "\\end{align*}$$\n",
    "where $W$ is the $(D\\times M)$-dim **factor loading matrix** \n",
    "\n",
    "- As we will see, the **observation noise covariance matrix** $\\Psi$ is always **diagonal** for interesting LC-LVM models. \n",
    "\n",
    "- Note that LC-LVM is very similar to very to [linear regression](http://nbviewer.ipython.org/github/bertdv/AIP-5SSB0/blob/master/lessons/06_linear_regression/Linear-Regression.ipynb): \n",
    "$$p(y|x) = \\mathcal{N}(y|\\theta^T {x}, \\sigma^2)$$\n",
    "\n",
    "- Note also that the components of the hidden variables $Z$ are **statistically independent** of each other; the components of the observed vector $X$ may be correlated.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  LC-LVM Analysis (1): The marginal distribution $p({x})$\n",
    "\n",
    "-  Since the product of Gaussians is Gaussian, both the joint $p(x,z) = p(x|z)p(z)$, the marginal $p(x)$ and the conditional\n",
    "$p(z|x)$ distributions are also Gaussian.\n",
    "\n",
    "- The marginal distribution for the observed data is\n",
    "$$\n",
    "\\boxed{ p(x) = \\mathcal{N}({x}|\\,{\\mu},W W^T + \\Psi) } \n",
    "$$\n",
    "since the **mean** evaluates to \n",
    "$$\\begin{align*}\n",
    "\\mathrm{E}[x] &= \\mathrm{E}[W z + \\mu+ \\epsilon] \\\\\n",
    " &= W \\mathrm{E}[z] + \\mu + \\mathrm{E}[\\epsilon] \\\\\n",
    " &= \\mu \n",
    "\\end{align*}$$\n",
    "and the **covariance** matrix is\n",
    "$$\\begin{align*}\n",
    "\\mathrm{cov}[x] &= \\mathrm{E}[({x}-{\\mu})({x}-{\\mu})^T] \\\\\n",
    "  &=  \\mathrm{E}[(W z +\\epsilon)(W z +\\epsilon^T] \\\\\n",
    "   &= W \\mathrm{E}[z z^T] W^T + \\mathrm{E}[\\epsilon \\epsilon^T] \\\\\n",
    "   &= W W^T + \\Psi \n",
    "\\end{align*}$$\n",
    "\n",
    "$\\Rightarrow$ **LC-LVM is just a MultiVariate Gaussian (MVG) model** $x \\sim \\mathcal{N}({\\mu},\\Sigma)$ with the restriction that $\\Sigma= W W^T + \\Psi$.\n",
    "\n",
    "-  The effective covariance is the low-rank outer product of two\n",
    "long skinny matrices plus a diagonal matrix.\n",
    "\n",
    "\n",
    "<img src=\"./figures/fig-FA-eq1.png\" width=\"400px\">\n",
    "\n",
    "$\\Rightarrow$ LC-LVM provides a MVG model of **intermediate complexity**. Compare the number of free parameters:\n",
    "  - $D(D+1)/2$ for full Gaussian covariance $\\Sigma$\n",
    "  - $D(M+1)$  for LC-LVM model where $\\Sigma = W W^T + \\Psi$. \n",
    "  - $D$ for diagonal Gaussian covariance $\\Sigma = \\mathrm{diag}(\\sigma_i^2)$\n",
    "  - $1$ for isotropic Gaussian noise $\\Sigma = \\sigma^2 \\mathrm{I}$\n",
    " \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  LC-LVM Analysis (2): The Factor Loading Matrix $W$ is Not Unique\n",
    "\n",
    "-  The factor loading matrix $W$ can only be estimated up to a rotation matrix $R$. Namely, if we rotate $W \\rightarrow WR $, then the covariance matrix for observations $x$ does not change (N.B.: a rotation (or orthogonal) matrix $R$ is a matrix such that $R^TR = R R^T = I$):\n",
    "\n",
    "$$\n",
    "W R (W R)^T + \\Psi = W R R^T W^T + \\Psi = W W^T + \\Psi\n",
    "$$\n",
    "\n",
    "\n",
    "$\\Rightarrow$ Two persons that estimate ML parameters for FA on the same data are **not guaranteed to find the same parameters**, since any rotation of $W$ is equally likely.\n",
    "\n",
    "$\\Rightarrow$ we can infer latent **subspaces** rather than individual components. One has to be careful when interpreting the numerical values of $W$ and $z$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LC-LVM analysis (3): Constraints on the Noise Variance $\\Psi$\n",
    "\n",
    "-  When doing ML estimation for the parameters, a trivial solution for the covariance matrix $\\Sigma_x = W W^T + \\Psi$ is setting $\\hat W=0$ and $\\hat\\Psi$ equal to the sample variance of the data.\n",
    "\n",
    "-  In this case, all data correlation is explained as noise. (We'd like to avoid this).\n",
    "\n",
    "$\\Rightarrow$ The LC-LVM model is uninteresting without some restriction on the observation noise covariance matrix $\\Psi$. \n",
    "\n",
    "- The interesting cases are mostly for diagonal $\\Psi$. Note that if $\\Psi$ is diagonal, all correlations between the $(D)$ components of $x$ **must be explained** by the rank-$M$ matrix $W W^T$. \n",
    "\n",
    "##### Factor Anaysis \n",
    "\n",
    "- In Factor Analysis (**FA**), $\\Psi$ is restricted to be _diagonal_:\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\Psi = \\mathrm{diag}(\\psi_i) \n",
    "\\end{align*}$$\n",
    "\n",
    "##### Probabilistic Principal Component Analysis \n",
    "\n",
    "- In Probabilistic Principal Component Analysis (**pPCA**), the variances are further restricted to be the same,\n",
    " \n",
    "$$\\begin{align*} \n",
    "\\Psi = \\sigma^2 I \n",
    "\\end{align*}$$\n",
    "\n",
    "##### Principal Component Analysis \n",
    "\n",
    "- The 'regular' (deterministic) Principal Component Analysis (**PCA**) procedure can be obtained by further requiring that\n",
    "$$\\begin{align*} \n",
    "\\Psi &= \\lim_{\\sigma^2\\rightarrow 0} \\sigma^2 I \\\\\n",
    "W^T W &= I\n",
    "\\end{align*}$$ \n",
    "i.e., the noise model is discarded altogether and the columns of $W$ are orthonormal. \n",
    "\n",
    "- Regular PCA is a well-known deterministic procedure for dimensionality reduction (that predates pPCA).\n",
    "\n",
    "$\\Rightarrow$ FA, pPCA and PCA differ by their model for the noise variance $\\Psi$ (namely, diagonal, isotropic and 'zeros')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Typical Applications\n",
    "-  In PCA (or pPCA), the noise variance is assumed to be the same for all components. This is appropriate if all components of the observed data are 'shifted' versions of each other.\n",
    "\n",
    "$\\Rightarrow$ **PCA is very widely applied to image and signal processing tasks!**\n",
    "\n",
    "-  Google (May-2015): [PCA \"face recognition\"] $>$ 300K hits; [PCA \"noise reduction\"] $>$ 100K hits \n",
    "-  FA is insensitive to scaling of individual components in the observed data (see appendix).\n",
    "-  Use FA if the data are not shifted versions of the same kind.\n",
    "\n",
    "$\\Rightarrow$ **FA has strong history in 'social sciences'**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  ML estimation for pPCA Model\n",
    "\n",
    "- Given the generative model for pPCA \n",
    "$$\\begin{align*}\n",
    "p(x_n|z_n) &= \\mathcal{N}(x_n\\mid W z_n + \\mu,\\sigma^2 \\mathrm{I})\\\\\n",
    "p(z_n) &= \\mathcal{N}(z_n \\mid0,\\mathrm{I})\n",
    "\\end{align*}$$\n",
    "and observations ${D}=\\{x_1,\\dotsc,x_N\\}$, find ML estimates for the parameters $\\theta=\\{W,\\mu,\\sigma\\}$ \n",
    "\n",
    "- **Inference for ${\\mu}$** is easy: ${x}$ is a multivariate Gaussian with mean ${\\mu}$, so its ML estimate is\n",
    "$$ \\hat {\\mu} = \\frac{1}{N}\\sum_n {x}_n$$\n",
    "Now subtract $\\hat {\\mu}$ from all data points (${x}_n:= {x}_n-\\hat {\\mu}$) and assume that we have zero-mean data.\n",
    "\n",
    "- For ML estimation of $W$ and $\\sigma^2$, both gradient-ascent and EM are possible. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution method 1: Gradient-ascent on the log-likelihood \n",
    "\n",
    "- Work out the gradients for the log-likelihood\n",
    "$$\\log p({D}|{\\theta}) = -\\frac{N}{2} \\log \\lvert 2\\pi(W W^T + \\sigma^2 \\mathrm{I})\\rvert  -\\frac{1}{2}\\sum_n {x}_n^T(W W^T + \\sigma^2 \\mathrm{I})^{-1}{x}_n$$\n",
    "and optimize w.r.t. $W$ and $\\sigma^2$. \n",
    "\n",
    "- This turns out to be quite difficult since it is not possible to decouple $W$ and $\\sigma^2$ (but it is possible, see [Tipping and Bishop, 1999](./files/bishop-ppca-jrss.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution method 2: Use EM\n",
    "\n",
    "- A big bonus for EM over gradient-based methods is that EM comfortably handles missing observations, e.g. through sensor malfunction. Missing observations are simply treated as hidden variables. \n",
    "\n",
    "- Maximizing the _expected complete-data log-likelihood_ leads to the following  (see Bishop, pg.578 for derivation): \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\textbf{E-step}:& \\\\\n",
    "M &= W^T W + \\sigma^2 \\mathrm{I}\\\\\n",
    "\\mathrm{E}\\left[ z_n\\right] &= M^{-1} W^T x_n \\\\\n",
    "\\mathrm{E}\\left[ z_n z_n^T\\right] &= \\sigma^2 M^{-1} +  \\mathrm{E}\\left[ z_n\\right] \\mathrm{E}\\left[ z_n\\right]^T\\\\\n",
    "\\\\\n",
    "\\textbf{M-step}:& \\\\\n",
    "W_{\\text{new}} &= \\left[ \\sum_{n=1}^N x_n \\mathrm{E}\\left[z_n\\right]^T\\right] \\left[ \\sum_{n=1}^N \\mathrm{E}\\left[ z_n z_n^T\\right]\\right]^{-1} \\\\\n",
    "\\sigma^2_{\\text{new}} &= \\frac{1}{ND} \\sum_{n=1}^N \\left\\{ x_n^T x_n - 2 \\mathrm{E}\\left[z_n\\right]^T W_{\\text{new}}^T x_n + \\mathrm{Tr}\\left( \\mathrm{E}\\left[ z_n z_n^T\\right] W_{\\text{new}}^T W_{\\text{new}} \\right) \\right\\}\n",
    "\\end{align*}$$\n",
    "\n",
    "- Note that after $x_n$ is observed, the unobserved 'input' $z_n$ is not known exactly; the uncertainty about input $z_n$, as expressed by the covariance $$\\text{cov}(z_n) = \\mathrm{E}\\left[ z_n z_n^T\\right] - \\mathrm{E}\\left[ z_n\\right] \\mathrm{E}\\left[ z_n\\right] ^T = \\sigma^2 M^{-1}$$ can be computed _before the data point $x_n$ has been seen_.\n",
    "  - Compare this to linear regression, where we have full knowledge about an input-output pair.\n",
    "\n",
    "- If there was no uncertainty about $z_n$, i.e., $\\mathrm{E}\\left[ z_n\\right] = z_n$ and $\\mathrm{E}\\left[ z_n z_n^T\\right] = z_n z_n^T$, then\n",
    "$$\n",
    "W_{\\text{new}} = \\left[ \\sum_{n=1}^N x_n z_n^T\\right] \\left[ \\sum_{n=1}^N  z_n z_n^T\\right]^{-1}\n",
    "$$\n",
    "  - Verify that this solution resembles the [ML solution for linear regression](http://nbviewer.ipython.org/github/bertdv/AIP-5SSB0/blob/master/lessons/06_linear_regression/Linear-Regression.ipynb).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Problem Revisited\n",
    "\n",
    "<span class=\"exercise\">\n",
    "MARCO Can you generate a grpah like  Fig 1 in [Tipping and Bishop (1999)](./files/bishop-ppca-jrss.pdf) )\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----\n",
    "_The cell below loads the style file_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!--\n",
       "This HTML file contains custom styles and some javascript.\n",
       "Include it a Jupyter notebook for improved rendering.\n",
       "-->\n",
       "\n",
       "<!-- Fonts -->\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "\n",
       "<!-- Custom style -->\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.cell { /* set cell width */\n",
       "    width: 750px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    width: 1000px;\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:600px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       ".emphasis {\n",
       "    color: red;\n",
       "}\n",
       "\n",
       ".exercise {\n",
       "    color: green;\n",
       "}\n",
       "\n",
       ".proof {\n",
       "    color: blue;\n",
       "}\n",
       "\n",
       "code {\n",
       "  padding: 2px 4px !important;\n",
       "  font-size: 90% !important;\n",
       "  color: #222 !important;\n",
       "  background-color: #efefef !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       "/* This removes the actual style cells from the notebooks */\n",
       ".cell:nth-last-child(-n+2) {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "</style>\n",
       "\n",
       "<!-- MathJax styling -->\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.html\") do f\n",
    "    display(\"text/html\", readall(f))\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.2",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
