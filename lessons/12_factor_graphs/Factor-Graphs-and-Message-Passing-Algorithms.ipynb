{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Factor Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Introduction to Forney-style factor graphs and message passing algorithms\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes   \n",
    "    - [Loeliger, 2007](./files/Loeliger-2007-The-factor-graph-approach-to-model-based-signal-processing.pdf), pp. 1295-1300 \n",
    "  - Optional\n",
    "    - [Video lecture](https://www.youtube.com/watch?v=Fv2YbVg9Frc&t=31) by Frederico Wadehn (ETH Zurich)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Why Factor Graphs?\n",
    "\n",
    "- A probabilistic inference task gets its computational load mainly through the need for marginalization (i.e., computing integrals). E.g., for a generative model $p(x_1,x_2,x_3,x_4,x_5)$, the inference task $p(x_2|x_3)$ is given by \n",
    "\n",
    "$$\\begin{align*}\n",
    "p(x_2|x_3) = \\frac{\\int p(x_1,x_2,x_3,x_4,x_5) \\, \\mathrm{d}x_1  \\mathrm{d}x_4 \\mathrm{d}x_5}{\\int p(x_1,x_2,x_3,x_4,x_5) \\, \\mathrm{d}x_1  \\mathrm{d}x_2 \\mathrm{d}x_4 \\mathrm{d}x_5}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "- Since these computations suffer from the \"curse of dimensionality\", we often need to solve a simpler problem in order to get an answer. \n",
    "\n",
    "- Factor graphs provide an computationally efficient approach to solving inference problems **if the generative distribution can be factorized**. \n",
    "\n",
    "- Factorization helps. For instance, if $p(x_1,x_2,x_3,x_4,x_5) = p(x_1)p(x_2,x_3)p(x_4)p(x_5|x_4)$, then\n",
    "$$\\begin{align*}\n",
    "p(x_2|x_3) &= \\frac{\\int p(x_1)p(x_2,x_3)p(x_4)p(x_5|x_4) \\, \\mathrm{d}x_1  \\mathrm{d}x_4 \\mathrm{d}x_5}{\\int p(x_1)p(x_2,x_3)p(x_4)p(x_5|x_4) \\, \\mathrm{d}x_1  \\mathrm{d}x_2 \\mathrm{d}x_4 \\mathrm{d}x_5} \\\\\n",
    "  &= \\frac{p(x_2,x_3)}{\\int p(x_2,x_3) \\mathrm{d}x_2}\n",
    "\\end{align*}$$\n",
    "which is computationally much cheaper than the general case above.\n",
    "\n",
    "- In this lesson, we discuss how computationally efficient inference in factorized probability distributions can be automated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###  Construction Rules\n",
    "\n",
    "- Consider a function \n",
    "$$\n",
    "f(x_1,x_2,x_3,x_4,x_5) = f_a(x_1,x_2,x_3) \\cdot f_b(x_3,x_4,x_5) \\cdot f_c(x_4)\n",
    "$$\n",
    "\n",
    "- The factorization of this function can be graphically represented by a **Forney-style Factor Graph** (FFG):\n",
    "\n",
    "<img src=\"./figures/ffg-example-1.png\" width=\"400px\">\n",
    "\n",
    "- An FFG is an **undirected** graph subject to the following construction rules ([Forney, 2001](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=910573&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F18%2F19638%2F00910573.pdf%3Farnumber%3D910573))\n",
    "\n",
    "  1. A **node** for every factor;\n",
    "  1. An **edge** (or **half-edge**) for every variable;\n",
    "  1. Node $g$ is connected to edge $x$ **iff** variable $x$ appears in factor $g$.\n",
    "\n",
    "##### some terminology\n",
    "\n",
    "- $f$ is called the **global function** and $f_\\bullet$ are the **factors**. \n",
    "\n",
    "- A **configuration** is an assigment of values to all variables.\n",
    "\n",
    "- The **configuration space** is the set of all configurations, i.e., the domain of $f$\n",
    "\n",
    "- A configuration $\\omega=(x_1,x_2,x_3,x_4,x_5)$ is said to be **valid** iff $f(\\omega) \\neq 0$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###  Equality Nodes for Branching Points\n",
    "\n",
    "\n",
    "- Note that a variable can appear in maximally two factors in an FFG (since an edge has only two end points).\n",
    "\n",
    "- Consider the factorization (where $x_2$ appears in three factors) \n",
    "\n",
    "$$\n",
    " f(x_1,x_2,x_3,x_4) = f_a(x_1,x_2)\\cdot f_b(x_2,x_3) \\cdot f_c(x_2,x_4)\n",
    "$$\n",
    "\n",
    "- For the factor graph representation, we will instead consider the function $g$, defined as\n",
    "$$\\begin{align*}\n",
    " g(x_1,x_2&,x_2^\\prime,x_2^{\\prime\\prime},x_3,x_4) \\\\\n",
    "  &= f_a(x_1,x_2)\\cdot f_b(x_2^\\prime,x_3) \\cdot f_c(x_2^{\\prime\\prime},x_4) \\cdot f_=(x_2,x_2^\\prime,x_2^{\\prime\\prime})\n",
    "\\end{align*}$$\n",
    "  where \n",
    "$$\n",
    "f_=(x_2,x_2^\\prime,x_2^{\\prime\\prime}) \\triangleq \\delta(x_2-x_2^\\prime)\\, \\delta(x_2-x_2^{\\prime\\prime})\n",
    "$$\n",
    "  (where $\\delta$ is the Kronecker delta for discrete variables and the Dirac delta for continuously valued variables)\n",
    "  \n",
    "- Note that through introduction of auxiliary variables $X_2^\\prime$ and $X_2^{\\prime\\prime}$ each variable in $g$ appears in maximally two factors. \n",
    "\n",
    "- The constraint $f_=(x,x^\\prime,x^{\\prime\\prime})$ enforces that $X=X^\\prime=X^{\\prime\\prime}$ **for every valid configuration**.\n",
    "\n",
    "<img src=\"./figures/ffg-wEquality-node.png\" width=\"400px\">\n",
    "\n",
    "- Also note that $f$ is a marginal of $g$, since\n",
    "$$\n",
    "f(x_1,x_2,x_3,x_4) = \\int g(x_1,x_2,x_2^\\prime,x_2^{\\prime\\prime},x_3,x_4)\\, \\mathrm{d}x_2^\\prime \\mathrm{d}x_2^{\\prime\\prime}\n",
    "$$\n",
    "\n",
    "- Therefore, any inference problem on $f$ can be executed by a corresponding inference problem on $g$, e.g.,\n",
    "\n",
    "$$\\begin{align*}\n",
    "f(x_1 \\mid x_2) &\\triangleq \\frac{\\int f(x_1,x_2,x_3,x_4) \\,\\mathrm{d}x_3 \\mathrm{d}x_4 }{ \\int f(x_1,x_2,x_3,x_4) \\,\\mathrm{d}x_1 \\mathrm{d}x_3 \\mathrm{d}x_4} \\\\\n",
    "  &= \\frac{\\int g(x_1,x_2,x_2^\\prime,x_2^{\\prime\\prime},x_3,x_4) \\,\\mathrm{d}x_2^\\prime \\mathrm{d}x_2^{\\prime\\prime} \\mathrm{d}x_3 \\mathrm{d}x_4 }{ \\int g(x_1,x_2,x_2^\\prime,x_2^{\\prime\\prime},x_3,x_4) \\,\\mathrm{d}x_1 \\mathrm{d}x_2^\\prime \\mathrm{d}x_2^{\\prime\\prime} \\mathrm{d}x_3 \\mathrm{d}x_4} \\\\\n",
    "  &\\triangleq g(x_1 \\mid x_2)\n",
    "\\end{align*}$$\n",
    "\n",
    "- $\\Rightarrow$ **Any factorization of a global function $f$ can be represented by a Forney-style Factor Graph**.\n",
    "\n",
    "- More generally, equality nodes are useful to express hard constraints between variables. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Probabilistic Models as Factor Graphs\n",
    "\n",
    "- FFGs can be used to express conditional independence (factorization) in probabilistic models. \n",
    "\n",
    "- For example, the (previously shown) graph for \n",
    "$$f_a(x_1,x_2,x_3) \\cdot f_b(x_3,x_4,x_5) \\cdot f_c(x_4)$$ \n",
    "could represent the probabilistic model\n",
    "$$\n",
    "p(x_1,x_2,x_3,x_4,x_5) = p(x_1,x_2|x_3) \\cdot p(x_3,x_5|x_4) \\cdot p(x_4)\n",
    "$$\n",
    "where we identify \n",
    "$$\\begin{align*}\n",
    "f_a(x_1,x_2,x_3) &= p(x_1,x_2|x_3) \\\\\n",
    "f_b(x_3,x_4,x_5) &= p(x_3,x_5|x_4) \\\\\n",
    "f_c(x_4) &= p(x_4)\n",
    "\\end{align*}$$\n",
    "\n",
    "- This is the graph\n",
    "\n",
    "<img src=\"./figures/ffg-example-prob-model.png\" width=\"400px\">\n",
    "\n",
    "- Factorizations provide opportunities to cut on the amount of needed computations when doing inference. In what follows, we will use FFGs to process these opportunities in an automatic way (i.e., by message passing). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Inference by Closing Boxes\n",
    "\n",
    "- Assume we wish to compute the marginal\n",
    "$$\n",
    "\\bar{f}(x_3) = \\sum_{x_1,x_2,x_4,x_5,x_6,x_7}f(x_1,x_2,\\ldots,x_7)\n",
    "$$\n",
    "where $f$ is factorized as given by the following FFG\n",
    "\n",
    "<img src=\"./figures/ffg-message-passing-1.png\" width=\"500px\">\n",
    "\n",
    "- Due to the factorization, we can decompose this sum by the **distributive law** as\n",
    "$$\\begin{align*}\n",
    "\\bar{f}(x_3) = & \\underbrace{ \\left( \\sum_{x_1,x_2} f_a(x_1)\\,f_b(x_2)\\,f_c(x_1,x_2,x_3)\\right) }_{\\overrightarrow{\\mu}_{X_3}(x_3)}  \\\\\n",
    "  & \\underbrace{ \\cdot\\left( \\sum_{x_4,x_5} f_d(x_4)\\,f_e(x_3,x_4,x_5) \\cdot \\underbrace{ \\left( \\sum_{x_6,x_7} f_f(x_5,x_6,x_7)\\,f_g(x_7)\\right) }_{\\overleftarrow{\\mu}_{X_5}(x_5)} \\right) }_{\\overleftarrow{\\mu}_{X_3}(x_3)}\n",
    "\\end{align*}$$\n",
    "which is computationally (much) lighter than executing the full sum $\\sum_{x_1,\\ldots,x_7}f(x_1,x_2,\\ldots,x_7)$\n",
    "\n",
    "<img src=\"./figures/ffg-message-passing-2.png\" width=\"600px\">\n",
    "\n",
    "- Note that messages may flow in both directions on any particular edge (here on $X_3$). We often draw _directed edges_ in a FFG in order to distinguish forward messages $\\overrightarrow{\\mu}_\\bullet(\\cdot)$ (in the same direction as the arrow of the edge) from backward messages $\\overleftarrow{\\mu}_\\bullet(\\cdot)$ (in opposite direction). With directed edges, the FFG looks as follows: \n",
    "\n",
    "<img src=\"./figures/ffg-message-passing-3.png\" width=\"600px\">\n",
    "\n",
    "- Crucially, drawing arrows on edges is only meant as a notational convenience. Technically, an FFG is an undirected graph. \n",
    "\n",
    "- Also note that the message  $\\overleftarrow{\\mu}_{X_5}(x_5)$ is obtained by _multiplying all factors that are enclosed by the dashed boxes ($f_f$ and $f_g$), followed by marginalization over all enclosed variables_ ($x_6$ and $x_7$). \n",
    "\n",
    "- This is the <span class=\"emphasis\">\"Closing the Box\"</span>-rule, which is a general recipe for marginalization of hidden variables and leads to a new factor with outgoing (sum-product) message \n",
    "\n",
    "$$ \\mu_{\\text{SP}} = \\sum_{ \\stackrel{ \\textrm{enclosed} }{ \\textrm{variables} } } \\;\\prod_{\\stackrel{ \\textrm{enclosed} }{ \\textrm{factors} }}\n",
    "$$\n",
    "\n",
    "\n",
    "- Let's evaluate the closing-the-box rule for individual nodes. \n",
    "  - First, closing a box around the 'terminal' nodes leads to $\\overrightarrow{\\mu}_{X_1}(x_1) \\triangleq f_a(x_1)$, $\\overrightarrow{\\mu}_{X_2}(x_2) \\triangleq f_b(x_2)$ etc. \n",
    "  - The messages from internal nodes then evaluate to:\n",
    "$$\\begin{align*}\n",
    "\\overrightarrow{\\mu}_{X_3}(x_3) &= \\sum_{x_1,x_2} f_a(x_1) \\,f_b(x_2) \\,f_c(x_1,x_2,x_3) \\\\\n",
    "  &= \\sum_{x_1,x_2} \\overrightarrow{\\mu}_{X_1}(x_1) \\overrightarrow{\\mu}_{X_2}(x_2) \\,f_c(x_1,x_2,x_3) \\\\\n",
    "\\overleftarrow{\\mu}_{X_5}(x_5) &= \\sum_{x_6,x_7} f_f(x_5,x_6,x_7)\\,f_g(x_7) \\\\\n",
    " &= \\sum_{x_6,x_7} \\overrightarrow{\\mu}_{X_7}(x_7)\\, f_f(x_5,x_6,x_7) \\\\\n",
    "\\overleftarrow{\\mu}_{X_3}(x_3) &=  \\sum_{x_4,x_5} \\overleftarrow{\\mu}_{X_5}(x_5) \\,f_d(x_4)\\,f_e(x_3,x_4,x_5) \\\\\n",
    " &=  \\sum_{x_4,x_5} \\overleftarrow{\\mu}_{X_5}(x_5) \\,\\overrightarrow{\\mu}_{X_4}(x_4)\\,f_e(x_3,x_4,x_5)\n",
    "\\end{align*}$$\n",
    "  - Crucially, all message update rules can be computed from information that is **locally available** at each node.\n",
    "\n",
    "- (**Sum-Product update rule**). This recursive pattern for computing messages applies generally and is called the <span style=\"color:red\">Sum-Product update rule</span>, which is really just a special case of the closing-the-box rule: For any node, the outgoing message is obtained by taking the product of all incoming messages and the node function, followed by summing out (marginalization) all incoming variables. What is left (the outgoing message) is a function of the outgoing variable only: \n",
    "\n",
    "$$ \\boxed{\n",
    "\\overrightarrow{\\mu}_{Y}(y) = \\sum_{x_1,\\ldots,x_n} \\overrightarrow{\\mu}_{X_1}(x_1)\\cdots \\overrightarrow{\\mu}_{X_n}(x_n) \\,f(y,x_1,\\ldots,x_n) }\n",
    "$$\n",
    "\n",
    "<img src=\"./figures/ffg-sum-product.png\" width=\"400px\">\n",
    "\n",
    "- (**Sum-Product Theorem**). If the factor graph for a function $f$ has **no cycles**, then the marginal $\\bar{f}(x_3) = \\sum_{x_1,x_2,x_4,x_5,x_6,x_7}f(x_1,x_2,\\ldots,x_7)$ is given by the <span style=\"color:red\">Sum-Product Theorem</span>:\n",
    "\n",
    "$$ \\boxed{\n",
    "\\bar{f}(x_3) = \\overrightarrow{\\mu}_{X_3}(x_3)\\cdot \\overleftarrow{\\mu}_{X_3}(x_3)}\n",
    "$$\n",
    "\n",
    "$\\Rightarrow$ The marginal $\\bar{f}(x_3) = \\sum_{x_1,x_2,x_4,x_5,x_6,x_7}f(x_1,x_2,\\ldots,x_7)$ can be efficiently computed through sum-product messages. Executing inference through SP message passing is called the **Sum-Product Algorithm**.\n",
    "\n",
    "- <span class=\"exercise\">Verfiy for yourself that all maginals in a cycle-free graph (a tree) can be computed by starting with messages at the terminals and working towards the root of the tree.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Half-edges and Terminal Nodes\n",
    "\n",
    "- In order to complete the message passing framework, we need to deal with graph terminals and half-edges.  \n",
    "\n",
    "- Let us first consider a **Gaussian factor**  with given parameters $m=\\hat{m}$ and $V=\\hat{V}$: \n",
    "\n",
    "$$\n",
    "f_{\\mathcal{N}}(x)= \\mathcal{N}(x|\\hat{m},\\hat{V})\\, .\n",
    "$$  \n",
    "\n",
    "<img src=\"./figures/ffg-Gaussian-node.png\" width=\"300px\">\n",
    "\n",
    "- The outgoing sum-product message of this node is \n",
    "$$\\begin{align*}\n",
    "\\overrightarrow{\\mu}_X(x) = f_{\\mathcal{N}}(x)\n",
    "\\end{align*}$$\n",
    "since there are no internal variables to be integrated over (nor any incoming messages). Nodes that are connected to one edge only are called **Terminal Nodes**. \n",
    "\n",
    "- Note that the **outgoing message for a terminal node is the factor itself**.\n",
    "\n",
    "- Terminal nodes are very convenient to describe \n",
    "  - **prior distributions** over parameters, e.g., $$f_\\Theta(\\theta) = \\begin{cases} 1 & \\text{for}\\; 0 \\leq\\theta \\leq 1 \\\\ 0 & \\text{otherwise}\\end{cases} \\,;$$ \n",
    "  - **initial conditions** (which are also prior distributions) for state variables, e.g., $$f_{Z}(z_0) = \\mathcal{N}(\\theta|1,2)$$ specifies out state of knowledge about $Z$ at $t=0$;\n",
    "  - **observed variables**, e.g., use a factor $$f_Y(y) = \\delta(y-3)$$ to terminate the edge for variable $Y$ if $y=3$ is observed. \n",
    "  \n",
    "- Terminal nodes for _observed_ variables interface to the rest of the graph by sending messages as delta distributions. We usually draw terminal nodes for observed variables in the graph by smaller solid-black squares. This is just to help the visualization of the graph, since the computational rules are no different than for other nodes (see cell below).  \n",
    "\n",
    "- Finally, terminal nodes can also be related to unobserved half-edges. Consider the two models \n",
    "$$\\begin{align*}\n",
    "f(x_1,x_2) &= f_a(x_1)\\cdot f_b(x_1,x_2) \\\\\n",
    "f^\\prime (x_1,x_2) &= f_a(x_1)\\cdot f_b(x_1,x_2) \\cdot f_c(x_2)\n",
    "\\end{align*}$$\n",
    "with $f_c(x_2)=1$. For all valid configurations, both models evaluate to exactly the same value. Hence, the **message coming from the open end of a half-edge always equals $1$** and can be understood as the outgoing message from a terminal node with an uninformative (uniform) distribution.    \n",
    "\n",
    "- In short, we use terminal nodes to interface the graph to the external world by setting priors on states and parameters, by specifying observations (and, mostly useful for software implementation, by terminating unobserved half-edges).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Processing Observations in a Factor Graph\n",
    "\n",
    "- We shortly expand here on observed nodes. Consider a generative model \n",
    "$$p(x,y_1,y_2) = p(x)\\,p(y_1|x)\\,p(y_2|x) .$$ \n",
    "  - This model expresses the assumption that $Y_1$ and $Y_2$ are independent measurements of $X$.\n",
    "\n",
    "<img src=\"./figures/ffg-observations.png\" width=\"300px\">\n",
    "\n",
    "- Assume that we are interested in the posterior for $X$ after observing $Y_1= \\hat y_1$ and $Y_2= \\hat y_2$. The posterior for $X$ can be inferred by applying the sum-product algorithm to the following graph:\n",
    "\n",
    "<img src=\"./figures/ffg-observations-2.png\" width=\"450px\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### CODE EXAMPLE\n",
    "\n",
    "We'll use ForneyLab, a factor graph toolbox for Julia, to build the above graph, and perform sum-product message passing to infer the posterior $p(x|y_1,y_2)$. We assume $p(y_1|x)$ and $p(y_2|x)$ to be Gaussian likelihoods with known variances:\n",
    "$$\\begin{align*}\n",
    "    p(y_1\\,|\\,x) &= \\mathcal{N}(y_1\\,|\\,x, v_{y1}) \\\\\n",
    "    p(y_2\\,|\\,x) &= \\mathcal{N}(y_2\\,|\\,x, v_{y2})\n",
    "\\end{align*}$$\n",
    "Under this model, the posterior is given by:\n",
    "$$\\begin{align*}\n",
    "    p(x\\,|\\,y_1,y_2) &\\propto \\overbrace{p(y_1\\,|\\,x)\\,p(y_2\\,|\\,x)}^{\\text{likelihood}}\\,\\overbrace{p(x)}^{\\text{prior}} \\\\\n",
    "    &=\\mathcal{N}(x\\,|\\,\\hat{y}_1, v_{y1})\\, \\mathcal{N}(x\\,|\\,\\hat{y}_2, v_{y2}) \\, \\mathcal{N}(x\\,|\\,m_x, v_x) \n",
    "\\end{align*}$$\n",
    "so we can validate the answer by solving the Gaussian multiplication manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: Module ForneyLab not found in current path.\nRun `Pkg.add(\"ForneyLab\")` to install the ForneyLab package.",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Module ForneyLab not found in current path.\nRun `Pkg.add(\"ForneyLab\")` to install the ForneyLab package.",
      "",
      " in require(::Symbol) at .\\loading.jl:365"
     ]
    }
   ],
   "source": [
    "using ForneyLab # version 0.6.0\n",
    "\n",
    "# Data\n",
    "y1_hat = 1.0\n",
    "y2_hat = 2.0\n",
    "\n",
    "# Construct the required nodes\n",
    "g      = FactorGraph()\n",
    "t_y1   = TerminalNode(y1_hat; id=:y1) # y1 is observed to be y1_hat\n",
    "t_y2   = TerminalNode(y2_hat; id=:y2) # y2 is observed to be y2_hat\n",
    "p_x    = PriorNode(Gaussian(m=0.0, V=4.0), id=:prior_x) # node for prior p(x)\n",
    "lik_y1 = GaussianNode(V=1.0; id=:likelihood_y1) # likelihood p(y1|x) = N(x,1.0)\n",
    "lik_y2 = GaussianNode(V=2.0; id=:likelihood_y2) # likelihood p(y2|x) = N(x,2.0)\n",
    "equ    = EqualityNode()\n",
    "\n",
    "# Create edges to connect the nodes (from left to right in the depicted graph)\n",
    "Edge(t_y1, lik_y1.i[:out]) # [y1]----[p(y1|x)]\n",
    "Edge(lik_y1.i[:mean], equ) # [p(y1|x)]----[=]\n",
    "edge_X = Edge(p_x, equ)    # This edge represents X, and we want the sum-product msgs in both directions on this edge\n",
    "Edge(equ, lik_y2.i[:mean]) # [=]----[p(y2|x)]\n",
    "Edge(lik_y2.i[:out], t_y2) # [p(y1|x)]----[y2]\n",
    "\n",
    "# Perform sum-product message passing\n",
    "sum_product_algo = SumProduct(edge_X) # Automatically derives a message passing schedule\n",
    "run(sum_product_algo) # Execute the message passing algorithm\n",
    "x_marginal = ensureParameters!(calculateMarginal(edge_X), (:m,:V)) # make sure marginal has mean-variance parametrization\n",
    "println(\"Sum-product message passing result: p(x|y1,y2) = $(x_marginal)\")\n",
    "\n",
    "# Calculate mean and variance of p(x|y1,y2) manually by multiplying 3 Gaussians (see lesson 4 for details)\n",
    "v = 1 / (1/4 + 1/1 + 1/2)\n",
    "m = v * (0/4 + y1_hat/1.0 + y2_hat/2.0)\n",
    "println(\"Manual result: p(x|y1,y2) = N(m=$(round(m,2)), V=$(round(v,2)))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### END OF CODE EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### <span style=\"color:red\">(OPTIONAL SLIDE)</span> Example: SP Messages for the Equality Node\n",
    "\n",
    "- Let´s compute the SP messages for the **equality node** $f_=(x,y,z) = \\delta(z-x)\\delta(z-y)$: \n",
    "\n",
    "<img src=\"./figures/ffg-equality-node.png\" width=\"200px\">\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\overrightarrow{\\mu}_{Z}(z) &= \\int  \\overrightarrow{\\mu}_{X}(x) \\overrightarrow{\\mu}_{Y}(y) \\,\\delta(z-x)\\delta(z-y) \\,\\mathrm{d}x \\mathrm{d}y \\\\\n",
    "   &=  \\overrightarrow{\\mu}_{X}(z)  \\int  \\overrightarrow{\\mu}_{Y}(y) \\,\\delta(z-y) \\,\\mathrm{d}y \\\\\n",
    "   &=  \\overrightarrow{\\mu}_{X}(z) \\overrightarrow{\\mu}_{Y}(z) \n",
    "\\end{align*}$$\n",
    "\n",
    "- By symmetry, this also implies (for the same equality node) that\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\overleftarrow{\\mu}_{X}(x) &= \\overrightarrow{\\mu}_{Y}(x) \\overleftarrow{\\mu}_{Z}(x) \\quad \\text{and} \\\\\n",
    "\\overleftarrow{\\mu}_{Y}(y) &= \\overrightarrow{\\mu}_{X}(y) \\overleftarrow{\\mu}_{Z}(y)\\,.\n",
    "\\end{align*}$$\n",
    "\n",
    "- Let us now consider the case of Gaussian messages $\\overrightarrow{\\mu}_{X}(x) = \\mathcal{N}(\\overrightarrow{m}_X,\\overrightarrow{V}_X)$, $\\overrightarrow{\\mu}_{Y}(y) = \\mathcal{N}(\\overrightarrow{m}_Y,\\overrightarrow{V}_Y)$ and $\\overrightarrow{\\mu}_{Z}(z) = \\mathcal{N}(\\overrightarrow{m}_Z,\\overrightarrow{V}_Z)$. Let´s also define the precision matrices $\\overrightarrow{W}_X \\triangleq \\overrightarrow{V}_X^{-1}$ and similarly for $Y$ and $Z$. Then applying the SP update rule leads to multiplication of two Gaussian distributions, resulting in \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\overrightarrow{W}_Z &= \\overrightarrow{W}_X + \\overrightarrow{W}_Y \\\\ \n",
    "\\overrightarrow{W}_Z \\overrightarrow{m}_z &= \\overrightarrow{W}_X \\overrightarrow{m}_X + \\overrightarrow{W}_Y \\overrightarrow{m}_Y\n",
    "\\end{align*}$$\n",
    "\n",
    "- It follows that **message passing through an equality node is similar to applying Bayes rule**, i.e., fusion of two information sources. Does this make sense?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "### <span style=\"color:red\">(OPTIONAL SLIDE)</span> Example: SP Messages for the Addition and Multiplication Nodes\n",
    "\n",
    "- Next, consider an **addition node** $f_+(x,y,z) = \\delta(z-x-y)$:  \n",
    "<img src=\"./figures/ffg-addition-node.png\" width=\"200px\">\n",
    "$$\\begin{align*}\n",
    "\\overrightarrow{\\mu}_{Z}(z) &= \\int  \\overrightarrow{\\mu}_{X}(x) \\overrightarrow{\\mu}_{Y}(y) \\,\\delta(z-x-y) \\,\\mathrm{d}x \\mathrm{d}y \\\\\n",
    "   &=  \\int  \\overrightarrow{\\mu}_{X}(z) \\overrightarrow{\\mu}_{Y}(z-x) \\,\\mathrm{d}x \\,, \n",
    "\\end{align*}$$\n",
    "i.e., $\\overrightarrow{\\mu}_{Z}$ is the convolution of the messages $\\overrightarrow{\\mu}_{X}$ and $\\overrightarrow{\\mu}_{Y}$.\n",
    "\n",
    "- Of course, for Gaussian messages, these update rules evaluate to\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\overrightarrow{m}_Z &= \\overrightarrow{m}_X + \\overrightarrow{m}_Y \\\\ \n",
    "\\overrightarrow{V}_z &= \\overrightarrow{V}_X + \\overrightarrow{V}_Y \\,.\n",
    "\\end{align*}$$\n",
    "\n",
    "- <div class=\"exercise\"><b>Exercise</b>: For the same summation node, work out the SP update rule for the <i>backward</i> message $\\overleftarrow{\\mu}_{X}(x)$ as a function of $\\overrightarrow{\\mu}_{Y}(y)$ and  $\\overleftarrow{\\mu}_{Z}(z)$? And further refine the answer for Gaussian messages. </div>   \n",
    "\n",
    "\n",
    "- Next, let us consider a **multiplication** by a fixed (invertable matrix) gain $f_A(x,y) = \\delta(y-Ax)$\n",
    "\n",
    "<img src=\"./figures/ffg-gain-node.png\" width=\"200px\">\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\overrightarrow{\\mu}_{Y}(y) &= \\int  \\overrightarrow{\\mu}_{X}(x) \\,\\delta(y-Ax) \\,\\mathrm{d}x \\\\\n",
    "   &= \\overrightarrow{\\mu}_{X}(A^{-1}y) \\,.\n",
    "\\end{align*}$$\n",
    "\n",
    "- For a Gaussian message input message $\\overrightarrow{\\mu}_{X}(x) = \\mathcal{N}(\\overrightarrow{m}_{X},\\overrightarrow{V}_{X})$, the output message is also Gaussian with \n",
    "$$\\begin{align*}\n",
    "\\overrightarrow{m}_{Y} &= A\\overrightarrow{m}_{X} \\\\\n",
    "\\overrightarrow{V}_{Y} &= A\\overrightarrow{V}_{X}A^T\n",
    "\\end{align*}$$\n",
    "since \n",
    "$$\\begin{align*}\n",
    "\\overrightarrow{\\mu}_{Y}(y) &= \\overrightarrow{\\mu}_{X}(A^{-1}y) \\\\\n",
    "  &\\propto \\exp \\left( -\\frac{1}{2} \\left( A^{-1}y - \\overrightarrow{m}_{X}\\right)^T \\overrightarrow{V}_{X}^{-1} \\left(  A^{-1}y - \\overrightarrow{m}_{X}\\right)\\right) \\\\\n",
    "   &= \\exp \\left( -\\frac{1}{2} \\left( y - A\\overrightarrow{m}_{X}\\right)^T A^{-T}\\overrightarrow{V}_{X}^{-1} A \\left( y - A\\overrightarrow{m}_{X}\\right)\\right) \\\\\n",
    "  &\\propto  \\mathcal{N}(A\\overrightarrow{m}_{X},A\\overrightarrow{V}_{X}A^T) \\,.\n",
    "\\end{align*}$$\n",
    "\n",
    "- <div class=\"exercise\"><b>Excercise</b>: Proof that, for the same factor $\\delta(y-Ax)$ and Gaussian messages, the (backward) sum-product message $\\overleftarrow{\\mu}_{X}$ is given by \n",
    "$$\\begin{align*}\n",
    "\\overleftarrow{\\xi}_{X} &= A^T\\overleftarrow{\\xi}_{Y} \\\\\n",
    "\\overleftarrow{W}_{X} &= A^T\\overleftarrow{W}_{Y}A\n",
    "\\end{align*}$$\n",
    "where $\\overleftarrow{\\xi}_X \\triangleq \\overleftarrow{W}_X \\overleftarrow{m}_X$ and $\\overleftarrow{W}_{X} \\triangleq \\overleftarrow{V}_{X}^{-1}$ (and similarly for $Y$).</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### CODE EXAMPLE\n",
    "\n",
    "Let's calculate the Gaussian forward and backward messages for the addition node in ForneyLab. <img src=\"./figures/ffg-addition-node.png\" width=\"200px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: FactorGraph not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: FactorGraph not defined",
      ""
     ]
    }
   ],
   "source": [
    "# Build factor graph\n",
    "g = FactorGraph()\n",
    "t_X = TerminalNode(Gaussian(m=1.0, V=1.0))\n",
    "t_Y = TerminalNode(Gaussian(m=2.0, V=1.0))\n",
    "t_Z = TerminalNode(Gaussian(m=4.0, V=2.0))\n",
    "add_node = AdditionNode()\n",
    "Edge(t_X, add_node.i[:in1])\n",
    "Edge(t_Y, add_node.i[:in2])\n",
    "Edge(add_node.i[:out], t_Z)\n",
    "\n",
    "msg_forward_Z = run(SumProduct(add_node.i[:out]))\n",
    "print(\"Forward message on Z: $(msg_forward_Z)\")\n",
    "\n",
    "msg_backward_X = run(SumProduct(add_node.i[:in1]))\n",
    "print(\"Backward message on X: $(msg_backward_X)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the same way we can also investigate the forward and backward messages for the gain node <img src=\"./figures/ffg-gain-node.png\" width=\"200px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: FactorGraph not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: FactorGraph not defined",
      ""
     ]
    }
   ],
   "source": [
    "# Build factor graph\n",
    "g = FactorGraph()\n",
    "t_X = TerminalNode(Gaussian(m=1.0, V=1.0))\n",
    "t_Y = TerminalNode(Gaussian(m=2.0, V=1.0))\n",
    "gain_node = GainNode(gain=4.0)\n",
    "Edge(t_X, gain_node.i[:in])\n",
    "Edge(gain_node.i[:out], t_Y)\n",
    "\n",
    "msg_forward_Y = run(SumProduct(gain_node.i[:out]))\n",
    "print(\"Forward message on Y: $(msg_forward_Y)\")\n",
    "\n",
    "msg_backward_X = run(SumProduct(gain_node.i[:in]))\n",
    "# Make sure the message has mean and variance parameters instead of precision\n",
    "ensureParameters!(msg_backward_X.payload, (:m,:V)) \n",
    "print(\"Backward message on X: $(msg_backward_X)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- <div class=\"exercise\"><b>Excercise</b>: Confirm for both examples the results of the sum-product rule by manual computation.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### END OF CODE EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### <span class=\"exercise\">Example: Bayesian Linear Regression</span>\n",
    "\n",
    "- Recall: the goal of regression is to estimate an unknown function from a set of (noisy) function values.\n",
    "\n",
    "- Assume we want to estimate some function $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}$ from data set $D = \\{(x_1,y_1), \\ldots, (x_N,y_N)\\}$, where $y_i = f(x_i) + \\epsilon_i$.\n",
    "\n",
    "- We will assume a linear model with white Gaussian noise adn a Gaussian prior on the coefficients $w$:\n",
    "$$\\begin{align*}\n",
    "  y_i &= w^T x_i  + \\epsilon_i \\\\\n",
    "  \\epsilon_i &\\sim \\mathcal{N}(0, \\sigma^2) \\\\ \n",
    "  w &\\sim \\mathcal{N}(0,\\Sigma)\n",
    "\\end{align*}$$\n",
    "or equivalently\n",
    "$$\\begin{align*}\n",
    "p(D,w) &= \\overbrace{p(w)}^{\\text{weight prior}} \\prod_{i=1}^N  \\overbrace{p(y_i\\,|\\,x_i,w,\\epsilon_i)}^{\\text{regression model}} \\overbrace{p(\\epsilon_i)}^{\\text{noise model}} \\\\\n",
    "  &= \\mathcal{N}(w\\,|\\,0,\\Sigma) \\prod_{i=1}^N \\delta(y_i - w^T x_i - \\epsilon_i) \\mathcal{N}(\\epsilon_i\\,|\\,0,\\sigma^2) \n",
    "\\end{align*}$$\n",
    "\n",
    "- We are interested in inferring the posterior $p(w|D)$\n",
    "\n",
    "- Here's the factor graph for this model\n",
    "<img src=\"./figures/ffg-bayesian-linear-regression.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### CODE EXAMPLE\n",
    "\n",
    "Let's build the factor graph in Julia (with the FFG toolbox ForneyLab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: Module ForneyLab not found in current path.\nRun `Pkg.add(\"ForneyLab\")` to install the ForneyLab package.",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Module ForneyLab not found in current path.\nRun `Pkg.add(\"ForneyLab\")` to install the ForneyLab package.",
      "",
      " in require(::Symbol) at .\\loading.jl:365"
     ]
    }
   ],
   "source": [
    "using ForneyLab, PyPlot\n",
    "include(\"script/innerproduct_node.jl\");\n",
    "\n",
    "# Parameters\n",
    "Σ = 1e5 * eye(3) # Covariance matrix of prior on β\n",
    "σ2 = 2.0         # Noise variance\n",
    "\n",
    "fg = FactorGraph()\n",
    "beta_prior = TerminalNode(MvGaussian(m=zeros(3), V=Σ), id=:beta_prior)\n",
    "beta_terminator = TerminalNode(vague(MvGaussian{3}), id=:beta_terminator)\n",
    "equ = EqualityNode()\n",
    "x = TerminalNode(MvDelta(zeros(3)), id=:x)\n",
    "dot = DotProductNode()\n",
    "noise = GaussianNode(V=σ2, id=:noise)\n",
    "y = TerminalNode(Delta(0.0), id=:y)\n",
    "\n",
    "Edge(beta_prior, equ)\n",
    "beta_edge = Edge(equ, beta_terminator)\n",
    "Edge(equ, dot.i[:β])\n",
    "Edge(x, dot.i[:in])\n",
    "Edge(dot.i[:out], noise.i[:mean])\n",
    "Edge(noise.i[:out], y)\n",
    "Wrap(beta_terminator, beta_prior);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Generate data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: σ2 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: σ2 not defined",
      ""
     ]
    }
   ],
   "source": [
    "β = [1.0; 2.0; 0.25]\n",
    "N = 30\n",
    "z = 10.0*rand(N)\n",
    "x_train = [[1.0; z; z^2] for z in z] # Feature vector x = [1.0; z; z^2]\n",
    "f(x) = (β'*x)[1]\n",
    "y_train = map(f, x_train) + sqrt(σ2)*randn(N) # y[i] = β' * x[i] + ϵ\n",
    "scatter(z, y_train); xlabel(L\"z\"); ylabel(L\"f([1.0, z, z^2]) + \\epsilon\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Perform sum-product message passing and plot result (mean of posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: SumProduct not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: SumProduct not defined",
      ""
     ]
    }
   ],
   "source": [
    "algo = SumProduct(beta_edge)\n",
    "attachReadBuffer(x, x_train)\n",
    "attachReadBuffer(y, y_train)\n",
    "run(algo)\n",
    "beta_posterior_dist = calculateMarginal(beta_edge)\n",
    "println(\"Posterior mean of β: $(mean(beta_posterior_dist))\")\n",
    "println(\"Posterior covariance of β: $(cov(beta_posterior_dist))\")\n",
    "\n",
    "scatter(z, y_train); xlabel(L\"z\"); ylabel(L\"f([1.0, z, z^2]) + \\epsilon\");\n",
    "f_est(x) = (mean(beta_posterior_dist)'*x)[1]\n",
    "z_test = collect(0:0.2:12)\n",
    "x_test = [[1.0; z; z^2] for z in z_test]\n",
    "plot(z_test, map(f_est, x_test), \"k-\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END OF CODE EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### <span class=\"exercise\">Exercise </span>\n",
    "\n",
    "<div class=\"exercise\">Reflect on the fact that we now have methods for both marginalization and processing observations in FFGs. In principle, we are sufficiently equipped to do inference in probabilistic models through message passing. Draw the graph for $$p(x_1,x_2,x_3)=f_a(x_1)\\cdot f_b(x_1,x_2)\\cdot f_c(x_2,x_3)$$ and show which boxes need to be closed for computing $p(x_1|x_2)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### <span class=\"exercise\">Exercise</span>\n",
    "\n",
    "<div class=\"exercise\"> \n",
    "Consider a variable $X$ with measurements $D=\\{x_1,x_2\\}$. We assume the following model for $X$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(D,\\theta) &= p(\\theta)\\cdot \\prod_{n=1}^2 p(x_n|\\theta)  \\\\\n",
    "p(\\theta) &= \\mathcal{N}(\\theta \\mid 0,1) \\\\\n",
    "p(x_n \\mid\\theta) &= \\mathcal{N}(x_n \\mid \\theta,1)\n",
    "\\end{align*}$$\n",
    "\n",
    "Draw the factor graph and infer $\\theta$ through the Sum-Product Algorithm. \n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Inference in Linear Gaussian Models by Sum-Product Message Passing\n",
    "\n",
    "- The foregoing message update rules can be extended to all scenarios involving additions, fixed-gain multiplications and branching (equality nodes), thus creating a completely **automatable inference framework** for factorized linear Gaussian models.\n",
    "\n",
    "- The update rules for elementary and important node types can be put in a Table (see **Tables 1 through 6** in [Loeliger, 2007](./files/Loeliger-2007-The-factor-graph-approach-to-model-based-signal-processing.pdf)).\n",
    "\n",
    "- If the update rules for all node types in a graph have been tabulated, then inference by message passing comes down to a set of table-lookup operations. This also works for large graphs (where 'manual' inference becomes intractable).\n",
    "\n",
    "- If the graph contains no cycles, the Sum-Product Algorithm computes **exact** marginals for all hidden variables.\n",
    "\n",
    "- If the graph contains cycles, we have in principle an infinite tree without terminals. In this case, the SP Algorithm is not guaranteed to find exact marginals. In practice, if we apply the SP algorithm for just a few iterations we often find satisfying approximate marginals.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The cell below loads the style file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "SystemError: opening file ../../styles/aipstyle.html: No such file or directory",
     "output_type": "error",
     "traceback": [
      "SystemError: opening file ../../styles/aipstyle.html: No such file or directory",
      "",
      " in #systemerror#51 at .\\error.jl:34 [inlined]",
      " in systemerror(::String, ::Bool) at .\\error.jl:34",
      " in open(::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at .\\iostream.jl:89",
      " in open(::##7#8, ::String) at .\\iostream.jl:111"
     ]
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.html\") do f display(\"text/html\", readstring(f)) end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
