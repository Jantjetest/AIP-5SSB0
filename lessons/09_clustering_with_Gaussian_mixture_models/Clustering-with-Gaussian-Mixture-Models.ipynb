{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Limitations of Simple IID Gaussian Models\n",
    "\n",
    "Sofar, model inference was solved analytically, but we\n",
    "used strong assumptions\n",
    "- IID sampling, $p(D) = \\prod_n p(x_n)$\n",
    "- Simple Gaussian (or multinomial) PDFs, $p(x_n) \\sim \\mathcal{N}(x_n|\\mu,\\Sigma)$\n",
    "- Some limitations of Simple Gaussian Models with IID Sampling\n",
    "  1. What if the PDF is **multi-modal** (or is just not Gaussian in any other way)?\n",
    "  2. Covariance matrix $\\sigma$ has $D(D+1)/2$ parameters.\n",
    "    - This quickly becomes **a very large number** for increasing dimension $D$.\n",
    "  3. Temporal signals are often **not IID**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Towards More Flexible Models\n",
    "\n",
    "-  What if the PDF is multi-modal (or is just not Gaussian in any other way)?\n",
    "  -   **Discrete latent** variable models (a.k.a. **mixture** models).\n",
    "    \n",
    "-  Covariance matrix $\\Sigma$ has $D(D+1)/2$ parameters. This quickly becomes very large for increasing dimension $D$.\n",
    "  -  **Continuous latent** variable models (a.k.a. **dimensionality reduction** models).\n",
    "    \n",
    "-  Temporal signals are often not IID.\n",
    "  -  Introduce **Markov dependencies** and **latent state** variable models.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  What if the Data are Not like This ...\n",
    "\\begin{center}\\includegraphics[height=8cm]{./figures/fig-2-class-data}\\end{center}\n",
    "\n",
    "\n",
    "###  ... but like This\n",
    "\\begin{center}\\includegraphics[height=8cm]{./figures/fig-unlabeled-data}\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Unobserved Classes\n",
    "\n",
    "Consider again a set of observed data $D=\\{x_1,\\dotsc,x_N\\}$\n",
    "\n",
    "- This time we suspect that there are unobserved class labels that would help explain (or predict) the data, e.g.,\n",
    "  - the observed data are the color of living things; the unobserved classes are animals and plants.\n",
    "  - observed are wheel sizes; unobserved categories are trucks and personal cars.\n",
    "  - observed is an audio signal; unobserved classes include speech, music, traffic noise, etc.\n",
    "    \n",
    "Classification problems with unobserved classes are called **Clustering** problems. The learning algorithm needs to **discover the classes from the observed data**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Latent Variable Model Specification\n",
    " \n",
    "If the categories were observed as well, these data could be nicely modeled by the previously discussed generative classification framework.\n",
    "\n",
    "-  Introduce the 1-of-$K$ variable $z = (z_1,\\ldots,z_K)^T$ to represent the unobserved classes.\n",
    "  - NB: our notation is: $Y_k$ for observed targets; $Z_k$ for unobserved outputs.\n",
    "-  Use completely **equivalent model assumptions to linear generative classification**, (except now the class\n",
    "    labels $z_k$ are not observed),\n",
    "    \n",
    "\\begin{align}\n",
    "p(x_n) &= \\sum_{k=1}^K p(z_{nk}) \\, p(x_n|z_{nk})  \\\\\n",
    "\t&= \\sum_k \\pi_k \\mathcal{N}\\left(x_n|\\mu_k,\\Sigma_k \\right)\n",
    "\\end{align}\n",
    "\n",
    "This model is called a **Gaussian Mixture Model**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Gaussian Mixture Models\n",
    "GMMs are **universal approximators of densities** (as long as there are enough Gaussians of course)\n",
    "\n",
    "\\begin{figure}\n",
    "\\begin{center}\n",
    "\\includegraphics[width=10.5cm]{./figures/fig-ZoubinG-GMM-universal-approximation}\n",
    "\\end{center}\n",
    "The red curves show the (weighted) Gaussians; the blue curve the resulting density.\n",
    "\\end{figure}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Inference: Log-Likelihood for GMM\n",
    "The log-likelihood for observed data $D=\\{x_1,\\dotsc,x_N\\}$,\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(D|\\theta) &\\stackrel{\\text{IID}}{=} \\sum_n \\log p(x_n|\\theta)\\\\\n",
    "  &= \\sum_n \\log \\sum_{z_n} p(x_n,z_{n}|\\theta)\\\\\n",
    "  &= \\sum_n \\log \\sum_{z_n} p(z_{n}|\\theta) p(x_n|z_n,\\theta) \\\\\n",
    "  &= \\sum_n \\log \\sum_k p(z_{nk}=1|\\theta)p(x_n|z_{nk}=1,\\theta) \\\\\n",
    "  &= \\boxed{\\sum_n \\log \\sum_k \\pi_k\\mathcal{N}(x_n|\\mu_k,\\Sigma_k)}\n",
    "\\end{align}\n",
    "\n",
    "... and now the log-of-sum cannot be further simplified.\n",
    "\n",
    "Compare to classification: $$\\sum_k N_k \\log \\pi_k + \\sum_{n,k} y_{nk} \\log \\mathcal{N}(x_n|\\mu_k,\\Sigma)$$\n",
    "\n",
    "- Fortunately GMMs can be trained by maximum likelihood using an efficient algorithm: Expectation-Maximization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Posterior Responsibility is a Soft Class Indicator\n",
    "\n",
    "Consider the (posterior) expectation for the (hidden) class labels\n",
    "\n",
    "\\begin{align}\n",
    "\\gamma_{nk} &\\triangleq \\mathrm{exp}\\left(z_{nk}|x_n,\\theta\\right) = 0\\times p(z_{nk}=0|x_n,\\theta) + 1\\times p(z_{nk}=1|x_n,\\theta) \\\\\n",
    "  &= p(z_{nk}=1|x_n,\\theta) = \\frac{p(x_n|z_{nk}=1)p(z_{nk}=1)}{\\sum_j p(x_n|z_{nj}=1)p(z_{nj}=1)} \\\\\n",
    "  &= \\frac{\\pi_k \\mathcal{N}(x_n|\\mu_k,\\Sigma_k)}{\\sum_j \\pi_j \\mathcal{N}(x_n|\\mu_j,\\Sigma_j)}\n",
    "\\end{align}\n",
    "            \n",
    "-  Note that $0 \\leq \\gamma_{nk} \\leq 1$ and is available (i.e., can be evaluated).\n",
    "-  $\\gamma_{nk}$ are (soft) **reponsibilities**.\n",
    "-  PLAN: Let's use the reponsibilities $\\gamma_{nk}$ (rather than the binary class indicators $y_{nk}$) and apply the classification formulas.\n",
    "\n",
    "%\\begin{center}\\includegraphics[height=4cm]{./Figure95.jpg}\\end{center}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ML estimation for Clustering\n",
    "\n",
    "-  Try parameter updates (like conditional Gaussian classification):\n",
    "\n",
    "$$\n",
    "\\hat \\pi_k = \\frac{N_k}{N};\\; \\hat \\mu_k = \\frac{1}{N_k} \\sum_n \\gamma_{nk} x_n; \\;  \\hat \\Sigma_k  = \\frac{1}{N_k} \\sum_{n} \\gamma_{nk} (x_n-\\hat \\mu_k)(x_n-\\hat \\mu_k)^T$$\n",
    "where $N_k = \\sum_n \\gamma_{nk}$ .\n",
    "\n",
    "-  But wait, the responsibilities $\\gamma_{nk}=\\frac{\\pi_k \\mathcal{N}(x_n|\\mu_k,\\Sigma_k)}{\\sum_j \\pi_j \\mathcal{N}(x_n|\\mu_j,\\Sigma_j)}$ are a function of the model parameters $\\{\\pi,\\mu,\\Sigma\\}$ and the parameter updates depend on the responsibilities ...\n",
    "-  **Solution(?)**: iterate between updating the responsibilities $\\gamma_{nk}$ and the model parameters $\\{\\pi,\\mu,\\Sigma\\}$.\n",
    "\n",
    "-  This iteration works (!) and is called the **Expectation-Maximization (EM)** algorithm.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering vs. Classification\n",
    "\n",
    "<table>\n",
    "<tr> <td></td><td>**Classification**</td> <td>**Clustering**</td> </tr> \n",
    "\n",
    "<tr> <td>1</td><td>Class label $y_n$ is observed</td> <td>Class label $z_n$ is latent</td> </tr>\n",
    "\n",
    "<tr> <td>2</td><td>log-likelihood **conditions** on observed class<br />$\\propto \\sum_{nk} y_{nk} \\log \\mathcal{N}(x_n|\\mu_k,\\Sigma_k)$</td> <td> log-likelihood **marginalizes** over latent classes<br />$\\propto \\sum_{n}\\log \\sum_k \\pi_k \\mathcal{N}(x_n|\\mu_k,\\Sigma_k)$</td> </tr>\n",
    "\n",
    "<tr> <td>3</td><td>'Hard' class selector<br />$y_{nk} = \\mathrm{logical}(y_n \\in \\mathcal{C}_k)$</td> <td>'Soft' class responsibility<br />$\\gamma_{nk} = p(z_{nk}=1|x_n,\\theta)$</td> </tr>\n",
    "\n",
    "<tr> <td>4</td>\n",
    "<td>Estimation:<BR /> \n",
    "\\begin{align}\n",
    "\\hat{\\pi}_k &= \\frac{1}{N}\\sum_n y_{nk} \\\\\n",
    "\\hat{\\mu}_k &= \\frac{\\sum_n y_{nk} x_n}{\\sum_n y_{nk}} \\\\\n",
    "\\hat{\\Sigma}_k &= \\frac{\\sum_n y_{nk} (x_n-\\hat\\mu_k)(x_n-\\hat\\mu_k)^T}{\\sum_n y_{nk}}\n",
    "\\end{align}\n",
    "</td> \n",
    "<td>Estimation (1 update step!)<BR />\n",
    "\\begin{align}\n",
    "\\hat{\\pi}_k &= \\frac{1}{N}\\sum_n \\gamma_{nk} \\\\\n",
    "\\hat{\\mu}_k &= \\frac{\\sum_n \\gamma_{nk} x_n}{\\sum_n \\gamma_{nk}} \\\\\n",
    "\\hat{\\Sigma}_k &= \\frac{\\sum_n \\gamma_{nk} (x_n-\\hat\\mu_k)(x_n-\\hat\\mu_k)^T}{\\sum_n \\gamma_{nk}}\n",
    "\\end{align}\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.6",
   "language": "julia",
   "name": "julia 0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
