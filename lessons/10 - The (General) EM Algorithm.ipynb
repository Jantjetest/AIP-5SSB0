{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The General EM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ignore this cell (pre-amble)\n",
    "$\\newcommand{\\llh}{{\\mathrm{L}}}$\n",
    "$\\newcommand{\\FE}{{\\mathrm{F}}}$\n",
    "$\\newcommand{\\KL}{{\\mathrm{D}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Intuition EM Algorithm\n",
    "\n",
    "-  Let's denote all observed data by $N\\times D$ matrix $\\mathbf{X}=\\left( x_1,\\ldots,x_n \\right)^T$ and the set of all latent variables by $N\\times K$ matrix $\\mathbf{Z}=\\left( z_1,\\ldots,z_n \\right)^T$.\n",
    "-  The ML optimization problem with observed data $\\mathbf{X}$ is,\n",
    "$$\\hat \\theta = \\arg\\max_\\theta \\log p(\\mathbf{X}|\\theta)$$\n",
    "but this appears to be a very difficult optimization problem.\n",
    "-  **Plan**: Introduce (extra) latent variables $\\mathbf{Z}$ such that the **complete-data log-likelihood** $\\log p(\\mathbf{X},\\mathbf{Z}|\\theta)$ is easily maximized.\n",
    "-  Alas, $\\mathbf{Z}$ are latent variables, i.e. NOT observed, and we cannot evaluate $p(\\mathbf{X},\\mathbf{Z}|\\theta)$ as a function of $\\theta$ only.\n",
    "-  **idea 1**: optimize instead the **expected complete-data likelihood**\n",
    "$$ \\sum_{z} p(\\mathbf{Z}|\\mathbf{X},\\theta) \\,\\log p(\\mathbf{X},\\mathbf{Z}|\\theta)$$\n",
    "which is no longer a function of (the unobserved) $\\mathbf{Z}$.\n",
    "-  Note that the posterior $p(\\mathbf{Z}|\\mathbf{X},\\theta)$ depends on $\\theta$.\n",
    "-  **idea 2**: Use **iterative optimization** so that we can use the estimate $\\hat \\theta$ from the previous optimization step in order to compute the posteriors $p(\\mathbf{Z}|\\mathbf{X},\\hat \\theta)$.\n",
    " -  These ideas lead to the iterative **EM Algorithm**,\n",
    "\n",
    "\\begin{align}\n",
    "    Q(\\theta,\\hat{\\theta}) &= \\sum_z p(\\mathbf{Z}|\\mathbf{X},\\hat{\\theta})\\,\\log p(\\mathbf{X},\\mathbf{Z}|\\theta) \\tag{E-step}\\\\\n",
    "    \\hat \\theta_{\\text{(new)}} &= \\arg\\max_{\\theta} Q(\\theta,\\hat{\\theta})\n",
    "\\tag{M-step} \\end{align}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-  (In the next few slides), we will show that the EM algorithm is guaranteed to converge to a local maximum, or saddle-point, of the **observed-data} likelihood function $ p(\\mathbf{X}|\\theta)$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Concavity and Jensen's Inequality\n",
    "\n",
    "- $f(x)$ is **concave$^\\frown$** over $(a,b)$ if, for all $x_1,x_2 \\in (a,b)$ and $0 \\leq \\lambda \\leq 1$,\n",
    "$$\n",
    "f\\left( \\lambda x_1 + \\left( 1-\\lambda \\right) x_2\\right) \\geq \n",
    "\\lambda f\\left( x_1\\right) + \\left(1-\\lambda \\right) f\\left( x_2\\right)\n",
    "$$\n",
    "- **Jensen's  Inequality**: If $f$ is concave$^\\frown$ and $x$ is a RV then:\n",
    "$$\n",
    "f\\left( {\\mathrm{exp} \\left[ x \\right]} \\right)  \\geq \\mathrm{exp} \\left[ {f(x)} \\right]\n",
    "$$\n",
    "- Example $$\\log\\left( \\mathrm{exp}[ x ] \\right)  \\geq \\mathrm{exp} \\left[ {\\log(x)} \\right]$$\n",
    "\n",
    "\n",
    "\\includegraphics[height=2.5cm]{./figures/fig-MacKay-centre-of-gravity\n",
    "\n",
    "- (Physical interpretation)}. Put masses $p_i$ at locations $(x_i,f(x_i))$. Then, **center of gravity** at $(\\mathrm{exp}[x],\\mathrm{exp}[f(x)])$ lies under the curve at location $\\left( \\mathrm{exp}[x],f(\\mathrm{exp}[x]) \\right)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The EM 'Trick'\n",
    "\n",
    "-  The log-likelihood based on data set ${\\mathbf{X}}$ is\n",
    "$$\n",
    "\\llh(\\theta) = \\log p(\\mathbf{X}|\\theta) = \\log \\sum_\\mathbf{Z} p(\\mathbf{X},\\mathbf{Z}|\\theta)\n",
    "$$\n",
    "\n",
    "-  For **any** distribution $q(\\mathbf{Z})$, we can write\n",
    "\\begin{align}\n",
    "\\llh(\\theta) &= \\log \\sum_\\mathbf{Z} q(\\mathbf{Z}) \\frac{p(\\mathbf{X},\\mathbf{Z}|\\theta)}{q(\\mathbf{Z})} \\\\\n",
    "  &\\stackrel{Jensen}{\\geq} \\sum_\\mathbf{Z} q(\\mathbf{Z}) \\log \\frac{p(\\mathbf{X},\\mathbf{Z}|\\theta)}{q(\\mathbf{Z})} \\\\\n",
    "  &\\equiv \\FE(\\theta,q) \\qquad \\text{(`free energy')}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "-  Furthermore, if we choose $q(\\mathbf{Z})=p(\\mathbf{Z}|\\mathbf{X},\\theta)$, then $\\FE(\\theta,q)=\\llh(\\theta)$ (is maximal), since\n",
    "\n",
    "\\begin{align}\n",
    "\\FE(\\theta,q)\\rvert_{ q=p(\\mathbf{Z}|\\mathbf{X},\\theta) } \n",
    "  &= \\sum_\\mathbf{Z} p(\\mathbf{Z}|\\mathbf{X},\\theta) \\log  \\frac{ p(\\mathbf{X},\\mathbf{Z}|\\theta) }{ p(\\mathbf{Z}|\\mathbf{X},\\theta) } \\\\\n",
    "    &= \\sum_\\mathbf{Z} p(\\mathbf{Z}|\\mathbf{X},\\theta) \\log \\frac{ p(\\mathbf{Z}|\\mathbf{X},\\theta) p(\\mathbf{X}|\\theta) }{ p(\\mathbf{Z}|\\mathbf{X},\\theta) } \\\\\n",
    "    &= \\sum_\\mathbf{Z} p(\\mathbf{Z}|\\mathbf{X},\\theta) \\log p(\\mathbf{X}|\\theta)  \\\\\n",
    "    &= \\log p(\\mathbf{X}|\\theta) \\sum_\\mathbf{Z} p(\\mathbf{Z}|\\mathbf{X},\\theta) \\\\\n",
    "    &= \\log p(\\mathbf{X}|\\theta) = \\llh(\\theta)\n",
    "\\end{align}\n",
    "\n",
    "$\\Rightarrow$ We have just shown that the following procedure (coordinate ascent on $\\FE$) will increase the log-likelihood:\n",
    "\\begin{align}\n",
    "q^{\\text{new}} &:= p(\\mathbf{Z}|\\mathbf{X},\\theta^{\\text{old}}) = \\arg\\max_q \\FE(\\theta^{\\text{old}},q) \\tag{E-step}\\\\\n",
    "\\theta^{\\text{new}} &:= \\arg\\max_{\\theta} \\FE(\\theta,q^{\\text{new}}) \\tag{M-step}\n",
    "\\end{align}\n",
    "\n",
    "- Usually optimizing $\\log p(x,z|\\theta)$ with both $x$ and $z$ observed is straightforward. (e.g. class-conditional Gaussian classification, linear regression)\n",
    "\n",
    "- Note: we also use notation: $\\langle \\log p(x,z|\\theta) \\rangle_q=\\mathrm{exp}_q[\\log p(x,z|\\theta)]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  EM involves optimizing lower bound\n",
    "\\begin{figure}\n",
    "         \\centering\n",
    "        \\includegraphics[height=8cm]{./figures/Bishop-Figure914}\n",
    "    \\end{figure}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  EM for GMM revisited\n",
    "\n",
    "- (total) log-likelihood $\\llh(\\theta) = \\sum_n \\log \\sum_k \\pi_k \\mathcal{N}(x_n|\\mu_k,\\sigma_k)$\n",
    "\n",
    "- **E-step**: compute responsibilities for latent classes\n",
    "$$\n",
    "\\gamma_{nk} = p(z_{nk}=1|x_n,\\hat{\\theta}) = \\frac{ \\hat{\\pi}_k \\mathcal{N}(x_n|\\hat{\\theta}_k) }{ \\sum_j \\hat{\\pi}_j \\mathcal{N}(x_n|\\hat{\\theta}_j) }\n",
    "$$\n",
    "\n",
    "- **M-step**: Maximize expected complete data log-likelihood\n",
    "\\begin{align}\n",
    "\\mathrm{E}_\\mathbf{Z}[ \\log p(\\mathbf{X},\\mathbf{Z}|\\mu,\\sigma,\\pi) ] &= \\sum_n \\sum_k \\gamma_{nk} \\log p(x_n,z_{nk}=1|\\theta) \\\\\n",
    "    &= \\sum_{nk} \\gamma_{nk} \\log \\mathcal{N}(x_n|\\mu_k,\\sigma_k) + \\sum_{nk} \\gamma_{nk} \\log \\pi_k\n",
    "\\end{align}\n",
    "\n",
    "We've maximized this before, see section on Density estimation (Gaussian, multinomial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  EM Example--Three Coins\n",
    "-  You have three coins in your pocket\n",
    "  -  Coin 0: $p(\\mathrm{Head}) = \\lambda$\n",
    "  -  Coin 1: $p(\\mathrm{Head}) = \\rho$\n",
    "  -  Coin 2: $p(\\mathrm{Head}) = \\theta$\n",
    "    \n",
    "-  (Scenario). Toss coin $0$. If Head comes up, toss three times with coin 1; otherwise, toss three times with coin 2.\n",
    "-  The observed sequences **after** each toss with coin 0 were $\\langle \\mathrm{HHH}\\rangle$, $\\langle \\mathrm{HTH}\\rangle$, $\\langle \\mathrm{HHT}\\rangle$, and $\\langle\\mathrm{HTT}\\rangle$\n",
    "\n",
    "-  [Q.] Estimate most likely values for $\\lambda$, $\\rho$ and $\\theta$\n",
    "-  [A.] homework. Use EM.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Some Properties of EM\n",
    "\n",
    "-  EM is a general procedure for learning in the presence of unobserved variables.\n",
    "-  In a sense, it is a **family of algorithms**. The update rules you will derive depend on the probability model assumed.\n",
    "-  (Good!) **No tuning parameters** such a learning rate, unlike gradient descent-type algorithms\n",
    "-  (Bad). EM is an iterative procedure that is very sensitive to initial\n",
    "conditions! EM converges to a **local optimum**.\n",
    "-  Start from trash $\\rightarrow$ end up with trash. Hence, we need a good and fast initialization procedure (often used: K-Means)\n",
    "-  Also used to train HMMs, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  (OPTIONAL) The Kullback-Leibler Divergence\n",
    "\n",
    "-  The **Kullback-Leibler Divergence** or **relative entropy** between two distributions $q$ and $p$ is defined as\n",
    "$$\n",
    "\\KL(q\\|p) \\equiv \\sum_z q(z) \\log \\frac{q(z)}{p(z)}\n",
    "$$\n",
    "\n",
    "-  In general $\\KL(q\\|p) \\neq \\KL(p\\|q)$\n",
    "\n",
    "-  Theorem: **Gibbs Inequality**:\n",
    "    \n",
    "$$\\boxed{ \\KL(q\\|p)\\geq 0 }$$\n",
    "with equality only iff $p=q$. \n",
    "\n",
    "-  Proof (use Jensen Inequality):\n",
    "Define $f(u)=-\\log u$ ($f$ is convex$^\\smile$) and let $u = p(x)/q(x)$.\n",
    "\\begin{align}\n",
    "\\KL(q\\|p) &= \\mathrm{E}_q [f(u)] \\stackrel{Jensen}{\\geq}  f\\left( \\mathrm{E}_q [u] \\right) \\\\\n",
    "  &= f\\left( \\sum_x q(x) \\frac{p(x)}{q(x)} \\right) \\\\\n",
    "     &= -\\log \\left(\\sum_x p(x) \\right) =0\n",
    "\\end{align}\n",
    "with equality only if $u$ is constant, i.e., $q(x)=p(x)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  (OPTIONAL) The Free Energy Functional\n",
    "\n",
    "-   For **any** distribution $q(\\mathbf{Z})$, the following holds:\n",
    "\n",
    "\\begin{align}\n",
    "\\llh(\\theta) &= \\log p(\\mathbf{X}|\\theta) = \\sum_\\mathbf{Z} q(\\mathbf{Z}) \\log p(\\mathbf{X}|\\theta) \\\\\n",
    "    &= \\sum_\\mathbf{Z} q(\\mathbf{Z}) \\left[ \\log p(\\mathbf{X}|\\theta) -\\log \\frac{ q(\\mathbf{Z}) }{ p(\\mathbf{Z}|\\mathbf{X}) } \\right] + \\sum_\\mathbf{Z} q(\\mathbf{Z}) \\log \\frac{ q(\\mathbf{Z}) }{ p(\\mathbf{Z}|\\mathbf{X}) } \\\\\n",
    "    &= \\sum_\\mathbf{Z} q(\\mathbf{Z}) \\log \\frac{ p(\\mathbf{X},\\mathbf{Z}|\\theta) }{ q(\\mathbf{Z}) } + \\KL\\left(q(\\mathbf{Z})\\|p(\\mathbf{Z}|\\mathbf{X}) \\right) \\\\\n",
    "    &\\stackrel{Gibbs}{\\geq} \\sum_\\mathbf{Z} q(\\mathbf{Z}) \\log \\frac{ p(\\mathbf{X},\\mathbf{Z}|\\theta) }{ q(\\mathbf{Z}) } \\\\\n",
    "    &= \\sum_\\mathbf{Z} q(\\mathbf{Z}) \\log p(\\mathbf{X}\\mathbf{Z}|\\theta) - \\sum_\\mathbf{Z} q(\\mathbf{Z}) \\log q(\\mathbf{Z}) \\\\\n",
    "    &= \\left\\langle\\log p(\\mathbf{X},\\mathbf{Z}|\\theta)\\right\\rangle_q + \\mathcal{H}(q)\\\\\n",
    "    &\\triangleq \\FE(\\theta,q) \\tag{free energy}\n",
    "\\end{align}\n",
    "with equality $\\llh(\\theta)=\\FE(\\theta,q)$ only if $q(\\mathbf{Z})=p(\\mathbf{Z}|\\mathbf{X})$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.8",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
