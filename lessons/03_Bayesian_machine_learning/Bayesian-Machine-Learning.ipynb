{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goals\n",
    "  - Introduction to Bayesian (i.e., probabilistic) modeling\n",
    "- Materials\n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional\n",
    "    - Bishop pp. 21-24   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Problem: Predicting a Coin Toss\n",
    "\n",
    "- **Question**. We observe a the following sequence of heads (h) and tails (t) when tossing the same coin repeatedly $$D=\\{hthhtth\\}\\,.$$\n",
    "\n",
    "- What is the probability that heads (h) comes up next?\n",
    "\n",
    "- **Answer** later in this lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bayesian Machine Learning Framework\n",
    "\n",
    "\n",
    "- Suppose that your task is to predict a 'new' datum $x$, based on $N$  observations $D=\\{x_1,\\dotsc,x_N\\}$.\n",
    "\n",
    "- The Bayesian approach for this task involves three stages: \n",
    "  1. Model specification\n",
    "  1. parameter estimation (inference, learning)\n",
    "  1. Prediction (apply the model)\n",
    "  \n",
    "Let's discuss these three stages in a bit more detail:\n",
    "\n",
    "1. **Model specification**. \n",
    "Your first task is to propose a model with tuning parameters $\\theta$ for generating the data $D$.\n",
    "  - This involves specification of $p(D|\\theta)$ and a prior for the parameters $p(\\theta)$.\n",
    "  - _You_ choose the distribution $p(D|\\theta)$ based on your physical understanding of the data generating process.\n",
    "    - Note that, for independent observations $x_n$,\n",
    "$$ p(D|\\theta) = \\prod_{n=1}^N p(x_n|\\theta)$$\n",
    "so usually you select a model for generating one observation $x_n$ and then use (in-)dependence assumptions to combine these models into a model for $D$. \n",
    "  - _You_ choose the prior $p(\\theta)$ to reflect what you know about the parameter values before you see the data $D$.\n",
    "2. **Parameter estimation**.\n",
    "After model specification, you need to measure/collect a data set $D$. Then, use Bayes rule to find the posterior distribution for the parameters,\n",
    "$$\n",
    "p(\\theta|D) = \\frac{p(D|\\theta) p(\\theta)}{p(D)} \\propto p(D|\\theta) p(\\theta)\n",
    "$$  \n",
    "  - Note that there's **no need for you to design a _smart_ parameter estimation algorithm**. The only complexity lies in the computational issues.  \n",
    "  - This \"recipe\" works only if the RHS factors can be evaluated; this is what machine learning is about     \n",
    "  $\\Rightarrow$ **Machine learning is easy, apart from computational details:)**\n",
    "3. **Prediction**. \n",
    "Given the data $D$, our knowledge about the yet unobserved datum $x$ is captured by\n",
    "$$\\begin{align*}\n",
    "p(x|D) &\\stackrel{s}{=} \\int p(x,\\theta|D) \\,\\mathrm{d}\\theta \\\\\n",
    " &\\stackrel{p}{=} \\int p(x|\\theta,D) p(\\theta|D) \\,\\mathrm{d}\\theta \\\\\n",
    " &\\stackrel{a}{=} \\int p(x|\\theta) p(\\theta|D) \\,\\mathrm{d}\\theta \\\\\n",
    "\\end{align*}$$\n",
    "  - (The symbols $s$, $p$ and $a$ relate to 'sum rule', 'product rule' and 'model assumption'. As discussed, each probabilistic manipulation will be based on the sum or product rule, or a model assumption that _you_ (the designer) makes.)  \n",
    "  - Again, **no need to invent a special prediction algorithm**. Probability theory takes care of all that. The complexity of prediction is just computational: how to carry out the marginalization over $\\theta$.\n",
    "  - In order to execute prediction, you need to have access to the factors $p(x|\\theta)$ and $p(\\theta|D)$. Where do these factors come from? Are they available?\n",
    "  - What did we learn from $D$? Without access to $D$, we would predict new observations through\n",
    "$$\n",
    "p(x) = \\int p(x,\\theta) \\,\\mathrm{d}\\theta = \\int p(x|\\theta) p(\\theta) \\,\\mathrm{d}\\theta\n",
    "$$\n",
    "  - NB The application of the learned posterior $p(\\theta|D)$ not necessarily has to be prediction. We use it here as an example, but other applications are of course also possible. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Model Comparison \n",
    "<h3 style=\"color:red\";>(OPTIONAL SLIDE)</h3> \n",
    "\n",
    "There appears to be a remaining problem: How good really were our model assumptions $p(x|\\theta)$ and $p(\\theta)$?  \n",
    "- Technically, this is a **model comparison** problem\n",
    "- [**Q**.] What if I have more candidate models, say $\\mathcal{M} = \\{m_1,\\ldots,m_K\\}$ where each model relates to specific prior $p(\\theta|m_k)$ and likelihood $p(D|\\theta,m_k)$? Can we evaluate the relative performance of a model against another model from the set?\n",
    "- [**A**.]: Start again with **model specification**. Specify a prior $p(m_k)$ for each of the models and then solve the desired inference problem:\n",
    "        \n",
    "$$\\begin{align*} \n",
    "p(m_k|D) &= \\frac{p(D|m_k) p(m_k)}{p(D)} \\\\\n",
    "  &\\propto p(m_k) \\cdot p(D|m_k) \\\\\n",
    "  &= p(m_k)\\cdot \\int_\\theta p(D,\\theta|m_k) \\,\\mathrm{d}\\theta\\\\\n",
    "  &= \\underbrace{p(m_k)}_{\\substack{\\text{model}\\\\\\text{prior}}}\\cdot \\int_\\theta \\underbrace{p(D|\\theta,m_k)}_{\\text{likelihood}} \\,\\underbrace{p(\\theta|m_k)}_{\\text{prior}}\\, \\mathrm{d}\\theta\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "- You, the engineer, have to choose the factors $p(D|\\theta,m_k)$, $p(\\theta|m_k)$ and $p(m_k)$. After that, for a given data set $D$, the model posterior $p(m_k|D)$ can be computed. \n",
    "\n",
    "- If you need to work with one model,select the model with largest posterior $p(m_k|D)$\n",
    "- Alternatively, if you don't want to choose a model, you can do prediction by **Bayesian model averaging** to utilitize the predictive power from all models:\n",
    "$$\\begin{align*}\n",
    "p(x|D) &= \\sum_k \\int p(x,\\theta,m_k|D)\\,\\mathrm{d}\\theta \\\\\n",
    "  &= \\sum_k \\underbrace{p(m_k|D)}_{\\substack{\\text{model}\\\\\\text{posterior}}} \\cdot \\int \\underbrace{p(\\theta|D)}_{\\substack{\\text{parameter}\\\\\\text{posterior}}} \\, \\underbrace{p(x|\\theta,m_k)}_{\\text{likelihood}} \\,\\mathrm{d}\\theta\n",
    "\\end{align*}$$ \n",
    "\n",
    "$\\Rightarrow$ In a Bayesian framework, **model comparison** follows the same recipe as parameter estimation; it just works at one higher hierarchical level.\n",
    "\n",
    "- More on this in part 2 (Tjalkens).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning and the Scientific Method Revisited\n",
    "\n",
    "- Bayesian probability theory provides a unified framework for information processing (and even the Scientific Method).\n",
    "\n",
    "<img src=\"./figures/ml-and-scientific-loop-2.png\" width=\"450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Solve the Example Problem: Predicting a Coin Toss\n",
    "\n",
    "- We observe a the following sequence of heads (h) and tails (t) when tossing the same coin repeatedly $$D=\\{hthhtth\\}\\,.$$\n",
    "\n",
    "- What is the probability that heads (h) comes up next? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Coin toss example (1): Model Specification\n",
    "\n",
    "We'll work on the general problem of observing a sequence of $N$ coin tosses $D=\\{x_1,\\ldots,x_N\\}$ with $n$ heads. \n",
    "\n",
    "##### Likelihood\n",
    "\n",
    "- Assume a Bernoulli distributed variable $p(x_n=h|\\mu)=\\mu$, which leads to a **binomial** distribution for the likelihood:\n",
    "$$   \n",
    "p(D|\\mu) = \\prod_{n=1}^N p(x_n|\\mu) = \\mu^n (1-\\mu)^{N-n}\n",
    "$$\n",
    "\n",
    "##### Prior\n",
    "\n",
    "-  Assume the prior belief is governed by a **beta distribution**\n",
    "$$\n",
    "p(\\mu) = \\mathcal{B}(\\mu|\\alpha,\\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\mu^{\\alpha-1}(1-\\mu)^{\\beta-1}\n",
    "$$\n",
    "  - The Beta distribution is a **conjugate prior** for the Binomial distribution, which means that \n",
    "$$\n",
    "\\text{beta} \\propto \\text{binomial} \\times \\text{beta}\n",
    "$$\n",
    "  - $\\alpha$ and $\\beta$ are called **hyperparameters**, since they parameterize the distribution for another parameter ($\\mu$). E.g., $\\alpha=\\beta=1$ (uniform), $\\alpha=\\beta=0.5$ (Jeffreys prior), $\\alpha=\\beta=0$ (improper Laplace prior)\n",
    "  - If $\\alpha,\\beta$ are integers, then $\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)(\\Gamma(\\beta)} = \\frac{(\\alpha+\\beta)!}{\\alpha!\\,\\beta!}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Coin toss example (2): Parameter estimation\n",
    "\n",
    "- Infer posterior PDF over $\\mu$ through Bayes rule\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\mu|D) &\\propto p(D|\\mu)\\,p(\\mu|\\alpha,\\beta)  \\\\ \n",
    "        &= \\mu^n (1-\\mu)^{N-n} \\times \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1} \\\\\n",
    "        &= \\mu^{n+\\alpha-1} (1-\\mu)^{N-n+\\beta-1} \n",
    "\\end{align*}$$\n",
    "\n",
    "hence the posterior is also beta distributed as\n",
    "\n",
    "$$\n",
    "p(\\mu|D) = \\mathcal{B}(\\mu|\\,n+\\alpha, N-n+\\beta)\n",
    "$$\n",
    "\n",
    "\n",
    "- Essentially, **here ends the machine learning activity**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coin Toss Example (3): Prediction\n",
    "\n",
    "- Marginalize over the parameter posterior to get the predictive PDF for a new coin toss $x_\\bullet$, given the data $D$,\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(x_\\bullet=h|D)  &= \\int_0^1 p(x_\\bullet=h|\\mu)\\,p(\\mu|D) \\,\\mathrm{d}\\mu \\\\\n",
    "  &= \\int_0^1 \\mu \\times  \\mathcal{B}(\\mu|\\,n+\\alpha, N-n+\\beta) \\,\\mathrm{d}\\mu  \\\\\n",
    "  &= \\frac{n+\\alpha}{N+\\alpha+\\beta} \\qquad \\mbox{(a.k.a. Laplace rule)}\\hfill\n",
    "\\end{align*}$$\n",
    "\n",
    "- Now we're ready to solve our example problem: for $D=\\{hthhtth\\}$ and uniform prior ($\\alpha=\\beta=1$), we get\n",
    "\n",
    "$$ p(x_\\bullet=h|D)=\\frac{n+1}{N+2} = \\frac{4+1}{7+2} = \\frac{5}{9}$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coin Toss Example: What did we learn?\n",
    "\n",
    "- What did we learn from the data? Before seeing any data, we think that $$p(x_\\bullet=h)=\\left. p(x_\\bullet=h|D) \\right|_{n=N=0} = \\frac{\\alpha}{\\alpha + \\beta}\\,.$$ \n",
    "\n",
    "- After the $N$ coin tosses, we think that \n",
    "$$\n",
    "p(x_\\bullet=h|D) = \\frac{n+\\alpha}{N+\\alpha+\\beta} \\,.\n",
    "$$\n",
    "\n",
    "- Note the following decomposition\n",
    "\n",
    "$$\\begin{align*}\n",
    "    p(x_\\bullet=h|\\,D) &= \\frac{n+\\alpha}{N+\\alpha+\\beta} \\\\\n",
    "        &= \\frac{n}{N+\\alpha+\\beta} + \\frac{\\alpha}{N+\\alpha+\\beta} \\\\\n",
    "        &= \\frac{N}{N+\\alpha+\\beta}\\cdot \\frac{n}{N} + \\frac{\\alpha+\\beta}{N+\\alpha+\\beta} \\cdot \\frac{\\alpha}{\\alpha+\\beta} \\\\\n",
    "        &= \\underbrace{\\frac{\\alpha}{\\alpha+\\beta}}_{prior} + \\underbrace{\\frac{N}{N+\\alpha+\\beta}}_{gain}\\cdot \\big( \\underbrace{\\frac{n}{N}}_{MLE} - \\underbrace{\\frac{\\alpha}{\\alpha+\\beta}}_{prior} \\big)\n",
    "\\end{align*}$$\n",
    "\n",
    "- Note that, since $0\\leq\\text{gain}\\lt 1$, the Bayesian estimate lies between prior and maximum likelihood estimate.\n",
    "\n",
    "- For large $N$, the gain goes to $1$ and $p(x_\\bullet=h|D)$ goes to the maximum likelihood estimate (the relative frequency) $n/N$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE EXAMPLE\n",
    "\n",
    "**Bayesian evolution of $p(\\mu|D)$ for the coin toss**\n",
    "\n",
    "Let's see how $p(\\mu|D)$ evolves as we increase the number of coin tosses $N$. We'll use two different priors to demonstrate the effect of the prior on the posterior (set $N=0$ to inspect the prior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"interact-js-shim\">\n",
       "    <script charset=\"utf-8\">\n",
       "(function (IPython, $, _, MathJax) {\n",
       "    $.event.special.destroyed = {\n",
       "\tremove: function(o) {\n",
       "\t    if (o.handler) {\n",
       "\t\to.handler.apply(this, arguments)\n",
       "\t    }\n",
       "\t}\n",
       "    }\n",
       "\n",
       "    var OutputArea = IPython.version >= \"4.0.0\" ? require(\"notebook/js/outputarea\").OutputArea : IPython.OutputArea;\n",
       "\n",
       "    var redrawValue = function (container, type, val) {\n",
       "\tvar selector = $(\"<div/>\");\n",
       "\tvar oa = new OutputArea(_.extend(selector, {\n",
       "\t    selector: selector,\n",
       "\t    prompt_area: true,\n",
       "\t    events: IPython.events,\n",
       "\t    keyboard_manager: IPython.keyboard_manager\n",
       "\t})); // Hack to work with IPython 2.1.0\n",
       "\n",
       "\tswitch (type) {\n",
       "\tcase \"image/png\":\n",
       "            var _src = 'data:' + type + ';base64,' + val;\n",
       "\t    $(container).find(\"img\").attr('src', _src);\n",
       "\t    break;\n",
       "\tcase \"text/latex\":\n",
       "\t\tif (MathJax){\n",
       "\t\t\tvar math = MathJax.Hub.getAllJax(container)[0];\n",
       "\t\t\tMathJax.Hub.Queue([\"Text\", math, val.replace(/^\\${1,2}|\\${1,2}$/g, '')]);\n",
       "\t\t\tbreak;\n",
       "\t\t}\n",
       "\tdefault:\n",
       "\t    var toinsert = OutputArea.append_map[type].apply(\n",
       "\t\toa, [val, {}, selector]\n",
       "\t    );\n",
       "\t    $(container).empty().append(toinsert.contents());\n",
       "\t    selector.remove();\n",
       "\t}\n",
       "    }\n",
       "\n",
       "\n",
       "    $(document).ready(function() {\n",
       "\tfunction initComm(evt, data) {\n",
       "\t    var comm_manager = data.kernel.comm_manager;\n",
       "        //_.extend(comm_manager.targets, require(\"widgets/js/widget\"))\n",
       "\t    comm_manager.register_target(\"Signal\", function (comm) {\n",
       "            comm.on_msg(function (msg) {\n",
       "                var val = msg.content.data.value;\n",
       "                $(\".signal-\" + comm.comm_id).each(function() {\n",
       "                var type = $(this).data(\"type\");\n",
       "                if (typeof(val[type]) !== \"undefined\" && val[type] !== null) {\n",
       "                    redrawValue(this, type, val[type], type);\n",
       "                }\n",
       "                });\n",
       "                delete val;\n",
       "                delete msg.content.data.value;\n",
       "            });\n",
       "\t    });\n",
       "\n",
       "\t    // coordingate with Comm and redraw Signals\n",
       "\t    // XXX: Test using Reactive here to improve performance\n",
       "\t    $([IPython.events]).on(\n",
       "\t\t'output_appended.OutputArea', function (event, type, value, md, toinsert) {\n",
       "\t\t    if (md && md.reactive) {\n",
       "                // console.log(md.comm_id);\n",
       "                toinsert.addClass(\"signal-\" + md.comm_id);\n",
       "                toinsert.data(\"type\", type);\n",
       "                // Signal back indicating the mimetype required\n",
       "                var comm_manager = IPython.notebook.kernel.comm_manager;\n",
       "                var comm = comm_manager.comms[md.comm_id];\n",
       "                comm.then(function (c) {\n",
       "                    c.send({action: \"subscribe_mime\",\n",
       "                       mime: type});\n",
       "                    toinsert.bind(\"destroyed\", function() {\n",
       "                        c.send({action: \"unsubscribe_mime\",\n",
       "                               mime: type});\n",
       "                    });\n",
       "                })\n",
       "\t\t    }\n",
       "\t    });\n",
       "\t}\n",
       "\n",
       "\ttry {\n",
       "\t    // try to initialize right away. otherwise, wait on the status_started event.\n",
       "\t    initComm(undefined, IPython.notebook);\n",
       "\t} catch (e) {\n",
       "\t    $([IPython.events]).on('kernel_created.Kernel kernel_created.Session', initComm);\n",
       "\t}\n",
       "    });\n",
       "})(IPython, jQuery, _, MathJax);\n",
       "</script>\n",
       "    <script>\n",
       "        window.interactLoadedFlag = true\n",
       "       $(\"#interact-js-shim\").bind(\"destroyed\", function () {\n",
       "           if (window.interactLoadedFlag) {\n",
       "               console.warn(\"JavaScript required by Interact will be removed if you remove this cell or run using Interact more than once.\")\n",
       "           }\n",
       "       })\n",
       "       $([IPython.events]).on(\"kernel_starting.Kernel kernel_restarting.Kernel\", function () { window.interactLoadedFlag = false })\n",
       "   </script>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "LoadError",
     "evalue": "InitError: PyError (:PyImport_ImportModule) <type 'exceptions.ImportError'>\nImportError('No module named mpl_toolkits.mplot3d',)\n\nduring initialization of module PyPlot",
     "output_type": "error",
     "traceback": [
      "InitError: PyError (:PyImport_ImportModule) <type 'exceptions.ImportError'>\nImportError('No module named mpl_toolkits.mplot3d',)\n\nduring initialization of module PyPlot",
      "",
      " in pyerr_check at /Users/bert/.julia/v0.5/PyCall/src/exception.jl:56 [inlined]",
      " in pyerr_check at /Users/bert/.julia/v0.5/PyCall/src/exception.jl:61 [inlined]",
      " in macro expansion at /Users/bert/.julia/v0.5/PyCall/src/exception.jl:81 [inlined]",
      " in pyimport(::String) at /Users/bert/.julia/v0.5/PyCall/src/PyCall.jl:387",
      " in __init__() at /Users/bert/.julia/v0.5/PyPlot/src/PyPlot.jl:256",
      " in _include_from_serialized(::String) at ./loading.jl:150",
      " in _require_from_serialized(::Int64, ::Symbol, ::String, ::Bool) at ./loading.jl:187",
      " in _require_search_from_serialized(::Int64, ::Symbol, ::String, ::Bool) at ./loading.jl:217",
      " in _require_search_from_serialized(::Int64, ::Symbol, ::String, ::Bool) at /Applications/Julia-0.5.app/Contents/Resources/julia/lib/julia/sys.dylib:?",
      " in require(::Symbol) at ./loading.jl:371",
      " in require(::Symbol) at /Applications/Julia-0.5.app/Contents/Resources/julia/lib/julia/sys.dylib:?"
     ]
    }
   ],
   "source": [
    "using Reactive, Interact, PyPlot, Distributions\n",
    "f = figure()\n",
    "range = linspace(0,1,100)\n",
    "μ = 0.4\n",
    "samples = rand(192) .<= μ # Flip 192 coins\n",
    "@manipulate for N=0:1:192; withfig(f) do\n",
    "        n = sum(samples[1:N]) # Count number of heads in first N flips\n",
    "        posterior1 = Beta(1+n, 1+(N-n))\n",
    "        posterior2 = Beta(5+n, 5+(N-n))\n",
    "        plot(range, pdf(posterior1,range), \"k-\")\n",
    "        plot(range, pdf(posterior2,range), \"k--\")\n",
    "        xlabel(L\"\\mu\"); ylabel(L\"p(\\mu|\\mathcal{D})\"); grid()\n",
    "        title(L\"p(\\mu|\\mathcal{D})\"*\" based on N=$(N), n=$(n) (real μ=$(μ))\")\n",
    "        legend([\"Based on uniform prior \"*L\"B(1,1)\",\"Based on prior \"*L\"B(5,5)\"], loc=4)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ **With more data, the relevance of the prior diminishes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END OF CODE EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### From Posterior to Point-Estimate\n",
    "Sometimes we want just one 'best' parameter (vector), rather than a posterior distribution over parameters. Why?\n",
    "\n",
    "- Recall Bayesian prediction\n",
    "$$\n",
    "p(x|D) = \\int p(x|\\theta)p(\\theta|D)\\,\\mathrm{d}{\\theta}\n",
    "$$\n",
    "\n",
    "- If we approximate posterior $p(\\theta|D)$ by a delta function for one 'best' value $\\hat\\theta$, then the predictive distribution collapses to\n",
    "\n",
    "$$\n",
    "p(x|D)= \\int p(x|\\theta)\\,\\delta(\\theta-\\hat\\theta)\\,\\mathrm{d}{\\theta} = p(x|\\hat\\theta)\n",
    "$$\n",
    "\n",
    "- This is the model $p(x|\\theta)$ evaluated at $\\theta=\\hat\\theta$.\n",
    "- Note that $p(x|\\hat\\theta)$ is much easier to evaluate than the integral for full Bayesian prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Some Well-known Point-Estimates\n",
    "\n",
    "1. **Bayes estimate**$$\n",
    "\\hat \\theta_{bayes}  = \\int \\theta \\, p\\left( \\theta |D \\right)\n",
    "\\,\\mathrm{d}{\\theta}\n",
    "$$\n",
    "\n",
    "  - (homework). Proof that the Bayes estimate minimizes the expected mean-square error, i.e., proof that\n",
    "$$\n",
    "\\hat \\theta_{bayes} = \\arg\\min_{\\hat \\theta} \\int_\\theta (\\hat \\theta -\\theta)^2 p \\left( \\theta |D \\right) \\,\\mathrm{d}{\\theta}\n",
    "$$\n",
    "\n",
    "2. **Maximum A Posteriori** (MAP) estimate \n",
    "$$\n",
    "\\hat \\theta_{\\text{map}}=  \\arg\\max _{\\theta} p\\left( \\theta |D \\right) =\n",
    "\\arg \\max_{\\theta}  p\\left(D |\\theta \\right) \\, p\\left(\\theta \\right)\n",
    "$$\n",
    "\n",
    "3. **Maximum Likelihood** (ML) estimate\n",
    "$$\n",
    "\\hat \\theta_{ml}  = \\arg \\max_{\\theta}  p\\left(D |\\theta\\right)\n",
    "$$\n",
    "  - Note that Maximum Likelihood is MAP with uniform prior\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian vs Maximum Likelihood Learning\n",
    "\n",
    "Consider the task: predict a datum $x$ from an observed data set $D$.\n",
    "<table>\n",
    "<tr><td></td><td style=\"text-align:center\"> <b>Bayesian</b></td><td style=\"text-align:center\"> <b>Maximum Likelihood </b></td></tr>\n",
    "<tr><td>1. <b>Model Specification</b></td><td>Choose a model $m$ with data generating distribution $p(x|\\theta,m)$ and parameter prior $p(\\theta|m)$</td><td>Choose a model $m$ with same data generating distribution $p(x|\\theta,m)$. No need for priors.</td></tr>\n",
    "<tr><td>2. <b>Learning</b></td><td>use Bayes rule to find the parameter posterior,\n",
    "$$\n",
    "p(\\theta|D) = \\propto p(D|\\theta) p(\\theta)\n",
    "$$  </td><td>By Maximum Likelihood (ML) optimization,\n",
    "$$ \n",
    "    \\hat \\theta  = \\arg \\max_{\\theta}  p(D |\\theta)\n",
    "$$</td></tr>\n",
    "<tr><td>3. <b>Prediction</b></td><td>$$\n",
    "p(x|D) = \\int p(x|\\theta) p(\\theta|D) \\,\\mathrm{d}\\theta\n",
    "$$</td><td>\n",
    "$$ \n",
    "    p(x|D) =  p(x|\\hat\\theta)\n",
    "$$</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Card on Maximum Likelihood Estimation\n",
    "- Maximum Likelihood (ML) is MAP with uniform prior, or MAP is 'penalized' ML\n",
    "$$\n",
    "\\hat \\theta_{map}  = \\arg \\max _\\theta  \\{ \\overbrace{\\log\n",
    "p\\left( D|\\theta  \\right)}^{\\mbox{log-likelihood}} + \\overbrace{\\log\n",
    "p\\left( \\theta \\right)}^{\\mbox{penalty}} \\}\n",
    "$$\n",
    "- (good!). Works rather well if we have a lot of data because the influence of the prior diminishes with more data.\n",
    "- (bad). Cannot be used for model comparison. E.g. best model does generally not correspond to largest likelihood (see part-2, Tjalkens).\n",
    "- (good). Computationally often do-able. Useful fact (since $\\log$ is monotonously increasing):\n",
    "$$\\arg\\max_\\theta \\log p(D|\\theta) =  \\arg\\max_\\theta p(D|\\theta)$$\n",
    "\n",
    "$\\Rightarrow$ **ML estimation is an approximation to Bayesian learning**, but in the face of lots of available data for good reason a very popular learning method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "_The cell below loads the style file_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!--\n",
       "This HTML file contains custom styles and some javascript.\n",
       "Include it a Jupyter notebook for improved rendering.\n",
       "-->\n",
       "\n",
       "<!-- Fonts -->\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "\n",
       "<!-- Custom style -->\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.container {\n",
       "    min-width: 960px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:800px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       ".emphasis {\n",
       "    color: red;\n",
       "}\n",
       "\n",
       ".exercise {\n",
       "    color: green;\n",
       "}\n",
       "\n",
       ".proof {\n",
       "    color: blue;\n",
       "}\n",
       "\n",
       "code {\n",
       "  padding: 2px 4px !important;\n",
       "  font-size: 90% !important;\n",
       "  color: #222 !important;\n",
       "  background-color: #efefef !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       "/* This removes the actual style cells from the notebooks, but no in print mode\n",
       "   as they will be removed through some other method */\n",
       "@media not print {\n",
       "  .cell:nth-last-child(-n+2) {\n",
       "    display: none;\n",
       "  }\n",
       "}\n",
       "\n",
       "</style>\n",
       "\n",
       "<!-- MathJax styling -->\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.html\") do f\n",
    "    display(\"text/html\", readstring(f))\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
