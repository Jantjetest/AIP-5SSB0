{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Introduction to discriminative classification models\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional\n",
    "    - Bishop pp. 203-206 \n",
    "    - [T. Minka, Discriminative modes, not discriminative traing](./files/Minka-2005 -Discriminative-models-not-discriminative-training.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Problem: difficult class-conditional data distribitions\n",
    "\n",
    "Our task will be the same as in the preceding class on (generative) classification. But this time, the class-conditional data dsitributions look very non-Gaussian, yet the linear discriminative boundary looks easy enough:\n",
    " \n",
    "<span style=\"color:red\">Marco can you generate a data set in julia as below and plot. there shoudl be two classes with very non-Gaussian class-conditional densities but relatively easy discrimination boundaries. Woudl be really cool if we can change teh data set with a parameter, eg to make it **very** non-gaussian</span>\n",
    "\n",
    "![](./figures/fig-classification-not-gaussian.png)\n",
    "\n",
    "In this class, we will build a classifier for such cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Main Idea of Discriminative Classification \n",
    "\n",
    "- Again, a data set is given by  $D = \\{(x_1,y_1),\\dotsc,(x_N,y_N)\\}$ with $x_n \\in \\mathbb{R}^D$ and $y_n \\in \\mathcal{C}_k$, with $k=1,\\ldots,K$.\n",
    "\n",
    "-  Sometimes, the precise assumptions of the (multinomial-Gaussian) generative model $$p(x,\\mathcal{C}_k|\\theta) =  \\pi_k \\cdot \\mathcal{N}(x|\\mu_k,\\Sigma)$$ clearly do not match the data distribution.\n",
    "\n",
    "##### Idea\n",
    "\n",
    "- Model the posterior $$p(\\mathcal{C}_k|x)$$  _directly_, without any assumptions on the class densities.\n",
    "\n",
    "- Of course, this implies also that we build direct models for the **discrimination boundaries** \n",
    "  $$\\log \\frac{p(\\mathcal{C}_k|x)}{p(\\mathcal{C}_j|x)} =0$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Specification \n",
    "\n",
    "- <span style=\"color:blue\">[Q.]</span>What model should we use for $p(\\mathcal{C}_k|x)$?\n",
    "\n",
    "-   <span style=\"color:blue\">[A.]</span> Get inspiration from the generative approach: choose the familiar softmax structure for the posterior class probability\n",
    "$$\n",
    "p(\\mathcal{C}_k|x,\\theta_k) = \\frac{e^{\\theta_k^T x}}{\\sum_j e^{\\theta_j^T x}}\n",
    "$$\n",
    "but **do not impose a Gaussian structure on the classes**.\n",
    "\n",
    "- $\\Rightarrow$ There are **two key differences** between the discriminative and generative approach: \n",
    "  1. In the discriminative appraoch, the parameters $\\theta_k$ are **not** structured into $\\{\\mu_k,\\Sigma,\\pi_k \\}$. This provides discriminative approach with more flexibility.\n",
    "  2. ML learning for the discriminative approach by optimization of _conditional_ likelihood $\\prod_n p(y_n|x_n,\\theta)$ rather than _joint_ likelihood $\\prod_n p(y_n,x_n|\\theta)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ###  ML Estimation for Discriminative Classification\n",
    " \n",
    "- As we will see, ML estimation for discriminative classification is more complex than for the linear Gaussian generative approach.\n",
    "\n",
    "-  Work out the conditional log-likelihood \n",
    "     $$\n",
    "    \\mathrm{L}(\\theta) = \\log \\prod_n \\prod_k {\\underbrace{p(\\mathcal{C}_k|x_n,\\theta)}_{p_{nk}}}^{y_{nk}} = \\sum_{n,k} y_{nk} \\log p_{nk}\n",
    "     $$\n",
    "     \n",
    "- Use the fact that the softmax $\\phi_k=e^{z_k}/{\\sum_j e^{z_j}}$ has analytical derivative:\n",
    "\n",
    "$$ \\begin{align*}\n",
    " \\frac{\\partial \\phi_k}{\\partial z_j} &= \\frac{(\\sum_j e^{z_j})e^{z_k}\\delta_{kj}-e^{z_j}e^{z_k}}{(\\sum_j e^{z_j})^2} = \\frac{e^{z_k}}{\\sum_j e^{z_j}}\\delta_{kj} - \\frac{e^{z_j}}{\\sum_j e^{z_j}} \\frac{e^{z_k}}{\\sum_j e^{z_j}}\\\\\n",
    "     &= \\phi_k \\cdot(\\delta_{kj}-\\phi_j)\n",
    " \\end{align*}$$\n",
    "\n",
    "<!---\n",
    "%    -  Again we try to minimize the cross-entropy ($\\sum_{nk} y_{nk} \\log \\frac{y_{nk}}{p_{nk}}$) between the data `targets' $t_{nk}$ and the model outputs $p_{nk}$.\n",
    "--->\n",
    "\n",
    " -  Take the derivative of $\\mathrm{L}(\\theta)$ (or: how to spend a hour ...)\n",
    "$$\\begin{align*} \n",
    "\\nabla_{\\theta_j} \\mathrm{L}(\\theta) &= \\sum_{n,k} \\frac{\\partial \\mathrm{L}_{nk}}{\\partial p_{nk}} \\cdot\\frac{\\partial p_{nk}}{\\partial z_{nj}}\\cdot\\frac{\\partial z_{nj}}{\\partial \\theta_j} \\\\\n",
    "  &= \\sum_{n,k} \\frac{y_{nk}}{p_{nk}} \\cdot p_{nk} (\\delta_{kj}-p_{nj}) \\cdot x_n \\\\\n",
    "  &= \\sum_n \\Big( y_{nj} (1-p_{nj}) -\\sum_{k\\neq j} y_{nk} p_{nj} \\Big) \\cdot x_n \\\\\n",
    "  &= -\\sum_n \\left( y_{nj} - p_{nj} \\right)\\cdot x_n \\\\\n",
    "  &= -\\sum_n \\Big( \\underbrace{y_{nj}}_{\\text{target}} - \\underbrace{\\frac{e^{\\theta_j^T x_n}}{\\sum_{j^\\prime} e^{\\theta_{j^\\prime}^T x_n}}}_{\\text{prediction}} \\Big)\\cdot x_n \n",
    "\\end{align*}$$\n",
    "\n",
    "  \n",
    "- The derivation for the derivative was painful, but the result is extremely simple. Compare the gradients for linear and logistic regression:\n",
    "$$\\begin{align*}\n",
    "\\nabla_\\theta \\mathrm{L}(\\theta) &= - \\sum_n \\left(y_n - \\theta^T x_n \\right)  x_n  \\tag{linear} \\\\\n",
    "\\nabla_\\theta \\mathrm{L}(\\theta) &= - \\sum_n \\left(y_n - \\frac{1}{1+e^{-\\theta^Tx_n}} \\right) x_n\n",
    " \\tag{logistic}\n",
    "\\end{align*}$$\n",
    "In both cases\n",
    "$$\n",
    "\\nabla_\\theta \\mathrm{L} = - \\sum_n \\left( \\text{target}_n - \\text{prediction}_n \\right) \\cdot \\text{input}_n \n",
    "$$\n",
    "\n",
    "- The parameter vector $\\theta$ for logistic regression can be estimated through iterative gradient-based adaptation. E.g. (with iteration index $i$),\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}^{(i+1)} = \\hat{\\theta}^{(i)} - \\eta \\cdot \\nabla_\\theta \\mathrm{L}(\\theta) \n",
    "$$\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application - Classify a new input\n",
    "\n",
    "-  Discriminative model-based prediction for a new input $x_\\bullet$ is easy, namely substitute the ML estimate in the model to get\n",
    "\n",
    "$$\n",
    "p(\\mathcal{C}_k |\\, x_\\bullet,\\hat\\theta) = \\frac{ \\mathrm{exp}\\left( \\hat \\theta_k^T x_\\bullet \\right) }{ \\sum_j \\mathrm{exp}\\left(\\hat \\theta_j^T x_\\bullet \\right)} \n",
    "  \\propto \\mathrm{exp}\\left(\\hat \\theta_k^T x_\\bullet\\right) \n",
    "$$\n",
    "\n",
    "-  The contours of equal probability (**discriminant boundaries**) are lines (hyperplanes) in feature space given by\n",
    "$$\n",
    "\\log \\frac{{p(\\mathcal{C}_k|x,\\hat \\theta )}}{{p(\\mathcal{C}_j|x,\\hat \\theta )}} = \\left( \\hat{\\theta}_{k} - \\hat{\\theta}_j\\right) ^T x = 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Marco: Can you finish the simulation here: \n",
    "(1) run discriminative classification and plot the discrimination boudaries. (2)computer posterior prob for a new data point. (3) Then run generartive classification on the same data set and show the difference,eg overlay discrimination boundaries in teh same plot .</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Recap Classification\n",
    "<table>\n",
    "<tr> <td></td><td>**Generative**</td> <td>**Discriminative**</td> </tr> \n",
    "\n",
    "<tr> <td>1</td><td>Like **density estimation**, model joint prob.\n",
    "$$p(\\mathcal{C}_k) p(x|\\mathcal{C}_k) = \\pi_k \\mathcal{N}(\\mu_k,\\Sigma)$$</td> <td>Like (linear) **regression**, model conditional\n",
    "$$p(\\mathcal{C}_k|x,\\theta)$$</td> </tr>\n",
    "\n",
    "<tr> <td>2</td><td>Leads to **softmax** posterior class probability\n",
    "$$ p(\\mathcal{C}_k|x,\\theta ) = e^{\\theta_k^T x}/Z$$\n",
    "with **structured** $\\theta$</td> <td> **Choose** also softmax posterior class probability\n",
    "$$ p(\\mathcal{C}_k|x,\\theta ) = e^{\\theta_k^T x}/Z$$\n",
    "but now with 'free' $\\theta$</td> </tr>\n",
    "\n",
    "<tr> <td>3</td><td>For Gaussian $p(x|\\mathcal{C}_k)$ and multinomial priors,\n",
    "$$\\hat \\theta_k  = \\left[ {\\begin{array}{c}\n",
    "   { - \\frac{1}{2} \\mu_k^T \\sigma^{-1} \\mu_k  + \\log \\pi_k}  \\\\\n",
    "   {\\sigma^{-1} \\mu_k }  \\\\\n",
    "\\end{array}} \\right]$$\n",
    "in one shot.</td> <td>Find $\\hat\\theta$ through gradient-based adaptation\n",
    "$$-\\nabla_{\\theta} \\log p(D|\\theta) = \\sum_n \\left(t_n - \\frac{1}{1+e^{-\\theta^Tx_n}}\\right)x_n$$</td> </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----\n",
    "_The cell below loads the style file_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.cell { /* set cell width */\n",
       "    width: 750px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    width: 1000px;\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:600px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.html\") do f\n",
    "    display(\"text/html\", readall(f))\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
