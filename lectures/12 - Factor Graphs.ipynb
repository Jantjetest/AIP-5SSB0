{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factor Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Goals of this lecture\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Materials\n",
    "\n",
    "- [Video lecture](https://www.youtube.com/watch?v=Fv2YbVg9Frc&t=31) by Frederico Wadehn (ETH Zurich)\n",
    "- [Loeliger, 2004](...), read sections ...\n",
    "- [Loeliger, 2007](...), read sections ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Factor Graphs?\n",
    "\n",
    "- Factor graphs provide an efficient approach to attacking the most important computational problems in probabilistic modelling:\n",
    "\n",
    "  - **Marginalization**\n",
    "  \n",
    "$$\n",
    "\\bar{f}(x_2) = \\int f(x_1,x_2,x_3,x_4,x_5) \\, \\mathrm{d}x_1  \\mathrm{d}x_3 \\mathrm{d}x_4 \\mathrm{d}x_5 \n",
    "$$\n",
    "\n",
    "  - **Maximization** (computing the \"max-marginal\")\n",
    "  \n",
    "$$\n",
    "\\hat{f}(x_2) = \\max_{x_1,x_3,x_4,x_5} f(x_1,x_2,x_3,x_4,x_5)  \n",
    "$$\n",
    "\n",
    "- Since these computations suffer from the \"curse of dimensionality\", we often need to solve a simpler problem in order to get an answer. \n",
    "\n",
    "- Factorization helps here, e.g., if $f(x_1,x_2,x_3,x_4,x_5) = \\prod_{k=1}^5 f_k(x_k)$, then \n",
    "\n",
    "$$\n",
    "\\hat{f}(x_2) = f_2(x_2) \\prod_{k=1,3,4,5} \\max f_k(x_k)\n",
    "$$\n",
    "which usually is _much_ easier to compute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "###  Construction Rules\n",
    "\n",
    "- Consider a function \n",
    "$$\n",
    "f(x_1,x_2,x_3,x_4,x_5) = f_a(x_1,x_2,x_3) \\cdot f_b(x_3,x_4,x_5) \\cdot f_c(x_4)\n",
    "$$\n",
    "\n",
    "- The factorization of this function can be graphically represented by a **Forney-style Factor Graph** (FFG):\n",
    "\n",
    "\\begin{center}\n",
    "INSERT FFG EXAMPLE GRAPH\n",
    "\\end{center}\n",
    "\n",
    "- An FFG is an **undirected** graph subject to the followong construction rules ([Forney, 2001](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=910573&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F18%2F19638%2F00910573.pdf%3Farnumber%3D910573), [Loeliger, 2004]())\n",
    "\n",
    "  1. A **node** for every factor;\n",
    "  1. An **edge** (or **half-edge**) for every variable;\n",
    "  1. Node $g$ is connected to edge $x$ **iff** variable $x$ appears in factor $g$.\n",
    "\n",
    "#### Some Terminology\n",
    "\n",
    "- $f$ is called the **global function** and $f_\\bullet$ are the **factors**. \n",
    "- A **configuration** is an assigment of values to all variables.\n",
    "- The **configuation space** is the set of all configutations, i.e., the domain of $f$\n",
    "- A configution $\\omega=(x_1,x_2,x_3,x_4,x_5)$ is said to be **valid** iff $f(\\omega) \\neq 0$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Equality Nodes for Branching Points\n",
    "\n",
    "\n",
    "- Note that a variable can appear in maximally two factors in an FFG.\n",
    "\n",
    "- Consider the factorization (where $x_2$ appears in three factors) \n",
    "$$\n",
    " f(x_1,x_2,x_3,x_4) = f_a(x_1,x_2)\\cdot f_b(x_2,x_3) \\cdot f_c(x_2,x_4)\n",
    "$$\n",
    "\n",
    "- For the factor graph representation, we will instead consider the function $g$, defined as\n",
    "$$\n",
    " g(x_1,x_2,x_2^\\prime,x_2^{\\prime\\prime},x_3,x_4) = f_a(x_1,x_2)\\cdot f_b(x_2^\\prime,x_3) \\cdot f_c(x_2^{\\prime\\prime},x_4) \\cdot f_=(x_2,x_2^\\prime,x_2^{\\prime\\prime})\n",
    "$$\n",
    "  where \n",
    "$$\n",
    "f_= \\triangleq \\delta(x-x_2^\\prime) \\delta(x-x_2^{\\prime\\prime})\n",
    "$$\n",
    "  (where $\\delta$ is the Kronecker delta for discrete variables and the Dirac delta for continuously valued variables)\n",
    "  \n",
    "- Note that through introduction of auxiliary variables $X_2^\\prime$ and $X_2^{\\prime\\prime}$ each variable in $g$ appears in maximally two factors. \n",
    "\n",
    "- The constraint $f_=(x,x^\\prime,x^{\\prime\\prime})$ enforces that $X=X^\\prime=X^{\\prime\\prime}$ **for every valid configuration**.\n",
    "\n",
    "[show graph of g]\n",
    "\n",
    "- Also note that $f$ is a marginal of $g$, since\n",
    "$$\n",
    "f(x_1,x_2,x_3,x_4) = \\int g(x_1,x_2,x_2^\\prime,x_2^{\\prime\\prime},x_3,x_4)\\, \\mathrm{d}x_2^\\prime \\mathrm{d}x_2^{\\prime\\prime}\n",
    "$$\n",
    "\n",
    "- Therefore, any inference problem on $f$ can be executed by a corresponding inference problem on $g$, e.g.,\n",
    "\\begin{align}\n",
    "f(x_1|x_2) &\\triangleq \\frac{\\int f(x_1,x_2,x_3,x_4) \\,\\mathrm{d}x_3 \\mathrm{d}x_4 }{ \\int f(x_1,x_2,x_3,x_4) \\,\\mathrm{d}x_1 \\mathrm{d}x_3 \\mathrm{d}x_4} \\\\\n",
    "  &= \\frac{\\int g(x_1,x_2,x_2^\\prime,x_2^{\\prime\\prime},x_3,x_4) \\,\\mathrm{d}x_2^\\prime \\mathrm{d}x_2^{\\prime\\prime} \\mathrm{d}x_3 \\mathrm{d}x_4 }{ \\int g(x_1,x_2,x_2^\\prime,x_2^{\\prime\\prime},x_3,x_4) \\,\\mathrm{d}x_1 \\mathrm{d}x_2^\\prime \\mathrm{d}x_2^{\\prime\\prime} \\mathrm{d}x_3 \\mathrm{d}x_4} \\\\\n",
    "  &\\triangleq g(x_1|x_2)\n",
    "\\end{align}\n",
    "\n",
    "$\\Rightarrow$ Any factorization of a global function $f$ can be represented by a Forney-style Factor Graph.\n",
    "\n",
    "- More generally, equality nodes are useful to express hard constraints between variables. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Models as Factor Graphs\n",
    "\n",
    "- FFGs can be used to express conditional independence (factorization) in probalistic models. \n",
    "- For example, the (previously shown) graph for $f_a(x_1,x_2,x_3) \\cdot f_b(x_3,x_4,x_5) \\cdot f_c(x_4)$ would also represent the model\n",
    "$$\n",
    "p(x_1,x_2,x_3,x_4,x_5) = p(x_1,x_2|x_3) \\cdot p(x_3,x_5|x_4) \\cdot p(x_4)\n",
    "$$\n",
    "where we identify $f_a(x_1,x_2,x_3)=p(x_1,x_2|x_3)$, $f_b(x_3,x_4,x_5)= p(x_3,x_5|x_4)$ and $f_c(x_4)$\n",
    "\n",
    "[show graph]\n",
    "\n",
    "- Factorizations provide opportunities to cut on the amount of needed computations when doing inference. In what follows, we will use FFGs to process these opportunities in an automatic way (i.e., by message passing). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Processing Observations in a Factor Graph\n",
    "\n",
    "- Consider a generative model \n",
    "$$p(x,y_1,y_2) = p(x)\\,p(y_1|x)\\,p(y_2|x) .$$ \n",
    "This model expresses the assumption that $Y_1$ and $Y_2$ are independent measurements of $X$.\n",
    "\n",
    "[show the FFG]\n",
    "\n",
    "- Assume that we are interested in the posterior for $X$ after observing $Y_1=y_1$ and $Y_2=y_2$. Using the product rule we get\n",
    "$$\n",
    "p(x|y_1,y_2) = \\frac{p(x,y_1,y_2)}{p(y_1,y_2)} \\propto p(x,y_1,y_2) = p(x)\\,p(y_1|x)\\,p(y_2|x)\n",
    "$$\n",
    "- Crucially, aside from a scaling factor $\\frac{1}{p(y_1,y_2)}$ (that is independent of $x$), the factorizations for the posterior $p(x|y_1,y_2)$ and the full model $p(x,y_1,y_2)$ are the same.\n",
    "\n",
    "- If we only are interested in posteriors over hidden states and parameters, the scale factors are not relevant since we can always normalize a scaled distribution.  \n",
    "\n",
    "$\\Rightarrow$ Making observations does not change the structure of the factor graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference by Closing Boxes\n",
    "\n",
    "- Assume we wish to compute the marginal\n",
    "$$\n",
    "\\bar{f}(x_3) = \\sum_{x_1,x_2,x_4,x_5,x_6,x_7}f(x_1,x_2,\\ldots,x_7)\n",
    "$$\n",
    "where $f$ is factorized as given by the following FFG\n",
    "\n",
    "<img src=\"message-passing-in-FFG-1.png\">\n",
    "\n",
    "- Due to the factorization, we decompose this sum by the distributive law as\n",
    "\\begin{align}\n",
    "\\bar{f}(x_3) = & \\underbrace{ \\left( \\sum_{x_1,x_2} f_a(x_1)\\,f_b(x_2)\\,f_c(x_1,x_2,x_3)\\right) }_{\\overrightarrow{\\mu}_{X_3}(x_3)}  \\\\\n",
    "  & \\underbrace{ \\cdot\\left( \\sum_{x_4,x_5} f_d(x_4)\\,f_e(x_3,x_4,x_5) \\cdot \\underbrace{ \\left( \\sum_{x_6,x_7} f_f(x_5,x_6,x_7)\\,f_g(x_7)\\right) }_{\\overleftarrow{\\mu}_{X_5}(x_5)} \\right) }_{\\overleftarrow{\\mu}_{X_3}(x_3)}\n",
    "\\end{align}\n",
    "which is computationally (much) lighter than executing the full sum $\\sum_{x_1,\\ldots,x_7}f(x_1,x_2,\\ldots,x_7)$\n",
    "\n",
    "- Note that the \"message\" $\\overleftarrow{\\mu}_{X_5}(x_5)$ is obtained by _multiplying all factors that are enclosed by the red box, followed by marginalization over all enclosed variables_. This is the **Closing the Box**-rule, which is a general recipe for marginalization of hidden variables and leads to a new factor with outgoing message $ = \\sum_{ \\stackrel{ \\textrm{enclosed} }{ \\textrm{variables} } } \\;\\prod_{\\stackrel{ \\textrm{enclosed} }{ \\textrm{factors} }}$.   \n",
    "\n",
    "\n",
    "- Observe a shared pattern for computing the \"messages\":\n",
    "\\begin{align}\n",
    "\\overrightarrow{\\mu}_{X_3}(x_3) &= \\sum_{x_1,x_2} \\overrightarrow{\\mu}_{X_1}(x_1) \\overrightarrow{\\mu}_{X_2}(x_2) f_c(x_1,x_2,x_3) \\\\\n",
    "\\overleftarrow{\\mu}_{X_5}(x_5) &= \\sum_{x_6,x_7} \\overrightarrow{\\mu}_{X_7}(x_7) f_f(x_5,x_6,x_7) \\\\\n",
    "\\overleftarrow{\\mu}_{X_3}(x_3) &= \\sum_{x_4,x_5} \\overrightarrow{\\mu}_{X_4}(x_4) \\overleftarrow{\\mu}_{X_3}(x_3) f_e(x_3,x_4,x_5)\n",
    "\\end{align}\n",
    "where we defined $\\overrightarrow{\\mu}_{X_1}(x_1) \\triangleq f_a(x_1)$, $\\overrightarrow{\\mu}_{X_2}(x_2) \\triangleq f_b(x_2)$ etc. \n",
    "\n",
    "- This pattern for computing messages applies generally and is called the **Sum-Product update rule**:\n",
    "\n",
    "$$\n",
    "\\overrightarrow{\\mu}_{Y}(y) = \\sum_{x_1,\\ldots,x_n} \\overrightarrow{\\mu}_{X_1}(x_1)\\cdots \\overrightarrow{\\mu}_{X_n}(x_n) \\,f(y,x_1,\\ldots,x_n) \n",
    "$$\n",
    "\n",
    "[show graph]\n",
    "\n",
    "The **Sum-Product Theorem**\n",
    "- If the factor graph for a function $f$ has **no cycles**, then \n",
    "$$\n",
    "f_X(x) = \\overrightarrow{\\mu}_{X}(x)\\cdot \\overleftarrow{\\mu}_{X}(x)\n",
    "$$\n",
    "\n",
    "[Insert graph with equality node the explain]\n",
    "\n",
    "\n",
    "$\\Rightarrow$ The marginal $\\bar{f}(x_3) = \\sum_{x_1,x_2,x_4,x_5,x_6,x_7}f(x_1,x_2,\\ldots,x_7)$ can be efficiently computed through sum-product messages. Computing marginals through SP message passing is called the **Sum-Product Algorithm**.\n",
    "\n",
    "\n",
    "Exercise: \n",
    "- Reflect on the fact that we now have methods for both marginalization and processing observations in FFGs. In principle, we are sufficiently equipped to do inference in probabilistic models. Draw the graph for $p(x_1,x_2,x_3)=f_a(x_1)\\cdot f_b(x_1,x_2)\\cdot f_c(x_2,x_3)$ and show which boxes need to be closed for computing $p(x_1|x_2)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SP Messages for the Equality Node\n",
    "\n",
    "Let´s compute the SP messages for the **equality node** $f_=(x,y,z) = \\delta(z-x)\\delta(z-y)$: \n",
    "\n",
    "[show graph of $f_=$]\n",
    "\n",
    "\\begin{align}\n",
    "\\overrightarrow{\\mu}_{Z}(z) &= \\int  \\overrightarrow{\\mu}_{X}(x) \\overrightarrow{\\mu}_{Y}(y) \\,\\delta(z-x)\\delta(z-y) \\,\\mathrm{d}x \\mathrm{d}y \\\\\n",
    "   &=  \\overrightarrow{\\mu}_{X}(z) \\cdot \\int  \\overrightarrow{\\mu}_{Y}(y) \\,\\delta(z-y) \\,\\mathrm{d}y \\\\\n",
    "   &=  \\overrightarrow{\\mu}_{X}(z) \\overrightarrow{\\mu}_{Y}(z) \n",
    "\\end{align}\n",
    "\n",
    "- By symmetry, this also implies (for the same equality node) that\n",
    "\n",
    "\\begin{align}\n",
    "\\overleftarrow{\\mu}_{X}(x) &= \\overrightarrow{\\mu}_{Y}(x) \\overleftarrow{\\mu}_{Z}(x) \\quad \\text{and} \\\\\n",
    "\\overleftarrow{\\mu}_{Y}(y) &= \\overrightarrow{\\mu}_{X}(y) \\overleftarrow{\\mu}_{Z}(y)\\,.\n",
    "\\end{align}\n",
    "\n",
    "- Let us now consider the case of Gaussian messages $\\overrightarrow{\\mu}_{X}(x) = \\mathcal{N}(\\overrightarrow{m}_X,\\overrightarrow{V}_X)$, $\\overrightarrow{\\mu}_{Y}(y) = \\mathcal{N}(\\overrightarrow{m}_Y,\\overrightarrow{V}_Y)$ and $\\overrightarrow{\\mu}_{Z}(z) = \\mathcal{N}(\\overrightarrow{m}_Z,\\overrightarrow{V}_Z)$. Let´s also define the precision matrices $\\overrightarrow{W}_X \\triangleq \\overrightarrow{V}_X^{-1}$ and similarly for $Y$ and $Z$. Then applying the SP update rule leads to multiplication of two Gaussian distributions, resulting in \n",
    "\n",
    "\\begin{align}\n",
    "\\overrightarrow{W}_Z &= \\overrightarrow{W}_X + \\overrightarrow{W}_Y \\\\ \n",
    "\\overrightarrow{W}_Z \\overrightarrow{m}_z &= \\overrightarrow{W}_X \\overrightarrow{m}_X + \\overrightarrow{W}_Y \\overrightarrow{m}_Y\n",
    "\\end{align}\n",
    "\n",
    "- It follows that message passing through an equality node is akin to applying Bayes rule, i.e., fusion of two information sources. Does this make sense?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SP Messages for the Addition and Multiplication Nodes\n",
    "\n",
    "Next, consider a **addition node** $f_+(x,y,z) = \\delta(z-x-y)$:  \n",
    "\n",
    "[show graph of $f_+$]\n",
    "\n",
    "\\begin{align}\n",
    "\\overrightarrow{\\mu}_{Z}(z) &= \\int  \\overrightarrow{\\mu}_{X}(x) \\overrightarrow{\\mu}_{Y}(y) \\,\\delta(z-x-y) \\,\\mathrm{d}x \\mathrm{d}y \\\\\n",
    "   &=  \\int  \\overrightarrow{\\mu}_{X}(z) \\overrightarrow{\\mu}_{Y}(z-x) \\,\\mathrm{d}x \\,, \n",
    "\\end{align}\n",
    "\n",
    "i.e., $\\overrightarrow{\\mu}_{Z}$ is the convolution of the messages $\\overrightarrow{\\mu}_{X}$ and $\\overrightarrow{\\mu}_{Y}$.\n",
    "\n",
    "- Of course, for Gaussian messages, these update rules evaluate to\n",
    "\\begin{align}\n",
    "\\overrightarrow{m}_Z &= \\overrightarrow{m}_X + \\overrightarrow{m}_Y \\\\ \n",
    "\\overrightarrow{V}_z &= \\overrightarrow{V}_X + \\overrightarrow{V}_Y \\,.\n",
    "\\end{align}\n",
    "\n",
    "- Exercise: \n",
    "  - For the same summation node, work out the SP update rule for the _backward_ message $\\overleftarrow{\\mu}_{X}(x)$ as a function of $\\overrightarrow{\\mu}_{Y}(y)$ and  $\\overleftarrow{\\mu}_{Z}(z)$? And further refine the answer for Gaussian messages.   \n",
    "\n",
    "\n",
    "- Next, let us consider a **multiplication** by a fixed (invertable matrix) gain $f_A(x,y) = \\delta(y-Ax)$\n",
    "\n",
    "[show graph of $f_A$]\n",
    "\n",
    "\\begin{align}\n",
    "\\overrightarrow{\\mu}_{Y}(y) &= \\int  \\overrightarrow{\\mu}_{X}(x) \\,\\delta(y-Ax) \\,\\mathrm{d}x \\\\\n",
    "   &= \\overrightarrow{\\mu}_{X}(A^{-1}y) \\,.\n",
    "\\end{align}\n",
    "\n",
    "For a Gaussian message input message $\\overrightarrow{\\mu}_{X}(x) = \\mathcal{N}(\\overrightarrow{m}_{X},\\overrightarrow{V}_{X})$, the output message is also Gaussian with \n",
    "\n",
    "\\begin{align}\n",
    "\\overrightarrow{m}_{Y} &= A\\overrightarrow{m}_{X} \\\\\n",
    "\\overrightarrow{V}_{Y} &= A\\overrightarrow{V}_{X}A^T\n",
    "\\end{align}\n",
    "\n",
    "since \n",
    "\n",
    "\\begin{align}\n",
    "\\overrightarrow{\\mu}_{Y}(y) &= \\overrightarrow{\\mu}_{X}(A^{-1}y) \\\\\n",
    "  &\\propto \\exp \\left( -\\frac{1}{2} \\left( A^{-1}y - \\overrightarrow{m}_{X}\\right)^T \\overrightarrow{V}_{X}^{-1} \\left(  A^{-1}y - \\overrightarrow{m}_{X}\\right)\\right) \\\\\n",
    "   &= \\exp \\left( -\\frac{1}{2} \\left( y - A\\overrightarrow{m}_{X}\\right)^T A^{-T}\\overrightarrow{V}_{X}^{-1} A \\left( y - A\\overrightarrow{m}_{X}\\right)\\right) \\\\\n",
    "  &\\propto  \\mathcal{N}(A\\overrightarrow{m}_{X},A\\overrightarrow{V}_{X}A^T) \\,.\n",
    "\\end{align}\n",
    "\n",
    "Excercise:\n",
    "\n",
    "- Proof that, for the same factor $\\delta(y-Ax)$ and Gaussian messages, the (backward) sum-product message $\\overleftarrow{\\mu}_{X}$ is given by \n",
    "\\begin{align}\n",
    "\\overleftarrow{\\xi}_{X} &= A^T\\overleftarrow{\\xi}_{Y} \\\\\n",
    "\\overleftarrow{W}_{X} &= A^T\\overleftarrow{W}_{Y}A\n",
    "\\end{align}\n",
    "where $\\overleftarrow{\\xi}_X \\triangleq \\overleftarrow{W}_X \\overleftarrow{m}_X$ and $\\overleftarrow{W}_{X} \\triangleq \\overleftarrow{V}_{X}^{-1}$ (and similarly for $Y$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SP Messages for Terminal Nodes\n",
    "\n",
    "- In order to complete the message passing framework, we need to deal with graph terminals and half-edges.  \n",
    "\n",
    "- Let us first consider a **Gaussian factor**  with given parameters $m=\\hat{m}$ and $V=\\hat{V}$: \n",
    "\n",
    "$$\n",
    "f_{\\mathcal{N}}(x)= \\mathcal{N}(x|\\hat{m},\\hat{V})\\, .\n",
    "$$  \n",
    "\n",
    "[insert graph]\n",
    "\n",
    "- The outgoing sum-product message of this node is \n",
    "\\begin{align}\n",
    "\\overrightarrow{\\mu}_X(x) = f_{\\mathcal{N}}(x)\n",
    "\\end{align}\n",
    "since there are no internal variables to be integrated over (nor any incoming messages). Nodes that are connected to one edge only are called **Terminal Nodes**. \n",
    "\n",
    "- Note that the _outgoing_ message for a terminal node is the factor itself. We can use this fact to show the dual interpretation the sum-product update rule is implied by the closing-the-box rule.   The two following networks are equivalent \n",
    "\n",
    "[show graph 1 with _messages_ $\\overrightarrow{\\mu}(x)$ on the half-egdes and graph 2 with terminal nodes with _factors_ $\\overrightarrow{\\mu}(x)$; Show the red box in graph2]. \n",
    "\n",
    "Application of the closing-the-box rule on the red box leads immediately to the sum-product update rule.  \n",
    "\n",
    "- Terminal nodes are very convenient to describe \n",
    "  - **prior distributions** over parameters, e.g., $$f_\\Theta(\\theta) = \\begin{cases} 1 & \\text{for}\\; 0 \\leq\\theta \\leq 1 \\\\ 0 & \\text{otherwise}\\end{cases} \\,;$$ \n",
    "  - **initial conditions** (which are also prior distributions) for state variables, e.g., $$f_{Z}(z_0) = \\mathcal{N}(\\theta|1,2)$$ specifies out state of knowledge about $Z$ at $t=0$;\n",
    "  - **observed variables**, e.g., use a factor $$f_Y(y) = \\delta(y-3)$$ to terminate the edge for variable $Y$ if $y=3$ is observed. \n",
    "  \n",
    "- Terminal nodes for _observed_ variables interface to the rest of the graph by sending messages as delta distributions. We usually draw terminal nodes for observed variables in the graph by smaller solid-black squares. This is just to help the visualization of the graph, since the computational rules are no different than for other nodes.  \n",
    "\n",
    "- Finally, terminal nodes can also be related to unobserved half-edges. Consider the two models \\begin{align}\n",
    "f(x_1,x_2) &= f_a(x_1)\\cdot f_b(x_1,x_2) \\\\\n",
    "f^\\prime (x_1,x_2) &= f_a(x_1)\\cdot f_b(x_1,x_2) \\cdot f_c(x_2)\n",
    "\\end{align}\n",
    "with $f_c(x_2)=1$. For all valid configurations, both models evaluate to exactly the same value. Hence, the message coming from the open end of a half-edge always equals $1$ and can be understood as the outgoing message from a terminal node with an uninformative (uniform) distribution. Please think about this.   \n",
    "\n",
    "- In short, we use terminal nodes to interface the graph to the external world by setting priors on states and parameters, by specifying observations (and, mostly useful for software implementation, by terminating unobserved half-edges).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference in Linear Gaussian Models by Sum-Product Message Passing\n",
    "\n",
    "- The foregoing message update rules can be extended to all scenarios involving additions, fixed-gain multiplications and branching (equality nodes), thus creating a completely **automatable inference framework** for factorized linear Gaussian models, cf. [Loeliger, 2007](http://www.isiweb.ee.ethz.ch/papers/docu/aloe-jdau-juhu-skor-2007-1.pdf).\n",
    "\n",
    "- The update rules for elementary and important node types can be put in a Table (see below). If the update rules for all node types in a graph have been tabulated, then inference by message passing comes down to a set of table-lookup operations. This also works for large graphs (where ´manual´ inference becomes intractable).\n",
    "\n",
    "- If the graph contains no cycles, the Sum-Product Algorithm computes **exact** marginals for all hidden variables.\n",
    "\n",
    "- If the graph contains cycles, we have in principle an infinite tree without terminals. In this case, the SP Algorithm is not guaranteed to find exact marginals. In practice, if we apply the SP algorithm for just a few iterations we often find satisfying approximate marginals.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "- Consider a variable $X$ with measurements $D=\\{x_1,x_2\\}$. We assume the following model for $X$:\n",
    "\\begin{align}\n",
    "p(D,\\theta) &= p(\\theta)\\cdot \\prod_{n=1}^2 p(x_n|\\theta)\\,,  \\\\\n",
    "p(\\theta) &= \\mathcal{N}(\\theta|0,1)\\,, \\\\\n",
    "p(x_n|\\theta) &= \\mathcal{N}(x_n|\\theta,1)\n",
    "\\end{align}\n",
    "Draw the factor graph and infer $\\theta$ through the SPA. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example: Gaussian Mixture Model\n",
    "\n",
    "[TODO Code a GMM with fixed parameters as a FFG here (in ForneyLab) and print all messages.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: the Hidden Markov Model (HMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.10",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
