{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Gaussians\n",
    "======="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sums and Transformations of Gaussian Variables\n",
    "\n",
    "- The Gaussian distribution\n",
    "$$\n",
    "\\mathcal{N}(x|\\mu,\\Sigma) = |2 \\pi \\Sigma |^{-\\frac{1}{2}} \\,\\mathrm{exp}\\left\\{-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\right\\}\n",
    "$$\n",
    "\n",
    "for variable $x$ is completely specified by its mean $\\mu$ and variance $\\Sigma$. \n",
    "\n",
    "- $\\Lambda = \\Sigma^{-1}$ is called the **precision matrix**.\n",
    "\n",
    "- The sum of two Gaussian _distributions_ is NOT Gaussian. Why not?\n",
    "\n",
    "- A **linear transformation** $z=Ax+b$ of a Gaussian variable $\\mathcal{N}(x|\\mu,\\Sigma)$ is Gaussian distributed as\n",
    "\n",
    "$$\n",
    "p(z) = \\mathcal{N} \\left(z|A\\mu+b, A\\Sigma A^T \\right) \\tag{SRG-4a}\n",
    "$$\n",
    "\n",
    "- The **sum of two independent Gaussian variables** is also Gaussian distributed. Specifically, if $x \\sim \\mathcal{N} \\left(x|\\mu_x, \\Sigma_x \\right)$ and $y \\sim \\mathcal{N} \\left(y|\\mu_y, \\Sigma_y \\right)$, then the PDF for $z=x+y$ is given by\n",
    "\n",
    "\\begin{align}\n",
    "p(z) &= \\mathcal{N}(x|\\mu_x,\\Sigma_x) \\ast \\mathcal{N}(y|\\mu_y,\\Sigma_y) \\notag\\\\\n",
    "  &= \\mathcal{N} \\left(z|\\mu_x+\\mu_y, \\Sigma_x +\\Sigma_y \\right) \\tag{SRG-8}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Gaussian Signals in a Linear System\n",
    "\n",
    "\\begin{figure}[h]\\centering\n",
    "\\includegraphics[height=2cm]{./figures/fig-linear-system}\n",
    "\\end{figure}\n",
    "\n",
    "- [Q.]: Given independent variables\n",
    "$x \\sim \\mathcal{N}(\\mu_x,\\sigma_y)$ and $y \\sim \\mathcal{N}(\\mu_y,\\sigma_y)$, what is the PDF for $z = A\\cdot(x -y) + b$ ?\n",
    "\n",
    "- [A.]: $z$ is also Gaussian with \n",
    "$$\n",
    "p_z(z) = \\mathcal{N}(z|A(\\mu_x-\\mu_y)+b, \\, A(\\sigma_x \\mathbf{+} \\sigma_y)A^T)\n",
    "$$\n",
    "\n",
    "- Think about the role of the Gaussian distribution for stochastic linear systems in relation to what sinusoidals mean for deterministic linear system analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Bayesian Estimation of a Constant\n",
    "\n",
    "- [Q.] Estimate a constant $\\theta$ from one 'noisy' measurement $x$ about that constant. Assume the following model specification:\n",
    "     \n",
    "\\begin{align}\n",
    "x &= \\theta + \\epsilon \\\\\n",
    "\\theta &\\sim \\mathcal{N}(\\theta|\\mu_\\theta,\\sigma_\\theta^2) \\\\\n",
    "\\epsilon &\\sim \\mathcal{N}(\\epsilon|0,\\sigma^2_{\\epsilon})\n",
    "\\end{align}\n",
    "\n",
    "- [A.]\n",
    "1. **Model specification**\n",
    "Note that you can rewrite these specifications in probabilistic notation as follows:\n",
    "\\begin{align}\n",
    "    p(x|\\theta) &=\\mathcal{N}(x|\\theta,\\sigma^2_{\\epsilon}) \\tag{likelihood}\\\\\n",
    "    p(\\theta) &=\\mathcal{N}(\\theta|\\mu_\\theta,\\sigma_\\theta^2) \\tag{prior}\n",
    "\\end{align}\n",
    "2. **Inference** for the posterior PDF $p(\\theta|x)$\n",
    "\\begin{align*}\n",
    "p(\\theta|x)  &= \\frac{p(x|\\theta)p(\\theta)}{p(x)} = \\frac{p(x|\\theta)p(\\theta)} { \\int p(x|\\theta)p(\\theta) \\, \\mathrm{d}\\theta } \\notag \\\\\n",
    "    &= \\frac{1}{C} \\,\\mathcal{N}(x|\\theta,\\sigma^2_{\\epsilon})\\, \\mathcal{N}(\\theta|\\mu_\\theta,\\sigma_\\theta^2) \\notag \\\\\n",
    "    &= \\frac{1}{C_1} \\mathrm{exp} \\left\\{ -\\frac{(x-\\theta)^2}{2\\sigma^2_{\\epsilon}} - \\frac{(\\theta-\\mu_\\theta)^2}{2\\sigma_\\theta^2} \\right\\} \\notag \\\\\n",
    "    &= \\frac{1}{C_1} \\mathrm{exp} \\left\\{ \\theta^2\\left( -\\frac{1}{2\\sigma^2_{\\epsilon}} - \\frac{1}{2\\sigma_\\theta^2} \\right) + \\theta \\left( \\frac{x}{\\sigma^2_{\\epsilon}} + \\frac{\\mu_\\theta}{\\sigma_\\theta^2} \\right) +  C_2 \\right\\} \\notag \\\\\n",
    "    &= \\frac{1}{C_1} \\mathrm{exp} \\left\\{ -\\frac{\\sigma_\\theta^2 + \\sigma^2_{\\epsilon}}{2\\sigma_\\theta^2 \\sigma^2_{\\epsilon}} \\left( \\theta - \\frac{x\\sigma_\\theta^2 + \\mu_s\\sigma^2_{\\epsilon}}{\\sigma_\\theta^2 + \\sigma^2_{\\epsilon}} \\right)^2 + C_3  \\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "- This computational 'trick' for multiplying two Gaussians is called **completing the square**. Compare the procedure to $$ax^2+bx+c_1 = a\\left(x+\\frac{b}{2a}\\right)^2+c_2$$\n",
    "\n",
    "- Hence, it follows that the posterior for $\\theta$ is\n",
    "$$\n",
    "    p(\\theta|x) = \\mathcal{N} (\\theta |\\, \\mu_{\\theta|x}, \\sigma_{\\theta|x}^2)\n",
    "$$\n",
    "where\n",
    "\n",
    "\\begin{align}\n",
    "  \\sigma_{\\theta|x}^2  &= \\frac{\\sigma^2_{\\epsilon}\\sigma_\\theta^2}{\\sigma^2_{\\epsilon} + \\sigma_\\theta^2} = \\left( \\frac{1}{\\sigma_\\theta^2} + \\frac{1}{\\sigma^2_{\\epsilon}}\\right)^{-1} \\\\\n",
    "  \\mu_{\\theta|x}   &= \\sigma_{\\theta|x}^2 \\, \\left( \\frac{1}{\\sigma^2_{\\epsilon}}x + \\frac{1}{\\sigma_\\theta^2} \\mu_\\theta \\right) \n",
    "\\end{align}\n",
    "\n",
    "- So, multiplication of two Gaussians yields another (unnormalized) Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Gaussian Multiplication\n",
    "- In general, the multiplication of two multi-variate Gaussians yields an (unnormalized) Gaussian, see [SRG-6]:\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(x|\\mu_a,\\Sigma_a) \\cdot \\mathcal{N}(x|\\mu_b,\\Sigma_b) = \\alpha \\cdot \\mathcal{N}(x|\\mu_c,\\Sigma_c)\n",
    "$$\n",
    "where\n",
    "\\begin{align*}\n",
    "\\Sigma_c &= \\left( \\Sigma_a^{-1} + \\Sigma_b^{-1} \\right)^{-1}\\\\\n",
    "\\mu_c &= \\Sigma_c \\left( \\Sigma_a^{-1}\\mu_a + \\Sigma_b^{-1}\\mu_b\\right)\n",
    "\\end{align*}\n",
    "\n",
    "and normalization constant $\\alpha = \\mathcal{N}(\\mu_a|\\, \\mu_b, \\sigma_a + \\sigma_b)$.\n",
    "\n",
    "- If we define the **precision** as $\\Lambda \\equiv \\Sigma^{-1}$, then we see that **precisions add** and **precision-weighted means add** too.\n",
    "- As we just saw, great application to Bayesian inference!\n",
    "\n",
    "$$\n",
    "\\underbrace{\\text{Gaussian}}_{\\text{posterior}}\n",
    " \\propto \\underbrace{\\text{Gaussian}}_{\\text{likelihood}} \\times \\underbrace{\\text{Gaussian}}_{\\text{prior}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditioning and Marginalization of a Gaussian\n",
    "\n",
    "Let $z = \\begin{bmatrix} x \\\\ y \\end{bmatrix}$ be jointly normal distributed as\n",
    "\n",
    "$$\n",
    "p(z|\\mu,\\Sigma) = \\mathcal{N} \\left( \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\left| \\begin{bmatrix} \\mu_x \\\\ \\mu_y \\end{bmatrix}, \n",
    "  \\begin{bmatrix} \\Sigma_x & \\Sigma_{xy} \\\\ \\sigma_{yx} & \\sigma_y \\end{bmatrix} \\right. \\right)\n",
    "$$\n",
    "\n",
    "- Note that the symmetry $\\Sigma=\\Sigma^T$ implies that $\\Sigma_x$ and $\\Sigma_y$ are symmetric and $Sigma_{xy} = \\Sigma_{yx}^T$.\n",
    "\n",
    "- Now let's factorize $p(x,y)$ into $p(y|x)\\, p(x)$ through conditioning and marginalization (for applications to Bayesian inference in jointly Gaussian systems).\n",
    "\n",
    "- **Marginalization**\n",
    "$$\n",
    "p(x) = \\int p(x,y)\\,\\mathrm{d}y = \\mathcal{N}\\left( x|\\mu_x, \\Sigma_x \\right), \\qquad p(y)=\\mathcal{N} \\left(y|\\mu_y, \\Sigma_y \\right)\n",
    "$$\n",
    "\n",
    "- **Conditioning**\n",
    "\\begin{align}\n",
    "p(y|x) &= p(x,y)/p(x) \\\\\n",
    " &= \\mathcal{N}\\left(y|\\mu_y + \\Sigma_{yx}\\Sigma_x^{-1}(x-\\mu_x),\\, \\Sigma_y - \\Sigma_{yx}\\Sigma_x^{-1}\\Sigma_{xy} \\right)\n",
    "\\end{align}\n",
    "\n",
    "- See [MJ-ch13] for a clear and detailed derivation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Example: Conditioning of Gaussian\n",
    "\n",
    "Consider (again) the system \n",
    "\n",
    "\\begin{align}\n",
    "x &= \\theta + \\epsilon \\\\\n",
    "\\theta &\\sim \\mathcal{N}(\\theta|\\mu_\\theta,\\sigma_\\theta^2) \\\\\n",
    "\\epsilon &\\sim \\mathcal{N}(\\epsilon|0,\\sigma^2_{\\epsilon})\n",
    "\\end{align}\n",
    "\n",
    "- This system is equivalent to (derive this!)\n",
    "$$\n",
    "p(\\theta,x|\\,\\mu,\\sigma) = \\mathcal{N} \n",
    "  \\left( \n",
    "  \\begin{bmatrix} \\theta\\\\ x \\end{bmatrix} \n",
    "  \\left| \\begin{bmatrix} \\mu_\\theta\\\\ \\mu_\\theta\\end{bmatrix}, \n",
    "         \\begin{bmatrix} \\sigma_\\theta^2 & \\sigma_\\theta^2\\\\ \\sigma_\\theta^2 & \\sigma_\\theta^2+\\sigma_{\\epsilon}^2 \n",
    "  \\end{bmatrix} \n",
    "  \\right. \n",
    "  \\right)\n",
    "$$\n",
    "\n",
    "- Direct substitution of the rule for Gaussian conditioning:\n",
    "\\begin{align*}\n",
    "K &= \\frac{\\sigma_\\theta^2}{\\sigma_\\theta^2+\\sigma_{\\epsilon}^2} \\tag{K is called Kalman gain}\\\\\n",
    "p(\\theta|x) &= \\mathcal{N} \\left( \\theta|\\, \\mu_\\theta + K\\cdot(x-\\mu_\\theta),\\,\\sigma_\\theta^2\\left( 1-k\\right) \\right)\n",
    "\\end{align*}\n",
    "    \n",
    "- Exercises: (1) Actually derive this; (2) show that the result is equivalent to the previous slide on 'estimation of a constant'; and (3) Try to interpret the resulting formula's}\n",
    "- homework: Derive this result\n",
    "- Moral: For jointly Gaussian systems, we do inference simply in one step by using the formulas for conditioning and marginalization.\n",
    "Compare this to Eqs.~\\ref{eq:recursive-bayes}-\\ref{eq:recursive-bayes-kalman-gain}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: Recursive Bayesian Estimation\n",
    "\n",
    "Now consider the signal $x(t)=\\theta+\\epsilon(t)$, where $D_t= \\left(x(1),\\ldots,x(t)\\right)$ is observed _sequentially_ (over time).\n",
    "\n",
    "- [Q.] We want a recursive algorithm for $p(\\theta|D_t)$.\n",
    "    \n",
    "- [A.] Again we assume prior $p(\\theta) = \\mathcal{N}(\\mu,\\sigma^2)$ and define $p(\\theta|D_t) = \\mathcal{N}(\\mu(t),\\sigma^2(t))$ \n",
    "        \n",
    "- We will solve this by using the estimate after $t-1$ as the **prior distribution** in conjunction with the **likelihood** for observation $x(t)$,\n",
    "$$\n",
    "p(\\mu(t)|D_t) \\propto p(x(t)|\\mu(t-1),\\sigma^2(t-1)) \\times p(\\mu(t)|D_{t-1})\n",
    "$$\n",
    "\n",
    "- Use the 'batch processing' posteriors for $\\mu$ and $\\sigma^2$ to get\n",
    "\\begin{align}\n",
    "\\hat \\mu(t) &= \\sigma_{\\mu}^2(t) \\, \\left( \\frac{1}{\\sigma^2_{\\epsilon}(t)}x(t) + \\frac{1}{\\sigma_{\\mu}^2(t-1)} \\hat \\mu(t-1) \\right) \\\\\n",
    "    &= \\frac{\\sigma^2_{\\epsilon}(t)}{\\sigma^2_{\\epsilon}(t)+\\sigma_{\\mu}^2(t-1)}x(t) + \\frac{\\sigma_{\\mu}^2(t-1)}{\\sigma^2_{\\epsilon}(t)+\\sigma_{\\mu}^2(t-1)} \\hat \\mu(t-1) \\\\\n",
    "    &= \\hat \\mu(t-1) + K(t) \\left[ x(t) - \\hat \\mu(t-1) \\right] \\\\\n",
    "\\sigma_{\\mu}^2(t) &= \\sigma_{\\mu}^2(t-1) \\frac{\\sigma^2_{\\epsilon}(t)}{\\sigma^2_{\\epsilon}(t)+\\sigma_{\\mu}^2(t-1)} \\\\\n",
    "    &= \\sigma_{\\mu}^2(t-1) \\left( 1-K(t) \\right)\n",
    "\\end{align}\n",
    "where we defined the **Kalman gain**\n",
    "$$\n",
    "    K(t) =  \\frac{\\sigma_{\\mu}^2(t-1)}{\\sigma^2_{\\epsilon}(t)+\\sigma_{\\mu}^2(t-1)}\n",
    "$$\n",
    "- This linear sequential estimator of mean and variance in Gaussian observations is a **Kalman Filter**.\n",
    "- The new observation $x(t)$ 'corrects' the old estimate $\\hat \\mu(t-1)$ by a quantity that is proportional to the _innovation_ (or _residual_)  $\\left( x(t) - \\hat \\mu(t-1) \\right)$.\n",
    "- Note that the uncertainty about $\\mu$ decreases over time\n",
    "- Recursive Bayesian estimation is the basis for **adaptive signal processing** algorithms such as Least Mean Squares (LMS) and Recursive Least Squares (RLS).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Gaussians\n",
    "The success of Gaussian distributions in probabilistic modeling is large due to the following properties:\n",
    "- The product of two Gaussian functions is another Gaussian function (use in Bayes rule). \n",
    "- The convolution of two Gaussian functions is another Gaussian function (use in sum of 2 variables)\n",
    "- A linear transformation of a Gaussian dsitributed variable is also Gaussian distributed\n",
    "- Conditioning and marginalization of multivariate Gaussian distributions produce Gaussians again (use in working with observations and when doing Bayesian predictions)\n",
    "- The Gaussian PDF has higher entropy than any other with the same variance. (Not discussed in this course).\n",
    "- Any smooth function with single rounded maximum, if raised to higher and higher powers, goes into a Gaussian function. (Not discussed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### What's Next?\n",
    "- We discussed how Bayesian probability theory provides an integrated framework for making predictions based on observed data.\n",
    "- The process involves model specification (your main task!), inference and actual model-based prediction.\n",
    "- The latter two tasks are only difficult because of computational issues.\n",
    "   - Maximum likelihood was introduced as a computationally simpler approximation to the Bayesian approach.\n",
    "   - In particular under some linear Gaussian assumptions, a few interesting models can be designed.\n",
    "   - The rest of this course (part-1) concerns introduction to these Linear Gaussian models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.6",
   "language": "julia",
   "name": "julia 0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
