{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(insert preamble cell)\n",
    "$\\DeclareMathOperator{\\trace}{\\mathrm{Tr}}$\n",
    "$\\newcommand{\\d}[1]{{\\,\\mathrm{d}#1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Density Estimation?\n",
    "\n",
    "Why are we interested to build a density model $p(x|\\theta)$ from data observations $D=\\{x_1,\\dotsc,x_N\\}$? Some examples:\n",
    "\n",
    "- **Outlier detection**. Suppose $D=\\{x_n\\}$ are benign mammogram images. Build $p(x | \\theta)$ from $D$. Then low value for $p(x^\\prime | \\theta)$ indicates that $x^\\prime$ is a risky mammogram.\n",
    "- **Compression**. Code a new data item based on **entropy**, which is a functional of $p(x|\\theta)$: \n",
    "$$\n",
    "H[p] = -\\sum_x p(x | \\theta)\\log p(x |\\theta)\n",
    "$$\n",
    "- **Classification**. Let $p(x | \\theta_1)$ be a model of attributes $x$ for credit-card holders that paid on time and $p(x | \\theta_2)$ for clients that defaulted on payments. Then, assign a potential new client $x^\\prime$ to either class based on the relative probability of $p(x^\\prime | \\theta_1)$ vs. $p(x^\\prime|\\theta_2)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Useful Matrix Calculus\n",
    "\n",
    "We define the **gradient** of a scalar function $f(A)$ w.r.t. an $n \\times k$ matrix $A$ as\n",
    "\n",
    "$$\n",
    "\\nabla_A f \\triangleq\n",
    "    \\begin{bmatrix}\n",
    "\\frac{\\partial{f}}{\\partial a_{11}} & \\frac{\\partial{f}}{\\partial a_{12}} & \\cdots & \\frac{\\partial{f}}{\\partial a_{1k}}\\\\\n",
    "\\frac{\\partial{f}}{\\partial a_{21}} & \\frac{\\partial{f}}{\\partial a_{22}} & \\cdots & \\frac{\\partial{f}}{\\partial a_{2k}}\\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots\\\\\n",
    "\\frac{\\partial{f}}{\\partial a_{n1}} & \\frac{\\partial{f}}{\\partial a_{n2}} & \\cdots & \\frac{\\partial{f}}{\\partial a_{nk}}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "    \n",
    "The following formulas are useful (see Bishop App.-C)\n",
    "\n",
    "\\begin{align}\n",
    "|A^{-1}|&=|A|^{-1} \\tag{B-C.4} \\\\\n",
    "\\nabla_A \\log |A| &= (A^{T})^{-1} = (A^{-1})^T \\tag{B-C.28} \\\\\n",
    "\\trace[ABC]&= \\trace[CAB] = \\trace[BCA] \\tag{B-C.9} \\\\\n",
    "\\nabla_A \\trace[AB] &=\\nabla_A \\trace[BA]= B^T \\tag{B-C.25} \\\\\n",
    "\\nabla_A \\trace[ABA^T] &= A(B+B^T)  \\tag{B-C.27}\\\\\n",
    " \\nabla_x x^TAx &= (A+A^T)x \\tag{from B-C.27}\\\\\n",
    "\\nabla_X a^TXb &= \\nabla_X \\trace[ba^TX] = ab^T \\notag\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Log-Likelihood for a Multivariate Gaussian (MVG)\n",
    "\n",
    "Assume we are given a set of IID data points $D=\\{x_1,\\ldots,x_N\\}$, where $x_n \\in \\Re^D$. We want to build a model for these data.\n",
    "\n",
    "1. **Model specification**. Let's assume a MVG model $x_n=\\mu+\\epsilon_n$ with $\\epsilon_n \\sim \\mathcal{N}(0,\\Sigma)$, or equivalently,\n",
    "\n",
    "$$\n",
    "p(x|\\mu,\\Sigma) = |2 \\pi \\Sigma|^{-\\frac{1}{2}} \\mathrm{exp} \\left\\{-\\frac{1}{2}(x_n-\\mu)^T\n",
    "\\Sigma^{-1} (x_n-\\mu) \\right\\}\n",
    "$$\n",
    "\n",
    "Since the data are IID, $p(D|\\theta)$ factorizes as\n",
    "$$  \n",
    "p(D|\\theta) = p(x_1,\\ldots,x_N|\\theta) \\stackrel{\\text{IID}}{=} \\prod_n p(x_n|\\theta)\n",
    "$$\n",
    "\n",
    "This choice of model yields the following log-likelihood (use (B-C.9) and (B-C.4)),\n",
    "\n",
    "\\begin{align}\n",
    " \\log p(D|\\theta) &= \\log \\prod_n p(x_n|\\theta) = \\sum_n \\log p(x_n|\\theta) \\\\\n",
    "     &= N \\cdot \\log | 2\\pi\\Sigma |^{-1/2} - \\frac{1}{2} \\sum\\nolimits_{n} (x_n-\\mu)^T \\Sigma^{-1} (x_n-\\mu)\n",
    " \\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood estimation of mean of MVG\n",
    "\n",
    "We want to maximize $\\log p(D|\\theta)$ wrt the parameters $\\theta=\\{\\mu,\\Sigma\\}$. Let's take derivatives; first to mean $\\mu$, (making use of  (B-C.25) and (B-C.27)),\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\mu \\log p(D|\\theta) &= -\\frac{1}{2}\\sum_n \\nabla_\\mu \\left[ (x_n-\\mu)^T \\Sigma^{-1} (x_n-\\mu) \\right] \\notag \\\\\n",
    "&= -\\frac{1}{2}\\sum_n \\nabla_\\mu \\trace \\left[ -2\\mu^T\\Sigma^{-1}x_n + \\mu^T\\Sigma^{-1}\\mu \\right] \\notag \\\\\n",
    "&= -\\frac{1}{2}\\sum_n \\left( -2\\Sigma^{-1}x_n + 2\\Sigma^{-1}\\mu \\right) \\notag \\\\\n",
    "&= \\Sigma^{-1}\\,\\sum_n \\left( x_n-\\mu \\right)\n",
    "\\end{align}\n",
    "\n",
    "Set to zero yields the **sample mean**\n",
    "$$\n",
    "\\boxed{\n",
    "\\hat \\mu = \\frac{1}{N} \\sum_n x_n\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Maximum Likelihood estimation of variance of MVG\n",
    "\n",
    "Now we take the gradient of the log-likelihood **wrt the precision matrix** $\\Sigma^{-1}$ (making use of B-C.28 and B-C.24)\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_{\\Sigma^{-1}}  \\log p(D|\\theta) &= \\nabla_{\\Sigma^{-1}} \\left[ \\frac{N}{2} \\log |2\\pi\\Sigma|^{-1} - \\frac{1}{2} \\sum_{n=1}^N (x_n-\\mu)^T \\Sigma^{-1} (x_n-\\mu)\\right]\\notag \\\\\n",
    "&= \\nabla_{\\Sigma^{-1}} \\left[ \\frac{N}{2} \\log |\\Sigma^{-1}| - \\frac{1}{2} \\sum_{n=1}^N \\trace \\left[ (x_n-\\mu) (x_n-\\mu)^T \\Sigma^{-1}\\right] \\right]\\\\\n",
    "&= \\frac{N}{2}\\Sigma -\\frac{1}{2}\\sum_n (x_n-\\mu)(x_n-\\mu)^T\n",
    "\\end{align}\n",
    "\n",
    "Get optimum by setting the gradient to zero,\n",
    "$$\n",
    "\\boxed{\n",
    "\\hat \\Sigma = \\frac{1}{N} \\sum_n (x_n-\\hat\\mu)(x_n - \\hat\\mu)^T}\n",
    "$$\n",
    "which is the **sample variance**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sufficient Statistics\n",
    "\n",
    "Note that the ML estimates can also be written as\n",
    "$$\n",
    "\\hat \\Sigma = \\sum_n x_n x_n^T - \\left( \\sum_n x_n\\right)\\left( \\sum_n x_n\\right)^T, \\quad \\hat \\mu = \\frac{1}{N} \\sum_n x_n\n",
    "$$\n",
    "\n",
    "I.o.w., the statistics $\\sum_n x_n$ and $\\sum_n x_n x_n^T$ are sufficient to estimate the parameters $\\mu$ and $\\Sigma$ from observations. In the literature, they are called **sufficient statistics**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Data: the 1-of-K Coding Scheme\n",
    "\n",
    "Consider a coin-tossing experiment with outcomes $x \\in\\{0,1\\}$ (corresponding to tail and head, resp.) and $0\\leq \\mu \\leq 1$ the probability of heads. This model can written as a **Bernoulli distribution**:\n",
    "\n",
    "$$ \n",
    "p(x|\\mu) = \\mu^{x}(1-\\mu)^{1-x}\n",
    "$$\n",
    "\n",
    "- Note that in expression $\\mu^{x}(1-\\mu)^{1-x}$, the variable $x$ acts as a (binary) **selector** for the tail or head probabilities.\n",
    "\n",
    "- **1-of-K scheme**. Now consider a $K$-sided coin, a.k.a. a _die_ (pl.: dice). It will be very convenient to code the outcomes by a vector $x=(x_1,\\ldots,x_K)^T$ with **binary selection variables**\n",
    "\n",
    "$$\n",
    "x_k = \\begin{cases} 1 & \\text{if die landed on $k$th face}\\\\\n",
    "0 & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "- E.g., For $K=6$, if the die lands on the 3rd face, we encode that as $x=(0,0,1,0,0,0)^T$.\n",
    "\n",
    "- Assume the probabilities $p(x_k=1) = \\mu_k$ with  $\\sum_k \\mu_k  = 1$. The data generating distribution is then (note the similarity to the Bernoulli distribution)\n",
    "\n",
    "$$\n",
    "p(x|\\mu) = \\mu_1^{x_1} \\mu_2^{x_2} \\cdots \\mu_k^{x_k}=\\prod_k \\mu_k^{x_k}\n",
    "$$\n",
    "\n",
    "- This distribution is sometimes (but not consistently) called the 'multi-noulli' distribution.\n",
    "- Note that $\\sum_k x_k = 1$ and verify for yourself that $\\mathrm{E}[x|\\mu] = \\mu$.\n",
    "\n",
    "- In these notes, we use the superscript to indicate that we are working with a **binary selection variable** in a 1-of-$K$ scheme.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Relation to The Multinomial Distribution\n",
    "\n",
    "Observe a data set $D=\\{x_1,\\ldots,x_N\\}$  of $N$ IID rolls of a $K$-sided die, with generating PDF\n",
    "$$\n",
    "p(D|\\mu) = \\prod_n \\prod_k \\mu_k^{x_{nk}} = \\prod_k \\mu_k^{\\sum_n x_{nk}} = \\prod_k \\mu_k^{m_k}\n",
    "$$\n",
    "where $m_k= \\sum_n x_{nk}$ is the total number of occurrences that we `threw' $k$ eyes.\n",
    "\n",
    "- This distribution depends on the observations **only** through the quantities $\\{m_k\\}$, with generally $K \\ll N$. \n",
    "\n",
    "- A related distribution is the distribution over $D_m=(m_1,\\ldots,m_K)^T$, which is called the **multinomial distribution**,\n",
    "$$\n",
    "p(D_m|\\mu) =\\frac{N!}{m_1! m_2!\\ldots m_K!} \\,\\prod_k \\mu_k^{m_k}\\,.\n",
    "$$\n",
    "\n",
    "- Note that $p(D|\\mu)$ and $p(D_m|\\mu)$ differ only in the normalization factor. Relate this to the fact that $D$ has $N$ components and $D_m$ has $K$ components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Maximum Likelihood Estimation for the Multinomial\n",
    "\n",
    "Now let's find the ML estimate for $\\mu$, based on $N$ throws of a $K$-sided die. \n",
    "\n",
    "- The log-likelihood with Lagrange multiplier to include the constraint is\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{L}^\\prime &\\triangleq \\log \\prod_k \\mu_k^{m_k}  + \\lambda \\cdot(1 - \\sum_k \\mu_k ) \\\\\n",
    "\t&= \\sum_k m_k \\log \\mu_k  + \\lambda \\cdot (1 - \\sum_k \\mu_k )\\,.\n",
    "\\end{align}\n",
    "\n",
    "Set derivative to zero yields the **sample proportion** for $\\mu_k$ %(get $\\lambda$ from $\\sum_k \\hat\\theta_k = 1$)\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mu_k}   \\mathrm{L}^\\prime = \\frac{m_k }\n",
    "{\\hat\\mu_k } - \\lambda  = 0 \\; \\Rightarrow \\; \\boxed{\\hat\\mu_k  = \\frac{m_k }\n",
    "{\\lambda } = \\frac{m_k }{N}}\n",
    "$$\n",
    "\n",
    "where we get $\\lambda$ from the constraint $$\\sum_k \\hat \\mu_k = \\sum_k \\frac{m_k}\n",
    "{\\lambda} = \\frac{N}{\\lambda} = 1$$\n",
    "\n",
    "Compare this answer to Laplace's rule for predicting the next coin toss ($p(h|D)=(N_h+1)/(N+2)$)\n",
    "\n",
    "- Interesting special case: **Binomial** (=$N$ coin tosses); \n",
    "$$p(x_n|\\theta)= \\theta^{x_n^h}(1-\\theta)^{1-x_n^h}=\\theta_h^{x_n^h} \\theta_t^{x_n^t}\n",
    "$$ \n",
    "yields $$ \\hat \\theta = \\frac{N_h}{N_h +N_t} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Recap ML for Density Estimation\n",
    "\n",
    "Given $N$ IID observations $D=\\{x_1,\\dotsc,x_N\\}$\n",
    "\n",
    "- For MVG model, $p(x_n|\\theta) = \\mathcal{N}(x_n|\\mu,\\Sigma)$, we obtain ML estimates\n",
    "\n",
    "\\begin{align}\n",
    "\\hat \\mu &= \\frac{1}{N} \\sum_n x_n \\tag{sample mean} \\\\\n",
    "\\hat \\Sigma &= \\frac{1}{N} \\sum_n (x_n-\\hat\\mu)(x_n - \\hat \\mu)^T \\tag{sample variance}\n",
    "\\end{align}\n",
    "\n",
    "- For discrete outcomes modeled by a 1-of-K multi-noulli distribution $p(x_n|\\mu) = \\prod_k \\mu_k^{x_{nk}}$ (with $\\sum_k \\mu_k  = 1$), we find\n",
    "\n",
    "\\begin{align}\n",
    "\\hat\\mu_k  = \\frac{1}{N} \\sum_n x_{nk} \\quad \\left(= \\frac{m_k}{N} \\right) \\tag{sample proportion}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Note the similarity for the means between discrete and continuous data. \n",
    "\n",
    "- We didn't use a co-variance matrix for discrete data. Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.6",
   "language": "julia",
   "name": "julia 0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
