{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Difficult Class-Conditional Densities\n",
    "\n",
    "Sometimes, the class-conditional densities are very non-Gaussian, but the linear discriminative boundary looks easy enough:\n",
    " \n",
    "\\begin{center}\n",
    "\\includegraphics[height=6cm]{./figures/fig-classification-not-gaussian}\n",
    "\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Discriminative Linear Classification\n",
    "\n",
    "-  Sometimes, the precise assumptions of the generative model $$p(x,\\mathcal{C}_k|\\theta) =  \\pi_k \\cdot \\mathcal{N}(x|\\mu_k,\\Sigma)$$ are not met.\n",
    "-  Alternative approach: model the posterior $p(\\mathcal{C}_k|x)$ directly, without any assumptions on the class densities.\n",
    "-  [Q.] What model should we use for $p(\\mathcal{C}_k|x)$?\n",
    "\n",
    "-  [A.] Get inspiration from the generative approach: choose the familiar softmax structure for the posterior class probability\n",
    "$$\n",
    "p(\\mathcal{C}_k|x,\\theta_k) = \\frac{e^{\\theta_k^T x}}{\\sum_j e^{\\theta_j^T x}}\n",
    "$$\n",
    "but **do not impose a Gaussian structure on the classes**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Discriminative vs. Generative Classification\n",
    "\n",
    "**Two key differences** for discriminative approach\n",
    "1. Parameter $\\theta_k$ is **not** structured into $\\{\\mu_k,\\Sigma,\\pi_k \\}$. This provides discriminative approach with more flexibility.\n",
    "2. ML learning by optimization of _conditional_ likelihood $\\prod_n p(t_n|x_n,\\theta)$ rather than _joint_ likelihood $\\prod_n p(t_n,x_n|\\theta)$.\n",
    "\n",
    "As we will see, ML estimation for discriminative classification is more complex than for the linear Gaussian generative approach.\n",
    "\n",
    "-  Discriminative model-based prediction for a new input $x$\n",
    "is easy, namely substitute the ML estimate in the model to get\n",
    "$$p(\\mathcal{C}_k|\\, x,\\hat\\theta) = \\frac{ \\mathrm{exp}\\left( \\hat \\theta_k^T x\\right) }{ \\sum_j \\mathrm{exp}\\left(\\hat \\theta_j^T x\\right)} \\propto \\mathrm{exp}\\left(\\hat \\theta_k^T x\\right) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ###  ML Estimation for Discriminative Classification\n",
    " \n",
    "-  Work out the conditional log-likelihood ($y_{k}$ is the target, while $y_{k}=p(\\mathcal{C}_k|x,\\theta)$ is the model prediction).\n",
    "     $$\n",
    "    \\mathrm{L}(\\theta) = \\log \\prod_n \\prod_k {\\underbrace{p(\\mathcal{C}_k|x_n,\\theta)}_{y_{nk}}}^{t_{nk}} = \\sum_{n,k} t_{nk} \\log y_{nk}\n",
    "     $$\n",
    "     -  Note that softmax $\\phi_k=e^{z_k}/{\\sum_j e^{z_j}}$ has analytical derivative,\n",
    " \\begin{align}\n",
    " \\frac{\\partial \\phi_k}{\\partial z_j} &= \\frac{(\\sum_j e^{z_j})e^{z_k}\\delta_{kj}-e^{z_j}e^{z_k}}{(\\sum_j e^{z_j})^2} = \\frac{e^{z_k}}{\\sum_j e^{z_j}}\\delta_{kj} - \\frac{e^{z_j}}{\\sum_j e^{z_j}} \\frac{e^{z_k}}{\\sum_j e^{z_j}}\\\\\n",
    "     &= \\phi_k \\cdot(\\delta_{kj}-\\phi_j)\n",
    " \\end{align}\n",
    "\n",
    "%    -  Again we try to minimize the cross-entropy ($\\sum_{nk} y_{nk} \\log \\frac{y_{nk}}{p_{nk}}$) between the data `targets' $t_{nk}$ and the model outputs $p_{nk}$.\n",
    "\n",
    " -  Taking the derivative (or: how to spend a hour ...)\n",
    "\\begin{align} \n",
    "\\nabla_{\\theta_j} \\mathrm{L}_{nk}(\\theta_j) &= \\sum_{n,k} \\frac{\\partial \\mathrm{L}_{nk}}{\\partial y_{nk}}\\,\\frac{\\partial y_{nk}}{\\partial z_{nj}}\\,\\frac{\\partial z_{nj}}{\\partial \\theta_j} = \\sum_{n,k} \\frac{t_{nk}}{y_{nk}}\\,y_{nk} (\\delta_{kj}-y_{nj})\\, x_n \\\\\n",
    "  &= \\sum_n \\left( t_{nj} (1-y_{nj}) -\\sum_{k\\neq j} t_{nk} y_{nj} \\right) x_n \\\\\n",
    "  &= -\\sum_n (t_{nj} - y_{nj})\\, x_n\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear vs. Logistic Regression\n",
    "\n",
    "The parameter vector $\\theta$ for logistic regression can be estimated through gradient-based adaptation, e.g.,\n",
    "\n",
    "\\begin{align}\n",
    "        \\theta_{n+1} &= \\theta_n - \\eta \\cdot \\nabla_\\theta \\mathrm{L}(\\theta) \\\\\n",
    "        &= \\theta_n + \\sum_n \\left( t_n - \\frac{1}{1+e^{-\\theta^Tx_n}} \\right)x_n\n",
    "\\end{align}\n",
    "    \n",
    "    Run \\texttt{demo\\_classification.m} in matlab\n",
    "\n",
    "    \n",
    "Compare the gradients for linear and logistic regression,\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\mathrm{L}(\\theta) &= - \\sum_n \\left(t_n - \\theta^T x_n \\right) x_n  \\tag{linear regression} \\\\\n",
    "\\nabla_\\theta \\mathrm{L}(\\theta) &= - \\sum_n \\left(t_n - \\frac{1}{1+e^{-\\theta^Tx_n}} \\right) x_n\n",
    " \\tag{logistic regression}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.6",
   "language": "julia",
   "name": "julia 0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
